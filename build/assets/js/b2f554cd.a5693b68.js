"use strict";(self.webpackChunkseata_website=self.webpackChunkseata_website||[]).push([[11477],{30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/seata-meetup-hangzhou","metadata":{"permalink":"/blog/seata-meetup-hangzhou","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-meetup-hangzhou.md","source":"@site/blog/seata-meetup-hangzhou.md","title":"seata-meetup-hangzhou","description":"Placeholder. DO NOT DELETE.","date":"2024-01-26T07:26:53.000Z","formattedDate":"January 26, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"nextItem":{"title":"Exploring the Journey of Open Source Development in Seata Project","permalink":"/blog/explore-seata-journey"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/explore-seata-journey","metadata":{"permalink":"/blog/explore-seata-journey","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/explore-seata-journey.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/explore-seata-journey.md","title":"Exploring the Journey of Open Source Development in Seata Project","description":"In this article, I will share my journey as a developer in the Seata community, along with the experiences and insights I have gained during this adventure.","date":"2023-11-27T00:00:00.000Z","formattedDate":"November 27, 2023","tags":[],"readingTime":11.105,"hasTruncateMarker":false,"authors":[{"name":"Yinxiangkun - Tsinghua University, participant in Seata Summer of Code"}],"frontMatter":{"title":"Exploring the Journey of Open Source Development in Seata Project","keywords":["Seata","open source","Summer of Code","distributed transactions"],"description":"In this article, I will share my journey as a developer in the Seata community, along with the experiences and insights I have gained during this adventure.","author":"Yinxiangkun - Tsinghua University, participant in Seata Summer of Code","date":"2023-11-27T00:00:00.000Z"},"prevItem":{"title":"seata-meetup-hangzhou","permalink":"/blog/seata-meetup-hangzhou"},"nextItem":{"title":"Seata-Raft Storage Mode in Depth and Getting Started","permalink":"/blog/seata-raft-detailed-explanation"}},"content":"> Seata is an open-source distributed transaction solution dedicated to providing high-performance and user-friendly distributed transaction services in a microservices architecture. During this year\'s Summer of Code event, I joined the Apache Seata (Incubator) community, completed the Summer of Code project, and have been actively involved in the community ever since. I was fortunate to share my developer experience at the YunQi Developer Show during the Cloud Conferen\\n\\n## Relevant Background\\n\\nBefore formally introducing my experiences, I would like to provide some relevant background information to explain why I chose to participate in open source and how I got involved. There are various motivations for participating in open source, and here are some of the main reasons I believe exist:\\n\\n- **Learning**: Participating in open source provides us with the opportunity to contribute to open-source projects developed by different organizations, interact with industry experts, and offers learning opportunities.\\n- **Skill Enhancement**: In my case, I usually work with Java and Python for backend development. However, when participating in the Seata project, I had the chance to learn the Go language, expanding my backend technology stack. Additionally, as a student, it\'s challenging to encounter production-level frameworks or applications, and the open-source community provided me with this opportunity.\\n- **Interest**: Many of my friends are passionate about open source, enjoying programming and being enthusiastic about open source.\\n- **Job Seeking**: Participating in open source can enrich our portfolio, adding weight to resumes.\\n- **Work Requirements**: Sometimes, involvement in open source is to address work-related challenges or meet job requirements.\\n\\nThese are some reasons for participating in open source. For me, learning, skill enhancement, and interest are the primary motivations. Whether you are a student or a working professional, if you have the willingness to participate in open source, don\'t hesitate. Anyone can contribute to open-source projects. Age, gender, occupation, and location are not important; the key is your passion and curiosity about open-source projects.\\n\\n**The opportunity for me to participate in open source arose when I joined the Open Source Promotion Plan (OSPP) organized by the Institute of Software, Chinese Academy of Sciences.**\\n\\nOSPP is an open-source activity for university developers. The community releases open-source projects, and student developers complete project development under the guidance of mentors. The completed results are contributed to the community, merged into the community repository, and participants receive project bonuses and certificates. OSPP is an excellent opportunity to enter the open-source community, and it was my first formal encounter with open-source projects. This experience opened a new door for me. I deeply realized that participating in the construction of open-source projects, sharing your technical achievements, and enabling more developers to use what you contribute is a joyful and meaningful endeavor.\\n\\nThe image below, officially released by OSPP, shows that the number of participating communities and students has been increasing year by year since 2020, and the event is getting better. This year, a total of 133 community projects were involved, each providing several topics, with each student selecting only one topic. Choosing a community to participate in and finding a suitable topic in such a large number of communities is a relatively complex task.\\n\\n![img](/img/blog/explore-seata-ospp.png)\\n\\n**Considering factors such as community activity, technical stack compatibility, and guidance for newcomers, I ultimately chose to join the Seata community.**\\n\\nSeata is an open-source distributed transaction framework that provides a complete distributed transaction solution, including AT, TCC, Saga, and XA transaction modes, and supports multiple programming languages and data storage solutions. Since its open source in 2019, Seata has been around for **5** years, with over **300** contributors in the community. The project has received **24k+** stars and is a mature community. Seata is compatible with **10+** mainstream RPC frameworks and RDBMS, has integration relationships with **20+** communities, and is applied to business systems by **thousands** of customers. It can be considered the de facto standard for distributed transaction solutions.\\n\\n![img](/img/blog/explore-seata-apache.png)\\n\\n### Seata\'s Journey to Apache Incubator\\n\\nOn October 29, 2023, Seata was formally donated to the Apache Software Foundation and became an incubating project. After incubation, Seata is expected to become the first top-level distributed transaction framework project under the Apache Software Foundation. This donation will propel Seata to a broader development scope, profoundly impacting ecosystem construction, and benefiting more developers. This significant milestone also opens up broader development opportunities for Seata.\\n\\n## Development Journey\\n\\n**Having introduced some basic information, the following sections will delve into my development journey in the Seata community.**\\n\\nBefore officially starting development, I undertook several preparatory steps. Given Seata\'s five years of development and the accumulation of hundreds of thousands of lines of code, direct involvement requires a certain learning curve. I share some preparatory experiences in the hope of providing inspiration.\\n\\n1. **Documentation and Blogs as Primary Resources**\\n   - Text materials such as documentation and blogs help newcomers quickly understand project background and code structure.\\n   - Official documentation is the primary reference material, providing insights into everything the official documentation deems necessary to know.\\n   ![img](/img/blog/explore-seata-docs.png)\\n   - Blogs, secondary to official documentation, are often written by developers or advanced users. Blogs may delve deeper into specific topics, such as theoretical models of projects, project structure, and source code analysis of specific modules.\\n   ![img](/img/blog/explore-seata-blogs.png)\\n   - Public accounts (such as WeChat) are similar to blogs, generally containing technical articles. An advantage of public accounts is the ability to subscribe for push notifications, allowing for technical reading during spare time.\\n   ![img](/img/blog/explore-seata-pubs.png)\\n   - Additionally, slides from online or offline community presentations and meetups provide meaningful textual materials.\\n   ![img](/img/blog/explore-seata-slides.png)\\n   - Apart from official materials, many third-party resources are available for learning, such as understanding specific implementations and practices through user-shared use cases, exploring the project\'s ecosystem through integration documentation from third-party communities, and learning through video tutorials. However, among all these materials, I consider official documentation and blogs to be the most helpful.\\n\\n2. **Familiarizing Yourself with the Framework**\\n   - Not all text materials need to be thoroughly read. Understanding is superficial if confined to paper. Practice should commence when you feel you understand enough. The \\"Get Started\\" section in the official documentation is a step-by-step guide to understanding the project\'s basic workflow.\\n   - Another approach is to find examples or demonstrations provided by the official project, build and run them, understand the meanings of code and configurations, and learn about the project\'s requirements, goals, existing features, and architecture through usage.\\n   - For instance, Seata has a repository named \\"seata-samples\\" containing over 20 use cases, covering scenarios like Seata integration with Dubbo, integration with SCA, and Nacos integration. These examples cover almost all supported scenarios.\\n\\n3. **Roughly Reading Source Code to Grasp Main Logic**\\n   - In the preparation phase, roughly reading the source code to grasp the project\'s main logic is crucial. Efficiently understanding a project\'s core content is a skill that requires long-term accumulation.\\n   - First, through the previously mentioned preparation steps, understanding the project\'s concepts, interactions, and process models is helpful.\\n   - Taking Seata as an example, through official documentation and practical operations, you can understand the three roles in Seata\'s transaction domain: TC (Transaction Coordinator), TM (Transaction Manager), and RM (Resource Manager). TC, deployed independently as a server, maintains the state of global and branch transactions, crucial for Seata\'s high availability. TM interacts with TC, defining the start, commit, or rollback of global transactions. RM manages resources for branch transaction processing, interacts with TC to register branch transactions and report branch transaction states, and drives branch transaction commit or rollback. After roughly understanding the interaction between these roles, grasping the project\'s main logic becomes easier.\\n   ![img](/img/solution.png)\\n   - Having a mental impression of these models makes it easier to extract the main logic from the source code. For example, analyzing the Seata TC transaction coordinator, as a server-side application deployed independently of the business, involves starting the server locally and tracking it through the startup class. This analysis can reveal some initialization logic, such as service registration and initialization of global locks. Tracking the code through RPC calls can reveal how TC persists global and branch transactions and how it drives global transaction commit or rollback.\\n   - However, for embedded client framework code without a startup class entry point for analysis, starting with a sample can be effective. Finding references to framework code in a sample allows for code reading. For instance, a crucial annotation in Seata is `GlobalTransaction`, used to identify a global transaction. To understand how TM analyzes this annotation, one can use the IDE\'s search function to find the interceptor for `GlobalTransaction` and analyze its logic.\\n   - Here\'s a tip: Unit tests often focus on the functional aspects of a single module. Reading unit tests can reveal a module\'s input-output, logic boundaries, and understanding the code through the unit test\'s call chain is an essential means of understanding the source code.\\n\\n**With everything prepared, the next step is to actively participate in the community.**\\n\\n## Ways to Contribute and Personal Insights\\n\\nThere are various ways to participate, with one of the most common being to check the project\'s Issues list. Communities often mark issues suitable for new contributors with special labels such as \\"good-first-issue,\\" \\"contributions-welcome,\\" and \\"help-wanted.\\" Interested tasks can be filtered through these labels.\\n\\n![img](/img/blog/explore-seata-issues.png)\\n\\nIn addition to Issues, GitHub provides a discussion feature where you can participate in open discussions and gain new ideas.\\n\\n![img](/img/blog/explore-seata-discussion.png)\\n\\nFurthermore, communities often hold regular meetings, such as weekly or bi-weekly meetings, where you can stay updated on the community\'s latest progress, ask questions, and interact with other community members.\\n\\n## Summary and Insights\\n\\nI initially joined the Seata community through the Open Source Summer Program. I completed my project, implemented new features for Seata Saga, and carried out a series of optimizations. However, I didn\'t stop there. My open-source experience with Seata provided me with the most valuable developer experience in my student career. Over time, I continued to stay active in the community through the aforementioned participation methods. This was mainly due to the following factors:\\n\\n1. **Communication and Networking:** The mentorship system provided crucial support. During development, the close collaboration between my mentor and me played a key role in adapting to community culture and workflow. My mentor not only helped me acclimate to the community but also provided design ideas and shared work-related experiences and insights, all of which were very helpful for my development. Additionally, Seata community founder Ming Cai provided a lot of assistance, including establishing contacts with other students, helping with code reviews, and offering many opportunities.\\n\\n2. **Positive Feedback:** During Seata\'s development, I experienced a virtuous cycle. Many details provided positive feedback, such as my contributions being widely used and beneficial to users, and the recognition of my development efforts by the community. This positive feedback strengthened my desire to continue contributing to the Seata community.\\n\\n3. **Skill Enhancement:** Participating in Seata development greatly enhanced my abilities. Here, I could learn production-level code, including performance optimization, interface design, and techniques for boundary judgment. I could directly participate in the operation of an open-source project, including project planning, scheduling, and communication. Additionally, I gained insights into how a distributed transaction framework is designed and implemented.\\n\\nIn addition to these valuable developer experiences, I gained some personal insights into participating in open source. To inspire other students interested in joining open-source communities, I made a simple summary:\\n\\n1. **Understand and Learn Community Culture and Values:** Every open-source community has different cultures and values. Understanding a community\'s culture and values is crucial for successful participation. Observing and understanding the daily development and communication styles of other community members is a good way to learn community culture. Respect others\' opinions and embrace different viewpoints in the community.\\n\\n2. **Dare to Take the First Step:** Don\'t be afraid of challenges; taking the first step is key to participating in open-source communities. You can start by tackling issues labeled \\"good-first-issue\\" or by contributing to documentation, unit tests, etc. Overcoming the fear of difficulties, actively trying, and learning are crucial.\\n\\n3. **Have Confidence in Your Work:** Don\'t doubt your abilities. Everyone starts from scratch, and no one is born an expert. Participating in open-source communities is a process of learning and growth that requires continuous practice and experience accumulation.\\n\\n4. **Actively Participate in Discussions, Keep Learning Different Technologies:** Don\'t hesitate to ask questions, whether about specific project technologies or challenges in the development process. Also, don\'t limit yourself to one domain. Try to learn and master different programming languages, frameworks, and tools. This broadens your technical perspective and provides valuable insights for the project.\\n\\n---\\n\\nThrough my open-source journey, I accumulated valuable experience and skills. This not only helped me grow into a more valuable developer but also gave me a profound understanding of the power of open-source communities. However, I am not just an individual participant; I represent a part of the Seata community. Seata, as a continuously growing and evolving open-source project, has tremendous potential and faces new challenges. Therefore, I want to emphasize the importance of the Seata community and its future potential. It has entered the incubation stage of the Apache Software Foundation, a significant milestone that will bring broader development opportunities for Seata. Seata welcomes more developers and contributors to join us. Let\'s work together to drive the development of this open-source project and contribute to the advancement of the distributed transaction field."},{"id":"/seata-raft-detailed-explanation","metadata":{"permalink":"/blog/seata-raft-detailed-explanation","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-raft-detailed-explanation.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-raft-detailed-explanation.md","title":"Seata-Raft Storage Mode in Depth and Getting Started","description":"From traditional storage and computing separation to integrated storage and computing relying on distributed consensus algorithms to ensure transaction data consistency under high availability mode, what changes has Seata 2.x made? This article will provide a detailed introduction to the architecture and performance comparison.","date":"2023-10-13T00:00:00.000Z","formattedDate":"October 13, 2023","tags":[],"readingTime":16.765,"hasTruncateMarker":false,"authors":[{"name":"funkye"}],"frontMatter":{"title":"Seata-Raft Storage Mode in Depth and Getting Started","description":"From traditional storage and computing separation to integrated storage and computing relying on distributed consensus algorithms to ensure transaction data consistency under high availability mode, what changes has Seata 2.x made? This article will provide a detailed introduction to the architecture and performance comparison.","keywords":["fescar","seata","distributed transactions","raft"],"author":"funkye","date":"2023/10/13"},"prevItem":{"title":"Exploring the Journey of Open Source Development in Seata Project","permalink":"/blog/explore-seata-journey"},"nextItem":{"title":"Seata:Bridging Data and Applications","permalink":"/blog/seata-connect-data-and-application"}},"content":"- [1. Overview](#)\\n- [2. Architecture Introduction](#)\\n- [3. Deployment and Usage](#)\\n- [4. Benchmark Comparison](#)\\n- [5. Conclusion](#)\\n\\n# 1. Overview\\n\\nSeata is an open-source distributed transaction solution with over 24000 stars and a highly active community. It is dedicated to providing high-performance and user-friendly distributed transaction services in microservices architecture.\\n\\nCurrently, Seata\'s distributed transaction data storage modes include file, db, and redis. This article focuses on the architecture, deployment and usage, benchmark comparison of Seata-Server Raft mode. It explores why Seata needs Raft and provides insights into the process from research and comparison to design, implementation, and knowledge accumulation.\\n\\nPresenter: Jianbin  Chen(funkye) github id: [funky-eyes](https://github.com/funky-eyes)\\n\\n# 2. Architecture Introduction\\n\\n## 2.1 What is Raft Mode?\\n\\nFirstly, it is essential to understand what the Raft distributed consensus algorithm is. The following excerpt is a direct quote from the official documentation of sofa-jraft:\\n\\n```\\nRAFT is a novel and easy-to-understand distributed consensus replication protocol proposed by Diego Ongaro and John Ousterhout at Stanford University. It serves as the central coordination component in the RAMCloud project. Raft is a Leader-Based variant of Multi-Paxos, providing a more complete and clear protocol description compared to protocols like Paxos, Zab, View Stamped Replication. It also offers clear descriptions of node addition and deletion. As a replication state machine, Raft is the most fundamental component in distributed systems, ensuring ordered replication and execution of commands among multiple nodes, guaranteeing consistency when the initial states of multiple nodes are consistent.\\n\\nIn summary, Seata\'s Raft mode is based on the Sofa-Jraft component, implementing the ability to ensure the data consistency and high availability of Seata-Server itself.\\n\\n```\\n## 2.2 Why Raft Mode is Needed\\n\\nAfter understanding the definition of Seata-Raft mode, you might wonder whether Seata-Server is now unable to ensure consistency and high availability. Let\'s explore how Seata-Server currently achieves this from the perspectives of consistency and high availability.\\n\\n### 2.2.1 Existing Storage Modes\\n\\nIn the current Seata design, the role of the Server is to ensure the correct execution of the two-phase commit for transactions. However, this depends on the correct storage of transaction records. To ensure that transaction records are not lost, it is necessary to drive all Seata-RM instances to perform the correct two-phase commit behavior while maintaining correct state. So, how does Seata currently store transaction states and records?\\n\\nFirstly, let\'s introduce the three transaction storage modes supported by Seata: file, db, and redis. In terms of consistency ranking, the db mode provides the best guarantee for transaction records, followed by the asynchronous flushing of the file mode, and finally the aof and rdb modes of redis.\\n\\nTo elaborate:\\n\\n- The file mode is Seata\'s self-implemented transaction storage method. It stores transaction information on the local disk in a sequential write manner. For performance considerations, it defaults to asynchronous mode and stores transaction information in memory to ensure consistency between memory and disk data. In the event of Seata-Server (TC) unexpected crash, it reads transaction information from the disk upon restarting and restores it to memory for the continuation of transaction contexts.\\n\\n- The db mode is another implementation of Seata\'s abstract transaction storage manager (AbstractTransactionStoreManager). It relies on databases such as PostgreSQL, MySQL, Oracle, etc., to perform transaction information operations. Consistency is guaranteed by the local transactions of the database, and data persistence is the responsibility of the database.\\n\\n- Redis, similar to db, is a transaction storage method using Jedis and Lua scripts. It performs transaction operations using Lua scripts, and in Seata 2.x, all operations (such as lock competition) are handled using Lua scripts. Data storage is similar to db, relying on the storage side (Redis) to ensure data consistency. Like db, redis adopts a computation and storage separation architecture design in Seata.\\n\\n\\n### 2.2.2 High Availability\\n\\nHigh availability is simply the ability of a cluster to continue running normally after the main node crashes. The common approach is to deploy multiple nodes providing the same service and use a registry center to real-time sense the online and offline status of the main node for timely switching to an available node.\\n\\nIt may seem that deploying a few more machines is all that\'s needed. However, there is a problem behind it \u2013 how to ensure that multiple nodes operate as a whole. If one node crashes, another node can seamlessly take over the work of the crashed node, including handling the data of the crashed node. The answer to solving this problem is simple: in a computation and storage separation architecture, store data in a shared middleware. Any node can access this shared storage area to obtain transaction information for all nodes\' operations, thus achieving high availability.\\n\\nHowever, the prerequisite is that computation and storage must be separated. Why is the integration of computation and storage not feasible? This brings us to the implementation of the File mode. As described earlier, the File mode stores data on local disks and node memory, with no synchronization in data writing operations. This means that the current File mode cannot achieve high availability and only supports single-machine deployment. For basic quick start and simple use, the File mode has lower applicability, and the high-performance, memory-based File mode is practically no longer used in production environments.\\n\\n## 2.3 How is Seata-Raft Designed?\\n\\n### 2.3.1 Design Principles\\n\\nThe design philosophy of Seata-Raft mode is to encapsulate the File mode, which is unable to achieve high availability, and use the Raft algorithm to synchronize data between multiple TCs. This mode ensures data consistency among multiple TCs when using the File mode and replaces asynchronous flushing operations with Raft logs and snapshots for data recovery.\\n\\n![flow](https://blog.funkye.icu/img/blog/Dingtalk_20230105203431.jpg)\\n\\nIn the Seata-Raft mode, the client-side, upon startup, retrieves its transaction group (e.g., default) and the IP addresses of relevant Raft cluster nodes from the configuration center. By sending a request to the control port of Seata-Server, the client can obtain metadata for the Raft cluster corresponding to the default group, including leader, follower, and learner member nodes. Subsequently, the client monitors (watches) any member nodes of non-leader nodes.\\n\\nAssuming that TM initiates a transaction, and the leader node in the local metadata points to the address of TC1, TM will only interact with TC1. When TC1 adds global transaction information, through the Raft protocol, denoted as step 1 in the diagram, TC1 sends the log to other nodes. Step 2 represents the response of follower nodes to log reception. When more than half of the nodes (such as TC2) accept and respond successfully, the state machine (FSM) on TC1 will execute the action of adding a global transaction.\\n\\n![watch](https://blog.funkye.icu/img/blog/Dingtalk_20230105204423.jpg)\\n![watch2](https://blog.funkye.icu/img/blog/Dingtalk_20230105211035.jpg)\\n\\nIf TC1 crashes or a reelection occurs, what happens? Since the metadata has been obtained during the initial startup, the client will execute the watch follower node\'s interface to update the local metadata information. Therefore, subsequent transaction requests will be sent to the new leader (e.g., TC2). Meanwhile, TC1\'s data has already been synchronized to TC2 and TC3, ensuring data consistency. Only at the moment of the election, if a transaction happens to be sent to the old leader, it will be actively rolled back to ensure data correctness.\\n\\nIt is important to note that in this mode, if a transaction is in the phase of sending resolution requests or the one-phase process has not yet completed at the moment of the election, and it happens exactly during the election, these transactions will be actively rolled back. This is because the RPC node has crashed or a reelection has occurred, and there is currently no implemented RPC retry. The TM side has a default retry mechanism of 5 times, but due to the approximately 1s-2s time required for the election, transactions in the \'begin\' state may not successfully resolve, so they are prioritized for rollback to release locks, avoiding impacting the correctness of other business.\\n\\n### 2.3.2 Fault Recovery\\n\\nIn Seata, when a TC experiences a failure, the data recovery process is as follows:\\n\\n![recover](https://blog.funkye.icu/img/blog/Dingtalk_20230106231817.jpg)\\n\\nAs shown in the above diagram:\\n\\n- Check for the Latest Data Snapshot: Firstly, the system checks for the existence of the latest data snapshot file. The data snapshot is a one-time full copy of the in-memory data state. If there is a recent data snapshot, the system directly loads it into memory.\\n\\n- Replay Based on Raft Logs After Snapshot: If there is the latest snapshot or no snapshot file, the system replays the data based on the previously recorded Raft logs. Each request in Seata-Server ultimately goes through the ServerOnRequestProcessor for processing, then moves to the specific coordinator class (DefaultCoordinator or RaftCoordinator), and further proceeds to the specific business code (DefaultCore) for the corresponding transaction processing (e.g., begin, commit, rollback).\\n\\n- After the log replay is complete, the leader initiates log synchronization and continues to execute the related transaction\'s add, delete, and modify actions.\\n\\nThrough these steps, Seata can achieve data recovery after a failure. It first attempts to load the latest snapshot, if available, to reduce replay time. Then, it replays based on Raft logs to ensure the consistency of data operations. Finally, through the log synchronization mechanism, it ensures data consistency among multiple nodes.\\n\\n### 2.3.3 Business Processing Synchronization Process\\n\\n![flow](https://blog.funkye.icu/img/blog/Dingtalk_20230106230931.jpg)\\nFor the case where the client side is obtaining the latest metadata while a business thread is executing operations such as begin, commit, or registry, Seata adopts the following handling:\\n\\n- On the client side:\\n\\n    - If the client is executing operations like begin, commit, or registry, and at this moment, it needs to obtain the latest metadata, the RPC request from the client might fail since the leader may no longer exist or is not the current leader. \\n    - If the request fails, the client receives an exception response, and in this case, the client needs to roll back based on the request result.\\n\\n- TC side for detecting the old leader:\\n\\n    - On the TC side, if the client\'s request reaches the old leader node, TC checks if it is the current leader. If it is not the leader, it rejects the request.\\n    - If it is the leader but fails midway, such as failing during the process of submitting a task to the state machine, the creation of the task (createTask) fails due to the current state not being the leader. In this case, the client also receives a response with an exception.\\n    - The old leader\'s task submission also fails, ensuring the consistency of transaction information.\\n\\nThrough the above handling, when the client obtains the latest metadata while a business operation is in progress, Seata ensures data consistency and transaction correctness. If the client\'s RPC request fails, it triggers a rollback operation. On the TC side, detection of the old leader and the failure of task submission prevent inconsistencies in transaction information. This way, the client\'s data can also maintain consistency.\\n\\n## 3. Usage and Deployment\\nIn terms of usage and deployment, the community adheres to the principles of minimal intrusion and minimal changes. Therefore, the overall deployment should be straightforward. The following sections introduce deployment changes separately for the client and server sides.\\n\\n### 3.1 Client\\nFirstly, those familiar with the use of registry configuration centers should be aware of the `seata.registry.type` configuration item in Seata\'s configuration, supporting options like Nacos, ZooKeeper, etcd, Redis, etc. After version 2.0, a configuration item for Raft was added.\\n\\n```\\n   registry:\\n      type: raft\\n      raft:\\n         server-addr: 192.168.0.111:7091, 192.168.0.112:7091, 192.168.0.113:7091\\n```\\nSwitch the `registry.type` to \'raft\' and configure the address for obtaining Raft-related metadata, which is unified as the IP of the seata-server + HTTP port. Then, it is essential to configure the traditional transaction group.\\n\\n```\\nseata:\\n   tx-service-group: default_tx_group\\n   service:\\n      vgroup-mapping:\\n         default_tx_group: default\\n```\\nIf the current transaction group used is `default_tx_group`, then the corresponding Seata cluster/group is \'default\'. There is a corresponding relationship, and this will be further explained in the server deployment section.\\nWith this, the changes on the client side are complete.\\n\\n### 3.2 Server\\nFor server-side changes, there might be more adjustments, involving familiarity with some tuning parameters and configurations. Of course, default values can be used without any modifications.\\n\\n```\\nseata:\\n  server:\\n    raft:\\n      group: default # This value represents the group of this raft cluster, and the value corresponding to the client\'s transaction group should match it.\\n      server-addr: 192.168.0.111:9091,192.168.0.112:9091,192.168.0.113:9091 # IP and port of the 3 nodes, the port is the netty port of the node + 1000, default netty port is 8091\\n      snapshot-interval: 600 # Take a snapshot every 600 seconds for fast rolling of raftlog. However, making a snapshot every 600 seconds may cause business response time jitter if there is too much transaction data in memory. But it is friendly for fault recovery and faster node restart. You can adjust it to 30 minutes, 1 hour, etc., according to the business. You can test whether there is jitter on your own, and find a balance point between rt jitter and fault recovery.\\n      apply-batch: 32 # At most, submit raftlog once for 32 batches of actions\\n      max-append-bufferSize: 262144 # Maximum size of the log storage buffer, default is 256K\\n      max-replicator-inflight-msgs: 256 # In the case of enabling pipeline requests, the maximum number of in-flight requests, default is 256\\n      disruptor-buffer-size: 16384 # Internal disruptor buffer size. If it is a scenario with high write throughput, you need to appropriately increase this value. Default is 16384\\n      election-timeout-ms: 1000 # How long without a leader\'s heartbeat to start a new election\\n      reporter-enabled: false # Whether the monitoring of raft itself is enabled\\n      reporter-initial-delay: 60 # Interval of monitoring\\n      serialization: jackson # Serialization method, do not change\\n      compressor: none # Compression method for raftlog, such as gzip, zstd, etc.\\n      sync: true # Flushing method for raft log, default is synchronous flushing\\n  config:\\n    # support: nacos, consul, apollo, zk, etcd3\\n    type: file # This configuration can choose different configuration centers\\n  registry:\\n    # support: nacos, eureka, redis, zk, consul, etcd3, sofa\\n    type: file # Non-file registration center is not allowed in raft mode\\n  store:\\n    # support: file, db, redis, raft\\n    mode: raft # Use raft storage mode\\n    file:\\n      dir: sessionStore # This path is the storage location of raftlog and related transaction logs, default is relative path, it is better to set a fixed location\\n```\\nIn 3 or more nodes of seata-server, after configuring the above parameters, you can directly start it, and you will see similar log output, which means the cluster has started successfully:\\n\\n```\\n2023-10-13 17:20:06.392  WARN --- [Rpc-netty-server-worker-10-thread-1] [com.alipay.sofa.jraft.rpc.impl.BoltRaftRpcFactory] [ensurePipeline] []: JRaft SET bolt.rpc.dispatch-msg-list-in-default-executor to be false for replicator pipeline optimistic.\\n2023-10-13 17:20:06.439  INFO --- [default/PeerPair[10.58.16.231:9091 -> 10.58.12.217:9091]-AppendEntriesThread0] [com.alipay.sofa.jraft.storage.impl.LocalRaftMetaStorage] [save] []: Save raft meta, path=sessionStore/raft/9091/default/raft_meta, term=4, votedFor=0.0.0.0:0, cost time=25 ms\\n2023-10-13 17:20:06.441  WARN --- [default/PeerPair[10.58.16.231:9091 -> 10.58.12.217:9091]-AppendEntriesThread0] [com.alipay.sofa.jraft.core.NodeImpl] [handleAppendEntriesRequest] []: Node <default/10.58.16.231:9091> reject term_unmatched AppendEntriesRequest from 10.58.12.217:9091, term=4, prevLogIndex=4, prevLogTerm=4, localPrevLogTerm=0, lastLogIndex=0, entriesSize=0.\\n2023-10-13 17:20:06.442  INFO --- [JRaft-FSMCaller-Disruptor-0] [io.seata.server.cluster.raft.RaftStateMachine] [onStartFollowing] []: groupId: default, onStartFollowing: LeaderChangeContext [leaderId=10.58.12.217:9091, term=4, status=Status[ENEWLEADER<10011>: Raft node receives message from new leader with higher term.]].\\n2023-10-13 17:20:06.449  WARN --- [default/PeerPair[10.58.16.231:9091 -> 10.58.12.217:9091]-AppendEntriesThread0] [com.alipay.sofa.jraft.core.NodeImpl] [handleAppendEntriesRequest] []: Node <default/10.58.16.231:9091> reject term_unmatched AppendEntriesRequest from 10.58.12.217:9091, term=4, prevLogIndex=4, prevLogTerm=4, localPrevLogTerm=0, lastLogIndex=0, entriesSize=0.\\n2023-10-13 17:20:06.459  INFO --- [Bolt-default-executor-4-thread-1] [com.alipay.sofa.jraft.core.NodeImpl] [handleInstallSnapshot] []: Node <default/10.58.16.231:9091> received InstallSnapshotRequest from 10.58.12.217:9091, lastIncludedLogIndex=4, lastIncludedLogTerm=4, lastLogId=LogId [index=0, term=0].\\n2023-10-13 17:20:06.489  INFO --- [Bolt-conn-event-executor-13-thread-1] [com.alipay.sofa.jraft.rpc.impl.core.ClientServiceConnectionEventProcessor] [onEvent] []: Peer 10.58.12.217:9091 is connected\\n2023-10-13 17:20:06.519  INFO --- [JRaft-Group-Default-Executor-0] [com.alipay.sofa.jraft.util.Recyclers] [<clinit>] []: -Djraft.recyclers.maxCapacityPerThread: 4096.\\n2023-10-13 17:20:06.574  INFO --- [JRaft-Group-Default-Executor-0] [com.alipay.sofa.jraft.storage.snapshot.local.LocalSnapshotStorage] [destroySnapshot] []: Deleting snapshot sessionStore/raft/9091/default/snapshot/snapshot_4.\\n2023-10-13 17:20:06.574  INFO --- [JRaft-Group-Default-Executor-0] [com.alipay.sofa.jraft.storage.snapshot.local.LocalSnapshotStorage] [close] []: Renaming sessionStore/raft/9091/default/snapshot/temp to sessionStore/raft/9091/default/snapshot/snapshot_4.\\n2023-10-13 17:20:06.689  INFO --- [JRaft-FSMCaller-Disruptor-0] [io.seata.server.cluster.raft.snapshot.session.SessionSnapshotFile] [load] []: on snapshot load start index: 4\\n2023-10-13 17:20:06.694  INFO --- [JRaft-FSMCaller-Disruptor-0] [io.seata.server.cluster.raft.snapshot.session.SessionSnapshotFile] [load] []: on snapshot load end index: 4\\n2023-10-13 17:20:06.694  INFO --- [JRaft-FSMCaller-Disruptor-0] [io.seata.server.cluster.raft.RaftStateMachine] [onSnapshotLoad] []: groupId: default, onSnapshotLoad cost: 110 ms.\\n2023-10-13 17:20:06.694  INFO --- [JRaft-FSMCaller-Disruptor-0] [io.seata.server.cluster.raft.RaftStateMachine] [onConfigurationCommitted] []: groupId: default, onConfigurationCommitted: 10.58.12.165:9091,10.58.12.217:9091,10.58.16.231:9091.\\n2023-10-13 17:20:06.705  INFO --- [JRaft-FSMCaller-Disruptor-0] [com.alipay.sofa.jraft.storage.snapshot.SnapshotExecutorImpl] [onSnapshotLoadDone] []: Node <default/10.58.16.231:9091> onSnapshotLoadDone, last_included_index: 4\\nlast_included_term: 4\\npeers: \\"10.58.12.165:9091\\"\\npeers: \\"10.58.12.217:9091\\"\\npeers: \\"10.58.16.231:9091\\"\\n\\n2023-10-13 17:20:06.722  INFO --- [JRaft-Group-Default-Executor-1] [com.alipay.sofa.jraft.storage.impl.RocksDBLogStorage] [lambda$truncatePrefixInBackground$2] []: Truncated prefix logs in data path: sessionStore/raft/9091/default/log from log index 1 to 5, cost 0 ms.\\n```\\n### 3.3 faq\\n- Once the `seata.raft.server-addr` is configured, cluster scaling or shrinking must be done through the server\'s openapi. Directly changing this configuration and restarting won\'t take effect. The API for this operation is `/metadata/v1/changeCluster?raftClusterStr=new_cluster_list`.\\n\\n- If the addresses in `server-addr:` are all on the local machine, you need to add a 1000 offset to the netty ports of different servers on the local machine. For example, if `server.port: 7092`, the netty port will be 8092, and the raft election and communication port will be 9092. You need to add the startup parameter `-Dserver.raftPort=9092`. On Linux, this can be specified using `export JAVA_OPT=\\"-Dserver.raftPort=9092\\"`.\\n\\n\\n## 4. Performance Test Comparison\\n\\nPerformance testing is divided into two scenarios. To avoid data hotspots and thread optimization, the client side initializes 3 million items and uses jdk21 virtual threads + Spring Boot3 + Seata AT for testing. Garbage collection is handled with the ZGC generational garbage collector. The testing tool used is Alibaba Cloud PTS. Server-side is uniformly configured with jdk21 (not yet adapted for virtual threads). Server configurations are as follows:\\n- TC: 4c8g * 3\\n- Client: 4c * 8G * 1\\n- Database: Alibaba Cloud RDS 4c16g\\n\\n- 64 concurrent performance test only increases the performance of the `@GlobalTransactional` annotated interface with empty submissions.\\n- Random 3 million data items are used for inventory deduction in a 32 concurrent scenario for 10 minutes.\\n\\n### 4.1 1.7.1 db mode\\n![raft pressure test model](https://img.alicdn.com/imgextra/i3/O1CN011dNh3H1UK8G5prQAg_!!6000000002498-0-tps-731-333.jpg)\\n\\n#### Empty submission 64C\\n![db64-2](https://img.alicdn.com/imgextra/i2/O1CN01pE1Anf1nRtgcnlx9t_!!6000000005087-0-tps-622-852.jpg)\\n\\n#### Random inventory deduction 32C\\n![db32-2](https://img.alicdn.com/imgextra/i2/O1CN016hZkJC20OJax9ce31_!!6000000006839-0-tps-624-852.jpg)\\n\\n### 4.2 2.0 raft mode\\n![raft pressure test model](https://img.alicdn.com/imgextra/i2/O1CN01nNL6oe1X95YcQQEjs_!!6000000002880-0-tps-773-353.jpg)\\n\\n#### Empty submission 64C\\n![raft64-2](https://img.alicdn.com/imgextra/i1/O1CN01rs1ykr1dhnH8qnXj3_!!6000000003768-0-tps-631-851.jpg)\\n\\n#### Random inventory deduction 32C\\n![raft32c-2](https://img.alicdn.com/imgextra/i4/O1CN015OwA2k20enquV7Yfu_!!6000000006875-0-tps-624-856.jpg)\\n\\n### 4.3 Test Result Comparison\\n32 concurrent random inventory deduction scenario with 3 million items\\n\\n| tps avg | tps max | count | rt | error | Storage Type |\\n| --- | --- | --- | --- | --- | --- |\\n| 1709 (42%\u2191) | 2019 (21%\u2191) | 1228803 (42%\u2191) | 13.86ms (30%\u2193) | 0 | Raft |\\n| 1201 | 1668 | 864105 | 19.86ms | 0 | DB |\\n\\n64 concurrent empty pressure on `@GlobalTransactional` interface (test peak limit is 8000)\\n\\n| tps avg | tps max | count | rt | error | Storage Type |\\n| --- | --- | --- | --- | --- | --- |\\n| 5704 (20%\u2191) | 8062 (30%\u2191) | 4101236 (20%\u2191) | 7.79ms (19%\u2193) | 0 | Raft |\\n| 4743 | 6172 | 3410240 | 9.65ms | 0 | DB |\\n\\nIn addition to the direct comparison of the above data, by observing the curves of the pressure test, it can be seen that under the raft mode, TPS and RT are more stable, with less jitter, and better performance and throughput.\\n\\n\\n## 5. Summary\\n\\nIn the future development of Seata, performance, entry threshold, and deployment and operation costs are directions that we need to pay attention to and continuously optimize. After the introduction of the raft mode, Seata has the following characteristics:\\n\\n1. In terms of storage, after the separation of storage and computation, Seata\'s upper limit for optimization has been raised, making it more self-controlled.\\n2. Lower deployment costs, no need for additional registration centers, storage middleware.\\n3. Lower entry threshold, no need to learn other knowledge such as registration centers; one can directly use Seata Raft.\\n\\nIn response to industry trends, some open-source projects such as ClickHouse and Kafka have started to abandon the use of ZooKeeper and instead adopt self-developed solutions, such as ClickKeeper and KRaft. These solutions ensure the storage of metadata and other information by themselves, reducing the need for third-party dependencies, thus reducing operational and learning costs. These features are mature and worth learning from.\\n\\nOf course, currently, solutions based on the Raft mode may not be mature enough and may not fully meet the beautiful descriptions above. However, precisely because of such theoretical foundations, the community should strive in this direction, gradually bringing practice closer to the theoretical requirements. Here, all students interested in Seata are welcome to join the community, contributing to the development of Seata!"},{"id":"/seata-connect-data-and-application","metadata":{"permalink":"/blog/seata-connect-data-and-application","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-connect-data-and-application.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-connect-data-and-application.md","title":"Seata:Bridging Data and Applications","description":"This article introduces the past, present, and future evolution of Seata.","date":"2023-06-30T00:00:00.000Z","formattedDate":"June 30, 2023","tags":[],"readingTime":13.905,"hasTruncateMarker":false,"authors":[{"name":"Ji Min - Founder of the Seata Open Source Community, Leader of the Distributed Transactions Team"}],"frontMatter":{"title":"Seata:Bridging Data and Applications","keywords":["Seata","Distributed Transactions","Data Consistency","Microservices"],"description":"This article introduces the past, present, and future evolution of Seata.","author":"Ji Min - Founder of the Seata Open Source Community, Leader of the Distributed Transactions Team","date":"June 30, 2023"},"prevItem":{"title":"Seata-Raft Storage Mode in Depth and Getting Started","permalink":"/blog/seata-raft-detailed-explanation"},"nextItem":{"title":"Observability Practices in Seata","permalink":"/blog/seata-observable-practice"}},"content":"This article mainly introduces the evolutionary journey of distributed transactions from internal development to commercialization and open source, as well as the current progress and future planning of the Seata community.\\nSeata is an open-source distributed transaction solution designed to provide a comprehensive solution for distributed transactions under modern microservices architecture. Seata offers complete distributed transaction solutions, including AT, TCC, Saga, and XA transaction modes, supporting various programming languages and data storage schemes. Seata also provides easy-to-use APIs, extensive documentation, and examples to facilitate quick development and deployment for enterprises applying Seata.\\n**Seata\'s advantages lie in its high availability, high performance, and high scalability, and it does not require extra complex operations for horizontal scaling.** Seata is currently used in thousands of customer business systems on Alibaba Cloud, and its reliability has been recognized and applied by major industry manufacturers.\\nAs an open-source project, the Seata community is also expanding continuously, becoming an important platform for developers to exchange, share, and learn, attracting more and more attention and support from enterprises.\\nToday, I will primarily share about Seata on the following three topics:\\n- **From TXC/GTS to Seata**\\n- **Latest developments in the Seata community**\\n- **Future planning for the Seata community**\\n  <br/>\\n### From TXC/GTS to Seata\\n#### The Origin of Distributed Transactions\\n![Product Matrix](/img/blog/\u4ea7\u54c1\u77e9\u9635.jpg)\\nSeata is internally codenamed TXC (taobao transaction constructor) within Alibaba, a name with a strong organizational structure flavor. TXC originated from Alibaba\'s Wushi (Five Color Stones) project, which in ancient mythology were the stones used by the goddess N\xfcwa to mend the heavens, symbolizing Alibaba\'s important milestone in the evolution from monolithic architecture to distributed architecture. During this project, a batch of epoch-making Internet middleware was developed, including the well-known \\"Big Three\\":\\n- **HSF service invocation framework**\\n  Solves service communication issues after the transition from monolithic applications to service-oriented architectures.\\n- **TDDL database sharding framework**\\n  Addresses storage capacity and connection count issues of databases at scale.\\n- **MetaQ messaging framework**\\n  Addresses asynchronous invocation issues.\\n  The birth of the Big Three satisfied the basic requirements of microservices-based business development, but the data consistency issues that arose after microservices were not properly addressed, lacking a unified solution. The likelihood of data consistency issues in microservices is much higher than in monolithic applications, and the increased complexity of moving from in-process calls to network calls exacerbates the production of exceptional scenarios. The increase in service hops also makes it impossible for upstream and downstream services to coordinate data rollback in the event of a business processing exception. TXC was born to address the pain points of data consistency at the application architecture layer, and the core data consistency scenarios it aimed to address included:\\n- **Consistency across services.** Coordinates rollback of upstream and downstream service nodes in the event of system exceptions such as call timeouts and business exceptions.\\n- **Data consistency in database sharding.** Ensures internal transactions during logical SQL operations on business layers are consistent across different data shards.\\n- **Data consistency in message sending.** Addresses the inconsistency between data operations and successful message sending.\\n  To overcome the common scenarios encountered, TXC was seamlessly integrated with the Big Three. When businesses use the Big Three for development, they are completely unaware of TXC\'s presence in the background, do not have to consider the design of data consistency, and leave it to the framework to ensure, allowing businesses to focus more on their own development, greatly improving development efficiency.\\n  <br/>\\n  ![GTS Architecture](/img/blog/GTS\u67b6\u6784.jpg)\\n  TXC has been widely used within Alibaba Group for many years and has been baptized by the surging traffic of large-scale events like Singles\' Day, significantly improving business development efficiency and ensuring data accuracy, eliminating financial and reputational issues caused by data inconsistencies. With the continuous evolution of the architecture, **a standard three-node cluster can now handle peak values of nearly 100K TPS and millisecond-level transaction processing. In terms of availability and performance, it has reached a four-nines SLA guarantee, ensuring no failures throughout the year even in unattended conditions.**\\n  <br/>\\n#### The Evolution of Distributed Transactions\\nThe birth of new things is always accompanied by doubts. Is middleware capable of ensuring data consistency reliable? The initial birth of TXC was just a vague theory, lacking theoretical models and engineering practice. After we conducted MVP (Minimum Viable Product) model testing and promoted business deployment, we often encountered faults and frequently had to wake up in the middle of the night to deal with issues, wearing wristbands to sleep to cope with emergency responses. These were the most painful years I went through technically after taking over the team.\\n![Evolution of Distributed Transactions](/img/blog/\u5206\u5e03\u5f0f\u4e8b\u52a1\u6f14\u8fdb.jpg)\\nSubsequently, we had extensive discussions and systematic reviews. We first needed to define the consistency problem. Were we to achieve majority consensus consistency like RAFT, solve database consistency issues like Google Spanner, or something else? Looking at the top-down layered structure from the application node, it mainly includes development frameworks, service invocation frameworks, data middleware, database drivers, and databases. We had to decide at which layer to solve the data consistency problem. We compared the consistency requirements, universality, implementation complexity, and business integration costs faced when solving data consistency issues at different levels. In the end, we weighed the pros and cons, decided to keep the implementation complexity to ourselves, and adopted the AT mode initially as a consistency component. We needed to ensure high consistency, but not be locked into specific database implementations, ensuring the generality of scenarios and the business integration costs were low enough to be easily implemented. This is also why TXC initially adopted the AT mode.\\n**A distributed transaction is not just a framework; it\'s a system.** We defined the consistency problem in theory, abstractly conceptualized modes, roles, actions, and isolation, etc. From an engineering practice perspective, we defined the programming model, including low-intrusion annotations, simple method templates, and flexible APIs, and defined basic and enhanced transaction capabilities (e.g., how to support a large number of activities at low cost), as well as capabilities in operations, security, performance, observability, and high availability.\\n![Transaction Logical Model](/img/blog/\u4e8b\u52a1\u903b\u8f91\u6a21\u578b.jpg)\\nWhat problems do distributed transactions solve? A classic and tangible example is the money transfer scenario. The transfer process includes subtracting balance and adding balance, how do we ensure the atomicity of the operation? Without any intervention, these two steps may encounter various problems, such as account B being canceled or service call timeouts, etc.\\n**Timeout issues have always been a difficult problem to solve in distributed applications**; we cannot accurately know whether service B has executed and in what order. From a data perspective, this means the money in account B may not be successfully added. After the service-oriented transformation, each node only has partial information, while the transaction itself requires global coordination of all nodes, thus requiring a centralized role with a god\'s-eye view, capable of obtaining all information, which is the **TC (transaction coordinator)**, used to globally coordinate the transaction state. The **TM (Transaction Manager)** is the role that drives the generation of transaction proposals. However, even gods nod off, and their judgments are not always correct, so we need an **RM (resource manager)** role to verify the authenticity of the transaction as a representative of the soul. This is TXC\'s most basic philosophical model. We have methodologically verified that its data consistency is very complete, of course, our cognition is bounded. Perhaps the future will prove we were turkey engineers, but under current circumstances, its model is already sufficient to solve most existing problems.\\n![Distributed Transaction Performance](/img/blog/\u5206\u5e03\u5f0f\u4e8b\u52a1\u6027\u80fd.jpg)\\n**After years of architectural evolution, from the perspective of transaction single-link latency, TXC takes an average of about 0.2 milliseconds to process at the start of the transaction and about 0.4 milliseconds for branch registration, with the entire transaction\'s additional latency within the millisecond range. This is also the theoretical limit value we have calculated. In terms of throughput, the TPS of a single node reaches 30,000 times/second, and the TPS of a standard cluster is close to 100,000 times/second.**\\n<br/>\\n#### Seata Open Source\\nWhy go open source? This is a question many people have asked me. In 2017, we commercialized the GTS (Global Transaction Service) product sold on Alibaba Cloud, with both public and private cloud forms. At this time, the internal group developed smoothly, but we encountered various problems in the process of commercialization. The problems can be summed up in two main categories: **First, developers are quite lacking in the theory of distributed transactions,** most people do not even understand what local transactions are, let alone distributed transactions. **Second, there are problems with product maturity,** often encountering various strange scenario issues, leading to a sharp rise in support and delivery costs, and R&D turning into after-sales customer service.\\nWe reflected on why we encountered so many problems. The main issue here is that Alibaba Group internally has a unified language stack and unified technology stack, and our polishing of specific scenarios is very mature. Serving Alibaba, one company, and serving thousands of enterprises on the cloud is fundamentally different, which also made us realize that our product\'s scenario ecology was not well developed. On GitHub, more than 80% of open-source software is basic software, and basic software primarily solves the problem of scenario universality, so it cannot be locked in by a single enterprise, like Linux, which has a large number of community distributions. Therefore, in order to make our product better, we chose to open source and co-build with developers to popularize more enterprise users.\\n![Alibaba Open Source](/img/blog/\u963f\u91cc\u5f00\u6e90.jpg)\\nAlibaba\'s open-source journey has gone through three main stages. **The first stage is the stage where Dubbo is located, where developers contribute out of love,** Dubbo has been open sourced for over 10 years, and time has fully proven that Dubbo is an excellent open-source software, and its microkernel plugin extensibility design is an important reference for me when I initially open sourced Seata. When designing software, we need to consider which is more important between extensibility and performance, whether we are doing a three-year design, a five-year design, or a ten-year design that meets business development. While solving the 0-1 service call problem, can we predict the governance problems after the 1-100 scale-up?\\n**The second stage is the closed loop of open source and commercialization, where commercialization feeds back into the open-source community, promoting the development of the open-source community.** I think cloud manufacturers are more likely to do open source well for the following reasons:\\n- First, the cloud is a scaled economy, which must be established on a stable and mature kernel foundation, packaging its product capabilities including high availability, maintenance-free, and elasticity on top of it. An unstable kernel will inevitably lead to excessive delivery and support costs, and high penetration of the R&D team\'s support Q&A will prevent large-scale replication, and high penetration rates will prevent rapid evolution and iteration of products.\\n- Second, commercial products know business needs better. Our internal technical teams often YY requirements from a development perspective, and what they make is not used by anyone, and thus does not form a value conversion. The business requirements collected through commercialization are all real, so its open source kernel must also evolve in this direction. Failure to evolve in this direction will inevitably lead to architectural splits on both sides, increasing the team\'s maintenance costs.\\n- Finally, the closed loop of open source and commercialization can promote better development of both parties. If the open-source kernel often has various problems, would you believe that its commercial product is good enough?\\n  **The third stage is systematization and standardization.** First, systematization is the basis of open-source solutions. Alibaba\'s open-source projects are mostly born out of internal e-commerce scenario practices. For example, Higress is used to connect Ant Group\'s gateways; Nacos carries services with millions of instances and tens of millions of connections; Sentinel provides degradation and throttling capabilities for high availability during major promotions; and Seata ensures transaction data consistency. This set of systematized open-source solutions is designed based on the best practices of Alibaba\'s e-commerce ecosystem. Second, standardization is another important feature. Taking OpenSergo as an example, it is both a standard and an implementation. In the past few years, the number of domestic open-source projects has exploded. However, the capabilities of various open-source products vary greatly, and many compatibility issues arise when integrating with each other. Therefore, open-source projects like OpenSergo can define some standardized capabilities and interfaces and provide some implementations, which will greatly help the development of the entire open-source ecosystem.\\n  <br/>\\n### Latest Developments in the Seata Community\\n#### Introduction to the Seata Community\\n![Community Introduction](/img/blog/\u793e\u533a\u7b80\u4ecb.jpg)\\n**At present, Seata has open-sourced 4 transaction modes, including AT, TCC, Saga, and XA, and is actively exploring other viable transaction solutions.** Seata has integrated with more than 10 mainstream RPC frameworks and relational databases, and has integrated or been integrated relationships with more than 20 communities. In addition, we are also exploring languages other than Java in the multi-language system, such as Golang, PHP, Python, and JS.\\nSeata has been applied to business systems by thousands of customers. Seata applications have become more mature, with successful cooperation with the community in the financial business scenarios of CITIC Bank and Everbright Bank, and successfully adopted into core accounting systems. The landing of microservices systems in financial scenarios is very stringent, which also marks a new level of maturity for Seata\'s kernel.\\n<br/>\\n#### Seata Ecosystem Expansion\\n![Ecosystem Expansion](/img/blog/\u6269\u5c55\u751f\u6001.jpg)\\n**Seata adopts a microkernel and plugin architecture design, exposing rich extension points in APIs, registry configuration centers, storage modes, lock control, SQL parsers, load balancing, transport, protocol encoding and decoding, observability, and more.** This allows businesses to easily perform flexible extensions and select technical components.\\n<br/>\\n#### Seata Application Cases\\n![Application Cases](/img/blog/\u5e94\u7528\u6848\u4f8b.jpg)\\n**Case 1: China Aviation Information\'s Air Travel Project**\\nThe China Aviation Information Air Travel project introduced Seata in the 0.2 version to solve the data consistency problem of ticket and coupon business, greatly improving development efficiency, reducing asset losses caused by data inconsistency, and enhancing user interaction experience.\\n**Case 2: Didi Chuxing\'s Two-Wheeler Business Unit**\\nDidi Chuxing\'s Two-Wheeler Business Unit introduced Seata in version 0.6.1, solving the data consistency problem of business processes such as blue bicycles, electric vehicles, and assets, optimizing the user experience, and reducing asset loss.\\n**Case 3: Meituan\'s Infrastructure**\\nMeituan\'s infrastructure team developed the internal distributed transaction solution Swan based on the open-source Seata project, which is used to solve distributed transaction problems within Meituan\'s various businesses.\\n**Case 4: Hema Town**\\nHema Town uses Seata to control the flower-stealing process in game interactions, significantly shortening the development cycle from 20 days to 5 days, effectively reducing development costs.\\n<br/>\\n#### Evolution of Seata Transaction Modes\\n![Mode Evolution](/img/blog/\u6a21\u5f0f\u6f14\u8fdb.jpg)\\n<br/>\\n#### Current Progress of Seata\\n- Support for Oracle and PostgreSQL multi-primary keys.\\n- Support for Dubbo3.\\n- Support for Spring Boot3.\\n- Support for JDK 17.\\n- Support for ARM64 images.\\n- Support for multiple registration models.\\n- Extended support for various SQL syntaxes.\\n- Support for GraalVM Native Image.\\n- Support for Redis lua storage mode.\\n  <br/>\\n### Seata 2.x Development Planning\\n![Development Planning](/img/blog/\u53d1\u5c55\u89c4\u5212.jpg)\\nMainly includes the following aspects:\\n- **Storage/Protocol/Features**\\n  Explore storage and computing separation in Raft cluster mode; better experience, unify the current 4 transaction mode APIs; compatible with GTS protocol; support Saga annotations; support distributed lock control; support data perspective insight and governance.\\n- **Ecosystem**\\n  Support more databases, more service frameworks, while exploring support for the domestic trust creation ecosystem; support the MQ ecosystem; further enhance APM support.\\n- **Solutions**\\n  In addition to supporting microservices ecosystems, explore multi-cloud solutions; closer to cloud-native solutions; add security and traffic protection capabilities; achieve self-convergence of core components in the architecture.\\n- **Multi-Language Ecosystem**\\n  Java is the most mature in the multi-language ecosystem, continue to improve other supported programming languages, while exploring Transaction Mesh solutions that are independent of languages.\\n- **R&D Efficiency/Experience**\\n  Improve test coverage, prioritize quality, compatibility, and stability; restructure the official website\'s documentation to improve the hit rate of document searches; simplify operations and deployment on the experience side, achieve one-click installation and metadata simplification; console supports transaction control and online analysis capabilities.\\n\\nIn one sentence, the 2.x plan is summarized as: **Bigger scenarios, bigger ecosystems, from usable to user-friendly.**\\n<br/>\\n### Contact Information for the Seata Community\\n![Contact Information](/img/blog/\u8054\u7cfb\u65b9\u5f0f.jpg)"},{"id":"/seata-observable-practice","metadata":{"permalink":"/blog/seata-observable-practice","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-observable-practice.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-observable-practice.md","title":"Observability Practices in Seata","description":"This article explores and discusses Seata\'s practices in the field of observability.","date":"2023-06-25T00:00:00.000Z","formattedDate":"June 25, 2023","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"rong.liu-Seata"}],"frontMatter":{"title":"Observability Practices in Seata","keywords":["Seata","distributed transaction","data consistency","microservices","observability"],"description":"This article explores and discusses Seata\'s practices in the field of observability.","author":"rong.liu-Seata","date":"2023/06/25"},"prevItem":{"title":"Seata:Bridging Data and Applications","permalink":"/blog/seata-connect-data-and-application"},"nextItem":{"title":"seata-go 1.2.0 Ready for Production Environment!!!","permalink":"/blog/seata-go-1.2.0"}},"content":""},{"id":"/seata-go-1.2.0","metadata":{"permalink":"/blog/seata-go-1.2.0","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-go-1.2.0.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-go-1.2.0.md","title":"seata-go 1.2.0 Ready for Production Environment!!!","description":"seata-go 1.2.0, ready for production environment!!!","date":"2023-06-08T00:00:00.000Z","formattedDate":"June 8, 2023","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"Seata Community"}],"frontMatter":{"title":"seata-go 1.2.0 Ready for Production Environment!!!","author":"Seata Community","keywords":["seata","distributed transaction","golang","1.2.0"],"description":"seata-go 1.2.0, ready for production environment!!!","date":"2023/06/08"},"prevItem":{"title":"Observability Practices in Seata","permalink":"/blog/seata-observable-practice"},"nextItem":{"title":"6 Major Topics Now Open for Selection | Welcome to Apply for Seata Open Source Summer","permalink":"/blog/iscas2023"}},"content":""},{"id":"/iscas2023","metadata":{"permalink":"/blog/iscas2023","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/iscas2023.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/iscas2023.md","title":"6 Major Topics Now Open for Selection | Welcome to Apply for Seata Open Source Summer","description":"","date":"2023-05-12T00:00:00.000Z","formattedDate":"May 12, 2023","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"Seata Community"}],"frontMatter":{"title":"6 Major Topics Now Open for Selection | Welcome to Apply for Seata Open Source Summer","author":"Seata Community","date":"2023/05/12","keywords":["open source summer","seata","distributed transaction"]},"prevItem":{"title":"seata-go 1.2.0 Ready for Production Environment!!!","permalink":"/blog/seata-go-1.2.0"},"nextItem":{"title":"Seata 1.6.0 Released with Significant Performance Improvement","permalink":"/blog/seata-1.6.0"}},"content":""},{"id":"/seata-1.6.0","metadata":{"permalink":"/blog/seata-1.6.0","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-1.6.0.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-1.6.0.md","title":"Seata 1.6.0 Released with Significant Performance Improvement","description":"Seata 1.6.0 Released with Significant Performance Improvement","date":"2022-12-17T00:00:00.000Z","formattedDate":"December 17, 2022","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"Seata Community"}],"frontMatter":{"title":"Seata 1.6.0 Released with Significant Performance Improvement","author":"Seata Community","keywords":["seata","distributed transaction","1.6.0"],"description":"Seata 1.6.0 Released with Significant Performance Improvement","date":"2022/12/17"},"prevItem":{"title":"6 Major Topics Now Open for Selection | Welcome to Apply for Seata Open Source Summer","permalink":"/blog/iscas2023"},"nextItem":{"title":"Seata 1.5.2 Released with XID Load Balancing Support","permalink":"/blog/seata-1.5.2"}},"content":""},{"id":"/seata-1.5.2","metadata":{"permalink":"/blog/seata-1.5.2","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-1.5.2.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-1.5.2.md","title":"Seata 1.5.2 Released with XID Load Balancing Support","description":"Seata 1.5.2 Released with XID Load Balancing Support","date":"2022-07-12T00:00:00.000Z","formattedDate":"July 12, 2022","tags":[],"readingTime":2.295,"hasTruncateMarker":false,"authors":[{"name":"Seata Community"}],"frontMatter":{"title":"Seata 1.5.2 Released with XID Load Balancing Support","author":"Seata Community","keywords":["seata","distributed transaction","1.5.2"],"description":"Seata 1.5.2 Released with XID Load Balancing Support","date":"2022/07/12"},"prevItem":{"title":"Seata 1.6.0 Released with Significant Performance Improvement","permalink":"/blog/seata-1.6.0"},"nextItem":{"title":"Alibaba Seata Resolves Idempotence, Dangling, and Empty Rollback Issues in TCC Mode","permalink":"/blog/seata-tcc-fence"}},"content":"### Seata 1.5.2 Released with XID Load Balancing Support\\n\\nSeata is an open-source distributed transaction solution that provides high-performance and easy-to-use distributed transaction services.\\n\\n**seata-server Download Links:**\\n\\n[source](https://github.com/apache/incubator-seata/archive/v1.5.2.zip) |\\n[binary](https://github.com/apache/incubator-seata/releases/download/v1.5.2/seata-server-1.5.2.zip)\\n\\nThe key updates in this version include:\\n\\n### Features:\\n- [[#4661](https://github.com/apache/incubator-seata/pull/4713)] Added support for XID load balancing algorithm.\\n- [[#4676](https://github.com/apache/incubator-seata/pull/4676)] Added support for Seata server to expose services through SLB when using Nacos as the registry center.\\n- [[#4642](https://github.com/apache/incubator-seata/pull/4642)] Added support for parallel processing of client batch requests.\\n- [[#4567](https://github.com/apache/incubator-seata/pull/4567)] Added support for the `find_in_set` function in the WHERE condition.\\n\\n### Bug Fixes:\\n- [[#4515](https://github.com/apache/incubator-seata/pull/4515)] Fixed an issue where SeataTCCFenceAutoConfiguration on the develop branch throws a ClassNotFoundException when the client does not use a DB.\\n- [[#4661](https://github.com/apache/incubator-seata/pull/4661)] Fixed SQL exceptions when using PostgreSQL in the console.\\n- [[#4667](https://github.com/apache/incubator-seata/pull/4682)] Fixed an exception when updating the map in RedisTransactionStoreManager on the develop branch.\\n- [[#4678](https://github.com/apache/incubator-seata/pull/4678)] Fixed the issue of cache penetration when the property `transport.enableRmClientBatchSendRequest` is not configured.\\n- [[#4701](https://github.com/apache/incubator-seata/pull/4701)] Fixed the issue of missing command line parameters.\\n- [[#4607](https://github.com/apache/incubator-seata/pull/4607)] Fixed a defect in skipping global lock verification.\\n- [[#4696](https://github.com/apache/incubator-seata/pull/4696)] Fixed the insertion issue when using the Oracle storage mode.\\n- [[#4726](https://github.com/apache/incubator-seata/pull/4726)] Fixed a possible NPE issue when sending messages in batches.\\n- [[#4729](https://github.com/apache/incubator-seata/pull/4729)] Fixed the issue of incorrect setting of `AspectTransactional.rollbackForClassName`.\\n- [[#4653](https://github.com/apache/incubator-seata/pull/4653)] Fixed the exception of non-numeric primary key in INSERT_ON_DUPLICATE.\\n\\n### Optimizations:\\n- [[#4650](https://github.com/apache/incubator-seata/pull/4650)] Fixed a security vulnerability.\\n- [[#4670](https://github.com/apache/incubator-seata/pull/4670)] Optimized the number of threads in the `branchResultMessageExecutor` thread pool.\\n- [[#4662](https://github.com/apache/incubator-seata/pull/4662)] Optimized the monitoring metrics for rolling back transactions.\\n- [[#4693](https://github.com/apache/incubator-seata/pull/4693)] Optimized the console navigation bar.\\n- [[#4700](https://github.com/apache/incubator-seata/pull/4700)] Fixed failures in the execution of maven-compiler-plugin and maven-resources-plugin.\\n- [[#4711](https://github.com/apache/incubator-seata/pull/4711)] Separated the lib dependency during deployment.\\n- [[#4720](https://github.com/apache/incubator-seata/pull/4720)] Optimized pom descriptions.\\n- [[#4728](https://github.com/apache/incubator-seata/pull/4728)] Upgraded the logback version dependency to 1.2.9.\\n- [[#4745](https://github.com/apache/incubator-seata/pull/4745)] Added support for mysql8 driver in the distribution package.\\n- [[#4626](https://github.com/apache/incubator-seata/pull/4626)] Used `easyj-maven-plugin` plugin instead of `flatten-maven-plugin` to fix compatibility issues between `shade` plugin and `flatten` plugin.\\n- [[#4629](https://github.com/apache/incubator-seata/pull/4629)] Checked the constraint relationship before and after updating the globalSession status.\\n- [[#4662](https://github.com/apache/incubator-seata/pull/4662)] Optimized the readability of EnhancedServiceLoader.\\n\\n### Tests:\\n- [[#4544](https://github.com/apache/incubator-seata/pull/4544)] Optimized the jackson package dependency issue in TransactionContextFilterTest.\\n- [[#4731](https://github.com/apache/incubator-seata/pull/4731)] Fixed unit test issues in AsyncWorkerTest and LockManagerTest.\\n\\nA big thanks to the contributors for their valuable code contributions. If inadvertently omitted, please report.\\n\\n\\n\x3c!-- Make sure your GitHub ID is in the list below --\x3e\\n- [slievrly](https://github.com/slievrly)\\n- [pengten](https://github.com/pengten)\\n- [YSF-A](https://github.com/YSF-A)\\n- [tuwenlin](https://github.com/tuwenlin)\\n- [2129zxl](https://github.com/2129zxl)\\n- [Ifdevil](https://github.com/Ifdevil)\\n- [wingchi-leung](https://github.com/wingchi-leung)\\n- [liurong](https://github.com/robynron)\\n- [opelok-z](https://github.com/opelok-z)\\n- [funky-eyes](https://github.com/funky-eyes)\\n- [Smery-lxm](https://github.com/Smery-lxm)\\n- [lvekee](https://github.com/lvekee)\\n- [doubleDimple](https://github.com/doubleDimple)\\n- [wangliang181230](https://github.com/wangliang181230)\\n- [Bughue](https://github.com/Bughue)\\n- [AYue-94](https://github.com/AYue-94)\\n- [lingxiao-wu](https://github.com/lingxiao-wu)\\n- [caohdgege](https://github.com/caohdgege)\\n\\nAt the same time, we have received many valuable issues and suggestions from the community, thank you very much.\\n\\n#### Link\\n\\n- **Seata:** https://github.com/apache/incubator-seata\\n- **Seata-Samples:** https://github.com/apache/incubator-seata-samples\\n- **Release:** https://github.com/apache/incubator-seata/releases\\n- **WebSite:** https://seata.io"},{"id":"/seata-tcc-fence","metadata":{"permalink":"/blog/seata-tcc-fence","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-tcc-fence.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-tcc-fence.md","title":"Alibaba Seata Resolves Idempotence, Dangling, and Empty Rollback Issues in TCC Mode","description":"Seata version 1.5.1 from Alibaba has finally resolved the issues of idempotence, dangling, and empty rollback in TCC (Try-Confirm-Cancel) mode. This article mainly explains how Seata addresses these problems.","date":"2022-06-25T00:00:00.000Z","formattedDate":"June 25, 2022","tags":[],"readingTime":10.97,"hasTruncateMarker":false,"authors":[{"name":"Zhu Jinjun"}],"frontMatter":{"title":"Alibaba Seata Resolves Idempotence, Dangling, and Empty Rollback Issues in TCC Mode","author":"Zhu Jinjun","keywords":["Seata","TCC","idempotence","dangling","empty rollback"],"description":"Seata version 1.5.1 from Alibaba has finally resolved the issues of idempotence, dangling, and empty rollback in TCC (Try-Confirm-Cancel) mode. This article mainly explains how Seata addresses these problems.","date":"2022/06/25"},"prevItem":{"title":"Seata 1.5.2 Released with XID Load Balancing Support","permalink":"/blog/seata-1.5.2"},"nextItem":{"title":"In-Depth Analysis of Seata TCC Mode (1)","permalink":"/blog/seata-tcc"}},"content":"Today, let\'s talk about how the new version (1.5.1) of Alibaba\'s Seata resolves the issues of idempotence, dangling, and empty rollback in TCC mode.\\n\\n## 1 TCC \\n\\nTCC mode is the most classic solution for distributed transactions. It divides the distributed transaction into two phases. In the try phase, resources are reserved for each branch transaction. If all branch transactions successfully reserve resources, the global transaction proceeds to the commit phase for committing the transaction globally. However, if any node fails to reserve resources, the global transaction enters the cancel phase to rollback the transaction globally.\\n\\nTaking traditional order, inventory, and account services as an example, in the try phase, resources are attempted to be reserved by inserting orders, deducting inventory, and deducting amounts. These three services require local transaction commits, and the resources can be transferred to an intermediate table. In the commit phase, the resources reserved in the try phase are transferred to the final table. In the cancel phase, the resources reserved in the try phase are released, such as returning the account amount to the customer\'s account.\\n\\n**Note: The try phase must involve committing local transactions. For example, when deducting the order amount, the money must be deducted from the customer\'s account. If it is not deducted, there will be a problem in the commit phase if the customer\'s account does not have enough money.**\\n\\n### 1.1 try-commit\\n\\nIn the try phase, resources are first reserved, and then they are deducted in the commit phase. The diagram below illustrates this process:\\n\\n![fence-try-commit](/img/blog/fence-try-commit.png)\\n\\n\\n### 1.2 try-cancel\\n\\nIn the try phase, resources are first reserved. If the deduction of inventory fails, leading to a rollback of the global transaction, the resources are released in the cancel phase. The diagram below illustrates this process:\\n\\n![fence-try-cancel](/img/blog/fence-try-cancel.png)\\n\\n\\n## 2 TCC Advantages\\n\\nThe biggest advantage of TCC mode is its high efficiency. In the try phase, the resource locking in TCC mode is not a true lock, but rather a real local transaction submission that reserves resources in an intermediate state without the need for blocking and waiting. Therefore, it is more efficient than other modes.\\n\\nAdditionally, the TCC mode can be optimized as follows:\\n\\n### 2.1 Asynchronous Commit\\n\\nAfter the try phase succeeds, instead of immediately entering the confirm/cancel phase, it is considered that the global transaction has already ended. A scheduled task is started to asynchronously execute the confirm/cancel phase, which involves deducting or releasing resources. This approach can greatly improve performance.\\n\\n### 2.2 Same-Database Mode\\n\\nIn the TCC mode, there are three roles:\\n\\n- TM: Manages the global transaction, including starting the global transaction and committing/rolling back the global transaction.\\n- RM: Manages the branch transaction.\\n- TC: Manages the state of the global transaction and branch transactions.\\n\\nThe diagram below is from the Seata official website:\\n\\n![fence-fiffrent-db](/img/blog/fence-fiffrent-db.png)\\n\\nWhen TM starts a global transaction, RM needs to send a registration message to TC, and TC saves the state of the branch transaction. When TM requests a commit or rollback, TC needs to send commit or rollback messages to RM. In this way, in a distributed transaction with two branch transactions, there are four RPCs between TC and RM.\\n\\nAfter optimization, the process is as shown in the diagram below:\\n\\nTC saves the state of the global transaction. When TM starts a global transaction, RM no longer needs to send a registration message to TC. Instead, it saves the state of the branch transaction locally. After TM sends a commit or rollback message to TC, the asynchronous thread in RM first retrieves the uncommitted branch transactions saved locally, and then sends a message to TC to obtain the state of the global transaction in which the local branch transaction is located, in order to determine whether to commit or rollback the local transaction.\\n\\nWith this optimization, the number of RPCs is reduced by 50%, resulting in a significant performance improvement.\\n\\n## 3 RM Code Example\\n\\nTaking the inventory service as an example, the RM inventory service interface code is as follows:\\n```Java\\n@LocalTCC\\npublic interface StorageService {\\n\\n    /**\\n     * decrease\\n     * @param xid \\n     * @param productId \\n     * @param count \\n     * @return\\n     */\\n    @TwoPhaseBusinessAction(name = \\"storageApi\\", commitMethod = \\"commit\\", rollbackMethod = \\"rollback\\", useTCCFence = true)\\n    boolean decrease(String xid, Long productId, Integer count);\\n\\n    /**\\n     * commit\\n     * @param actionContext\\n     * @return\\n     */\\n    boolean commit(BusinessActionContext actionContext);\\n\\n    /**\\n     * rollback\\n     * @param actionContext\\n     * @return\\n     */\\n    boolean rollback(BusinessActionContext actionContext);\\n}\\n```\\nBy using the `@LocalTCC` annotation, when the RM is initialized, it registers a branch transaction with the TC. The `try` phase method (e.g., `decrease` method) is annotated with `@TwoPhaseBusinessAction`, which defines the branch transaction\'s `resourceId`, `commit` method, `cancel` method, and the `useTCCFence` property, which will be explained in the next section.\\n\\n## 4 Issues with TCC\\n\\nThere are three major issues with the TCC pattern: idempotence, suspension, and empty rollback. In version 1.5.1 of Seata, a transaction control table named `tcc_fence_log` is introduced to address these issues. The `useTCCFence` property mentioned in the previous `@TwoPhaseBusinessAction` annotation is used to enable or disable this mechanism, with a default value of `false`.\\n\\nThe creation SQL statement for the `tcc_fence_log` table (in MySQL syntax) is as follows:\\n\\n```SQL\\nCREATE TABLE IF NOT EXISTS `tcc_fence_log`\\n(\\n    `xid`           VARCHAR(128)  NOT NULL COMMENT \'global id\',\\n    `branch_id`     BIGINT        NOT NULL COMMENT \'branch id\',\\n    `action_name`   VARCHAR(64)   NOT NULL COMMENT \'action name\',\\n    `status`        TINYINT       NOT NULL COMMENT \'status(tried:1;committed:2;rollbacked:3;suspended:4)\',\\n    `gmt_create`    DATETIME(3)   NOT NULL COMMENT \'create time\',\\n    `gmt_modified`  DATETIME(3)   NOT NULL COMMENT \'update time\',\\n    PRIMARY KEY (`xid`, `branch_id`),\\n    KEY `idx_gmt_modified` (`gmt_modified`),\\n    KEY `idx_status` (`status`)\\n) ENGINE = InnoDB\\nDEFAULT CHARSET = utf8mb4;\\n```\\n\\n### 4.1 Idempotence\\n\\nDuring the commit/cancel phase, if the TC does not receive a response from the branch transaction, it needs to retry the operation. Therefore, it is necessary for the branch transaction to support idempotence.\\n\\nLet\'s take a look at how this is addressed in the new version. The following code is from the `TCCResourceManager` class:\\n\\n```Java\\n@Override\\npublic BranchStatus branchCommit(BranchType branchType, String xid, long branchId, String resourceId,\\n\\t\\t\\t\\t\\t\\t\\t\\t String applicationData) throws TransactionException {\\n\\tTCCResource tccResource = (TCCResource)tccResourceCache.get(resourceId);\\n\\tObject targetTCCBean = tccResource.getTargetBean();\\n\\tMethod commitMethod = tccResource.getCommitMethod();\\n\\ttry {\\n\\t\\t//BusinessActionContext\\n\\t\\tBusinessActionContext businessActionContext = getBusinessActionContext(xid, branchId, resourceId,\\n\\t\\t\\tapplicationData);\\n\\t\\tObject[] args = this.getTwoPhaseCommitArgs(tccResource, businessActionContext);\\n\\t\\tObject ret;\\n\\t\\tboolean result;\\n\\t\\t//whether the useTCCFence property is set to true\\n\\t\\tif (Boolean.TRUE.equals(businessActionContext.getActionContext(Constants.USE_TCC_FENCE))) {\\n\\t\\t\\ttry {\\n\\t\\t\\t\\tresult = TCCFenceHandler.commitFence(commitMethod, targetTCCBean, xid, branchId, args);\\n\\t\\t\\t} catch (SkipCallbackWrapperException | UndeclaredThrowableException e) {\\n\\t\\t\\t\\tthrow e.getCause();\\n\\t\\t\\t}\\n\\t\\t} else {\\n\\t\\t}\\n\\t\\tLOGGER.info(\\"TCC resource commit result : {}, xid: {}, branchId: {}, resourceId: {}\\", result, xid, branchId, resourceId);\\n\\t\\treturn result ? BranchStatus.PhaseTwo_Committed : BranchStatus.PhaseTwo_CommitFailed_Retryable;\\n\\t} catch (Throwable t) {\\n\\t\\treturn BranchStatus.PhaseTwo_CommitFailed_Retryable;\\n\\t}\\n}\\n```\\nThe above code shows that when executing the commit method of the branch transaction, it first checks if the `useTCCFence` property is `true`. If it is `true`, it follows the `commitFence` logic in the `TCCFenceHandler` class; otherwise, it follows the normal commit logic.\\n\\nThe `commitFence` method in the `TCCFenceHandler` class calls the `commitFence` method of the same class. The code is as follows:\\n\\n```Java\\npublic static boolean commitFence(Method commitMethod, Object targetTCCBean,\\n\\t\\t\\t\\t\\t\\t\\t\\t  String xid, Long branchId, Object[] args) {\\n\\treturn transactionTemplate.execute(status -> {\\n\\t\\ttry {\\n\\t\\t\\tConnection conn = DataSourceUtils.getConnection(dataSource);\\n\\t\\t\\tTCCFenceDO tccFenceDO = TCC_FENCE_DAO.queryTCCFenceDO(conn, xid, branchId);\\n\\t\\t\\tif (tccFenceDO == null) {\\n\\t\\t\\t\\tthrow new TCCFenceException(String.format(\\"TCC fence record not exists, commit fence method failed. xid= %s, branchId= %s\\", xid, branchId),\\n\\t\\t\\t\\t\\t\\tFrameworkErrorCode.RecordAlreadyExists);\\n\\t\\t\\t}\\n\\t\\t\\tif (TCCFenceConstant.STATUS_COMMITTED == tccFenceDO.getStatus()) {\\n\\t\\t\\t\\tLOGGER.info(\\"Branch transaction has already committed before. idempotency rejected. xid: {}, branchId: {}, status: {}\\", xid, branchId, tccFenceDO.getStatus());\\n\\t\\t\\t\\treturn true;\\n\\t\\t\\t}\\n\\t\\t\\tif (TCCFenceConstant.STATUS_ROLLBACKED == tccFenceDO.getStatus() || TCCFenceConstant.STATUS_SUSPENDED == tccFenceDO.getStatus()) {\\n\\t\\t\\t\\tif (LOGGER.isWarnEnabled()) {\\n\\t\\t\\t\\t\\tLOGGER.warn(\\"Branch transaction status is unexpected. xid: {}, branchId: {}, status: {}\\", xid, branchId, tccFenceDO.getStatus());\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\treturn false;\\n\\t\\t\\t}\\n\\t\\t\\treturn updateStatusAndInvokeTargetMethod(conn, commitMethod, targetTCCBean, xid, branchId, TCCFenceConstant.STATUS_COMMITTED, status, args);\\n\\t\\t} catch (Throwable t) {\\n\\t\\t\\tstatus.setRollbackOnly();\\n\\t\\t\\tthrow new SkipCallbackWrapperException(t);\\n\\t\\t}\\n\\t});\\n}\\n```\\nFrom the code, we can see that when committing the transaction, it first checks if there is a record in the `tcc_fence_log` table. If a record exists, it checks the transaction execution status and returns. This ensures idempotence by avoiding duplicate commits if the transaction status is already `STATUS_COMMITTED`. If there is no record in the `tcc_fence_log` table, a new record is inserted for later retry detection.\\n\\nThe rollback logic is similar to the commit logic and is implemented in the `rollbackFence` method of the `TCCFenceHandler` class.\\n\\n### 4.2 Empty Rollback\\n\\nIn the scenario shown in the following diagram, the account service consists of a cluster of two nodes. During the try phase, the account service on Node 1 encounters a failure. Without considering retries, the global transaction must reach the end state, requiring a cancel operation to be performed on the account service.\\n\\n![fence-empty-rollback](/img/blog/fence-empty-rollback.png)\\n\\nSeata\'s solution is to insert a record into the `tcc_fence_log` table during the try phase, with the `status` field set to `STATUS_TRIED`. During the rollback phase, it checks if the record exists, and if it doesn\'t, the rollback operation is not executed. The code is as follows:\\n\\n```Java\\n//TCCFenceHandler \\npublic static Object prepareFence(String xid, Long branchId, String actionName, Callback<Object> targetCallback) {\\n\\treturn transactionTemplate.execute(status -> {\\n\\t\\ttry {\\n\\t\\t\\tConnection conn = DataSourceUtils.getConnection(dataSource);\\n\\t\\t\\tboolean result = insertTCCFenceLog(conn, xid, branchId, actionName, TCCFenceConstant.STATUS_TRIED);\\n\\t\\t\\tLOGGER.info(\\"TCC fence prepare result: {}. xid: {}, branchId: {}\\", result, xid, branchId);\\n\\t\\t\\tif (result) {\\n\\t\\t\\t\\treturn targetCallback.execute();\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tthrow new TCCFenceException(String.format(\\"Insert tcc fence record error, prepare fence failed. xid= %s, branchId= %s\\", xid, branchId),\\n\\t\\t\\t\\t\\t\\tFrameworkErrorCode.InsertRecordError);\\n\\t\\t\\t}\\n\\t\\t} catch (TCCFenceException e) {\\n\\t\\t} catch (Throwable t) {\\n\\t\\t}\\n\\t});\\n}\\n```\\nThe processing logic in the Rollback phase is as follows:\\n\\n```Java\\n//TCCFenceHandler \\npublic static boolean rollbackFence(Method rollbackMethod, Object targetTCCBean,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tString xid, Long branchId, Object[] args, String actionName) {\\n\\treturn transactionTemplate.execute(status -> {\\n\\t\\ttry {\\n\\t\\t\\tConnection conn = DataSourceUtils.getConnection(dataSource);\\n\\t\\t\\tTCCFenceDO tccFenceDO = TCC_FENCE_DAO.queryTCCFenceDO(conn, xid, branchId);\\n\\t\\t\\t// non_rollback\\n\\t\\t\\tif (tccFenceDO == null) {\\n\\t\\t\\t\\t//The rollback logic is not executed\\n\\t\\t\\t\\treturn true;\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tif (TCCFenceConstant.STATUS_ROLLBACKED == tccFenceDO.getStatus() || TCCFenceConstant.STATUS_SUSPENDED == tccFenceDO.getStatus()) {\\n\\t\\t\\t\\t\\tLOGGER.info(\\"Branch transaction had already rollbacked before, idempotency rejected. xid: {}, branchId: {}, status: {}\\", xid, branchId, tccFenceDO.getStatus());\\n\\t\\t\\t\\t\\treturn true;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tif (TCCFenceConstant.STATUS_COMMITTED == tccFenceDO.getStatus()) {\\n\\t\\t\\t\\t\\tif (LOGGER.isWarnEnabled()) {\\n\\t\\t\\t\\t\\t\\tLOGGER.warn(\\"Branch transaction status is unexpected. xid: {}, branchId: {}, status: {}\\", xid, branchId, tccFenceDO.getStatus());\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\treturn false;\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t\\treturn updateStatusAndInvokeTargetMethod(conn, rollbackMethod, targetTCCBean, xid, branchId, TCCFenceConstant.STATUS_ROLLBACKED, status, args);\\n\\t\\t} catch (Throwable t) {\\n\\t\\t\\tstatus.setRollbackOnly();\\n\\t\\t\\tthrow new SkipCallbackWrapperException(t);\\n\\t\\t}\\n\\t});\\n}\\n```\\nupdateStatusAndInvokeTargetMethod method executes the following SQL:\\n\\n```sql\\nupdate tcc_fence_log set status = ?, gmt_modified = ?\\n    where xid = ? and  branch_id = ? and status = ? ;\\n```\\nAs we can see, it updates the value of the status field in the tcc_fence_log table from STATUS_TRIED to STATUS_ROLLBACKED. If the update is successful, the rollback logic is executed.\\n\\n### 4.3 Hanging\\nHanging refers to a situation where, due to network issues, the RM did not receive the try instruction initially, but after executing the rollback, the RM receives the try instruction and successfully reserves resources. This leads to the inability to release the reserved resources, as shown in the following diagram:\\n\\n![fence-suspend](/img/blog/fence-suspend.png)\\n\\nSeata solves this problem by checking if there is a record for the current xid in the tcc_fence_log table before executing the rollback method. If there is no record, it inserts a new record into the tcc_fence_log table with the status STATUS_SUSPENDED and does not perform the rollback operation. The code is as follows:\\n\\n```Java\\npublic static boolean rollbackFence(Method rollbackMethod, Object targetTCCBean,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tString xid, Long branchId, Object[] args, String actionName) {\\n\\treturn transactionTemplate.execute(status -> {\\n\\t\\ttry {\\n\\t\\t\\tConnection conn = DataSourceUtils.getConnection(dataSource);\\n\\t\\t\\tTCCFenceDO tccFenceDO = TCC_FENCE_DAO.queryTCCFenceDO(conn, xid, branchId);\\n\\t\\t\\t// non_rollback\\n\\t\\t\\tif (tccFenceDO == null) {\\n\\t\\t\\t\\tboolean result = insertTCCFenceLog(conn, xid, branchId, actionName, TCCFenceConstant.STATUS_SUSPENDED);\\n\\t\\t\\t\\treturn true;\\n\\t\\t\\t} else {\\n\\t\\t\\t}\\n\\t\\t\\treturn updateStatusAndInvokeTargetMethod(conn, rollbackMethod, targetTCCBean, xid, branchId, TCCFenceConstant.STATUS_ROLLBACKED, status, args);\\n\\t\\t} catch (Throwable t) {\\n\\t\\t}\\n\\t});\\n}\\n```\\nWhen executing the try phase method, a record for the current xid is first inserted into the tcc_fence_log table, which causes a primary key conflict. The code is as follows:\\n```Java\\n//TCCFenceHandler \\npublic static Object prepareFence(String xid, Long branchId, String actionName, Callback<Object> targetCallback) {\\n\\treturn transactionTemplate.execute(status -> {\\n\\t\\ttry {\\n\\t\\t\\tConnection conn = DataSourceUtils.getConnection(dataSource);\\n\\t\\t\\tboolean result = insertTCCFenceLog(conn, xid, branchId, actionName, TCCFenceConstant.STATUS_TRIED);\\n\\t\\t} catch (TCCFenceException e) {\\n\\t\\t\\tif (e.getErrcode() == FrameworkErrorCode.DuplicateKeyException) {\\n\\t\\t\\t\\tLOGGER.error(\\"Branch transaction has already rollbacked before,prepare fence failed. xid= {},branchId = {}\\", xid, branchId);\\n\\t\\t\\t\\taddToLogCleanQueue(xid, branchId);\\n\\t\\t\\t}\\n\\t\\t\\tstatus.setRollbackOnly();\\n\\t\\t\\tthrow new SkipCallbackWrapperException(e);\\n\\t\\t} catch (Throwable t) {\\n\\t\\t}\\n\\t});\\n}\\n```\\nNote: The queryTCCFenceDO method in the SQL statement uses for update, so there is no need to worry about not being able to determine the execution result of the local transaction in the rollback method due to the inability to obtain records from the tcc_fence_log table.\\n\\n### 5 Summary\\nTCC mode is a very important transaction mode in distributed transactions. However, idempotence, hanging, and empty rollback have always been issues that need to be considered in TCC mode. The Seata framework perfectly solves these problems in version 1.5.1.\\nThe operations on the tcc_fence_log table also need to consider transaction control. Seata uses a proxy data source to execute the operations on the tcc_fence_log table and the RM business operations in the same local transaction. This ensures that the local operations and the operations on the tcc_fence_log table succeed or fail together."},{"id":"/seata-tcc","metadata":{"permalink":"/blog/seata-tcc","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-tcc.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-tcc.md","title":"In-Depth Analysis of Seata TCC Mode (1)","description":"Seata currently supports AT mode, XA mode, TCC mode, and SAGA mode. Previous articles have talked more about non-intrusive AT mode. Today, we will introduce TCC mode, which is also a two-phase commit.","date":"2022-01-18T00:00:00.000Z","formattedDate":"January 18, 2022","tags":[],"readingTime":11.26,"hasTruncateMarker":false,"authors":[{"name":"Zhang Chenghui"}],"frontMatter":{"title":"In-Depth Analysis of Seata TCC Mode (1)","author":"Zhang Chenghui","keywords":["Seata\u3001distributed transaction\u3001TCC"],"description":"Seata currently supports AT mode, XA mode, TCC mode, and SAGA mode. Previous articles have talked more about non-intrusive AT mode. Today, we will introduce TCC mode, which is also a two-phase commit.","date":"2022/01/18"},"prevItem":{"title":"Alibaba Seata Resolves Idempotence, Dangling, and Empty Rollback Issues in TCC Mode","permalink":"/blog/seata-tcc-fence"},"nextItem":{"title":"In-Depth Analysis of Seata AT Mode Transaction Isolation Levels and Global Lock Design","permalink":"/blog/seata-at-lock"}},"content":"Seata currently supports AT mode, XA mode, TCC mode, and SAGA mode. Previous articles have talked more about non-intrusive AT mode. Today, we will introduce TCC mode, which is also a two-phase commit.\\n\\n# What is TCC\\n\\nTCC is a two-phase commit protocol in distributed transactions. Its full name is Try-Confirm-Cancel. Their specific meanings are as follows:\\n\\n1. Try: Check and reserve business resources;\\n2. Confirm: Commit the business transaction, i.e., the commit operation. If Try is successful, this step will definitely be successful;\\n3. Cancel: Cancel the business transaction, i.e., the rollback operation. This step will release the resources reserved in Try.\\n\\nTCC is an intrusive distributed transaction solution. All three operations need to be implemented by the business system itself, which has a significant impact on the business system. The design is relatively complex, but the advantage is that TCC does not rely on the database. It can manage resources across databases and applications, and can implement an atomic operation for different data access through intrusive coding, better solving the distributed transaction problems in various complex business scenarios.\\n\\n<img src=\\"/img/blog/20220116160157.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\n# Seata TCC mode\\n\\nSeata TCC mode follows the same principle as the general TCC mode. Let\'s first use Seata TCC mode to implement a distributed transaction:\\n\\nSuppose there is a business that needs to use service A and service B to complete a transaction operation. We define a TCC interface for this service in service A:\\n\\n```java\\npublic interface TccActionOne {\\n    @TwoPhaseBusinessAction(name = \\"DubboTccActionOne\\", commitMethod = \\"commit\\", rollbackMethod = \\"rollback\\")\\n    public boolean prepare(BusinessActionContext actionContext, @BusinessActionContextParameter(paramName = \\"a\\") String a);\\n\\n    public boolean commit(BusinessActionContext actionContext);\\n\\n    public boolean rollback(BusinessActionContext actionContext);\\n}\\n```\\n\\nSimilarly, we define a TCC interface for this service in service B:\\n\\n```java\\npublic interface TccActionTwo {\\n    @TwoPhaseBusinessAction(name = \\"DubboTccActionTwo\\", commitMethod = \\"commit\\", rollbackMethod = \\"rollback\\")\\n    public void prepare(BusinessActionContext actionContext, @BusinessActionContextParameter(paramName = \\"b\\") String b);\\n\\n    public void commit(BusinessActionContext actionContext);\\n\\n    public void rollback(BusinessActionContext actionContext);\\n}\\n```\\n\\nIn the business system, we start a global transaction and execute the TCC reserve resource methods for service A and service B:\\n\\n```java\\n@GlobalTransactional\\npublic String doTransactionCommit(){\\n    // Service A transaction participant\\n    tccActionOne.prepare(null,\\"one\\");\\n    // Service B transaction participant\\n    tccActionTwo.prepare(null,\\"two\\");\\n}\\n```\\n\\nThe example above demonstrates the implementation of a global transaction using Seata TCC mode. It can be seen that the TCC mode also uses the `@GlobalTransactional` annotation to initiate a global transaction, while the TCC interfaces of Service A and Service B are transaction participants. Seata treats a TCC interface as a Resource, also known as a TCC Resource.\\n\\nTCC interfaces can be RPC or internal JVM calls, meaning that a TCC interface has both a sender and a caller identity. In the example above, the TCC interface is the sender in Service A and Service B, and the caller in the business system. If the TCC interface is a Dubbo RPC, the caller is a dubbo:reference and the sender is a dubbo:service.\\n\\n<img src=\\"/img/blog/20220116161933.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nWhen Seata starts, it scans and parses the TCC interfaces. If a TCC interface is a sender, Seata registers the TCC Resource with the TC during startup, and each TCC Resource has a resource ID. If a TCC interface is a caller, Seata proxies the caller and intercepts the TCC interface calls. Similar to the AT mode, the proxy intercepts the call to the Try method, registers a branch transaction with the TC, and then executes the original RPC call.\\n\\nWhen the global transaction decides to commit/rollback, the TC will callback to the corresponding participant service to execute the Confirm/Cancel method of the TCC Resource using the resource ID registered by the branch.\\n\\n# How Seata Implements TCC Mode\\n\\nFrom the above Seata TCC model, it can be seen that the TCC mode in Seata also follows the TC, TM, RM three-role model. How to implement TCC mode in these three-role models? I mainly summarize the implementation as resource parsing, resource management, and transaction processing.\\n\\n## Resource Parsing\\n\\nResource parsing is the process of parsing and registering TCC interfaces. As mentioned earlier, TCC interfaces can be RPC or internal JVM calls. In the Seata TCC module, there is a remoting module that is specifically used to parse TCC interfaces with the `TwoPhaseBusinessAction` annotation:\\n\\n<img src=\\"/img/blog/20220116175059.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nThe `RemotingParser` interface mainly has methods such as `isRemoting`, `isReference`, `isService`, `getServiceDesc`, etc. The default implementation is `DefaultRemotingParser`, and the parsing of various RPC protocols is executed in `DefaultRemotingParser`. Seata has already implemented parsing of Dubbo, HSF, SofaRpc, and LocalTCC RPC protocols while also providing SPI extensibility for additional RPC protocol parsing classes.\\n\\nDuring the Seata startup process, the `GlobalTransactionScanner` annotation is used for scanning and executes the following method:\\n\\n`io.seata.spring.util.TCCBeanParserUtils#isTccAutoProxy`\\n\\nThe purpose of this method is to determine if the bean has been TCC proxied. In the process, it first checks if the bean is a Remoting bean. If it is, it calls the `getServiceDesc` method to parse the remoting bean, and if it is a sender, it registers the resource:\\n\\nio.seata.rm.tcc.remoting.parser.DefaultRemotingParser#parserRemotingServiceInfo\\n\\n```java\\npublic RemotingDesc parserRemotingServiceInfo(Object bean, String beanName, RemotingParser remotingParser){\\n    RemotingDesc remotingBeanDesc = remotingParser.getServiceDesc(bean, beanName);\\n    if(remotingBeanDesc == null){\\n    return null;\\n    }\\n    remotingServiceMap.put(beanName, remotingBeanDesc);\\n\\n    Class<?> interfaceClass = remotingBeanDesc.getInterfaceClass();\\n    Method[] methods = interfaceClass.getMethods();\\n    if (remotingParser.isService(bean, beanName)) {\\n        try {\\n            //service bean, registry resource\\n            Object targetBean = remotingBeanDesc.getTargetBean();\\n            for (Method m : methods) {\\n                TwoPhaseBusinessAction twoPhaseBusinessAction = m.getAnnotation(TwoPhaseBusinessAction.class);\\n                if (twoPhaseBusinessAction != null) {\\n                    TCCResource tccResource = new TCCResource();\\n                    tccResource.setActionName(twoPhaseBusinessAction.name());\\n                    tccResource.setTargetBean(targetBean);\\n                    tccResource.setPrepareMethod(m);\\n                    tccResource.setCommitMethodName(twoPhaseBusinessAction.commitMethod());\\n                    tccResource.setCommitMethod(interfaceClass.getMethod(twoPhaseBusinessAction.commitMethod(),\\n                    twoPhaseBusinessAction.commitArgsClasses()));\\n                    tccResource.setRollbackMethodName(twoPhaseBusinessAction.rollbackMethod());\\n                    tccResource.setRollbackMethod(interfaceClass.getMethod(twoPhaseBusinessAction.rollbackMethod(),\\n                    twoPhaseBusinessAction.rollbackArgsClasses()));\\n                    // set argsClasses\\n                    tccResource.setCommitArgsClasses(twoPhaseBusinessAction.commitArgsClasses());\\n                    tccResource.setRollbackArgsClasses(twoPhaseBusinessAction.rollbackArgsClasses());\\n                    // set phase two method\'s keys\\n                    tccResource.setPhaseTwoCommitKeys(this.getTwoPhaseArgs(tccResource.getCommitMethod(),\\n                    twoPhaseBusinessAction.commitArgsClasses()));\\n                    tccResource.setPhaseTwoRollbackKeys(this.getTwoPhaseArgs(tccResource.getRollbackMethod(),\\n                    twoPhaseBusinessAction.rollbackArgsClasses()));\\n                    // registry tcc resource\\n                    DefaultResourceManager.get().registerResource(tccResource);\\n                }\\n            }\\n        } catch (Throwable t) {\\n            throw new FrameworkException(t, \\"parser remoting service error\\");\\n        }\\n    }\\n    if (remotingParser.isReference(bean, beanName)) {\\n        // reference bean, TCC proxy\\n        remotingBeanDesc.setReference(true);\\n    }\\n    return remotingBeanDesc;\\n    }\\n```\\n\\nThe above method first calls the parsing class `getServiceDesc` method to parse the remoting bean and puts the parsed `remotingBeanDesc` into the local cache `remotingServiceMap`. At the same time, it calls the parsing class `isService` method to determine if it is the initiator. If it is the initiator, it parses the content of the `TwoPhaseBusinessAction` annotation to generate a `TCCResource` and registers it as a resource.\\n\\n## Resource Management\\n\\n**1. Resource Registration**\\n\\nThe resource for Seata TCC mode is called `TCCResource`, and its resource manager is called `TCCResourceManager`. As mentioned earlier, after parsing the TCC interface RPC resource, if it is the initiator, it will be registered as a resource:\\n\\nio.seata.rm.tcc.TCCResourceManager#registerResource\\n\\n```java\\npublic void registerResource(Resource resource){\\n    TCCResource tccResource=(TCCResource)resource;\\n    tccResourceCache.put(tccResource.getResourceId(),tccResource);\\n    super.registerResource(tccResource);\\n    }\\n```\\n\\n`TCCResource` contains the relevant information of the TCC interface and is cached locally. It continues to call the parent class `registerResource` method (which encapsulates communication methods) to register with the TC. The TCC resource\'s resourceId is the actionName, and the actionName is the name in the `@TwoParseBusinessAction` annotation.\\n\\n**2. Resource Commit/Rollback**\\n\\nio.seata.rm.tcc.TCCResourceManager#branchCommit\\n\\n```java\\npublic BranchStatus branchCommit(BranchType branchType,String xid,long branchId,String resourceId,\\n    String applicationData)throws TransactionException{\\n    TCCResource tccResource=(TCCResource)tccResourceCache.get(resourceId);\\n    if(tccResource==null){\\n    throw new ShouldNeverHappenException(String.format(\\"TCC resource is not exist, resourceId: %s\\",resourceId));\\n    }\\n    Object targetTCCBean=tccResource.getTargetBean();\\n    Method commitMethod=tccResource.getCommitMethod();\\n    if(targetTCCBean==null||commitMethod==null){\\n    throw new ShouldNeverHappenException(String.format(\\"TCC resource is not available, resourceId: %s\\",resourceId));\\n    }\\n    try{\\n    //BusinessActionContext\\n    BusinessActionContext businessActionContext=getBusinessActionContext(xid,branchId,resourceId,\\n    applicationData);\\n    // ... ... \\n    ret=commitMethod.invoke(targetTCCBean,args);\\n    // ... ... \\n    return result?BranchStatus.PhaseTwo_Committed:BranchStatus.PhaseTwo_CommitFailed_Retryable;\\n    }catch(Throwable t){\\n    String msg=String.format(\\"commit TCC resource error, resourceId: %s, xid: %s.\\",resourceId,xid);\\n    LOGGER.error(msg,t);\\n    return BranchStatus.PhaseTwo_CommitFailed_Retryable;\\n    }\\n    }\\n```\\n\\nWhen the TM resolves the phase two commit, the TC will callback to the corresponding participant (i.e., TCC interface initiator) service to execute the Confirm/Cancel method of the TCC Resource registered by the branch.\\n\\nIn the resource manager, the corresponding `TCCResource` will be found in the local cache based on the resourceId, and the corresponding `BusinessActionContext` will be found based on xid, branchId, resourceId, and applicationData, and the parameters to be executed are in the context. Finally, the commit method of the `TCCResource` is executed to perform the phase two commit.\\n\\nThe phase two rollback is similar.\\n\\n## Transaction Processing\\n\\nAs mentioned earlier, if the TCC interface is a caller, the Seata TCC proxy will be used to intercept the caller and register the branch before processing the actual RPC method call.\\n\\nThe method `io.seata.spring.util.TCCBeanParserUtils#isTccAutoProxy` not only parses the TCC interface resources, but also determines whether the TCC interface is a caller. If it is a caller, it returns true:\\n\\nio.seata.spring.annotation.GlobalTransactionScanner#wrapIfNecessary\\n\\n<img src=\\"/img/blog/20220116192544.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nAs shown in the figure, when `GlobalTransactionalScanner` scans the TCC interface caller (Reference), it will proxy and intercept it with `TccActionInterceptor`, which implements `MethodInterceptor`.\\n\\nIn `TccActionInterceptor`, it will also call `ActionInterceptorHandler` to execute the interception logic, and the transaction-related processing is in the `ActionInterceptorHandler#proceed` method:\\n\\n```java\\npublic Object proceed(Method method, Object[] arguments, String xid, TwoPhaseBusinessAction businessAction, \\n    Callback<Object> targetCallback) throws Throwable {\\n    //Get action context from arguments, or create a new one and then reset to arguments\\n    BusinessActionContext actionContext = getOrCreateActionContextAndResetToArguments(method.getParameterTypes(), arguments);\\n    //Creating Branch Record\\n    String branchId = doTccActionLogStore(method, arguments, businessAction, actionContext);\\n    // ... ... \\n    try {\\n    // ... ...\\n    return targetCallback.execute();\\n    } finally {\\n    try {\\n    //to report business action context finally if the actionContext.getUpdated() is true\\n    BusinessActionContextUtil.reportContext(actionContext);\\n    } finally {\\n    // ... ... \\n    }\\n    }\\n}\\n```\\n\\nIn the process of executing the first phase of the TCC interface, the `doTccActionLogStore` method is called for branch registration, and the TCC-related information such as parameters is placed in the context. This context will be used for resource submission/rollback as mentioned above.\\n\\n# How to control exceptions\\n\\nIn the process of executing the TCC model, various exceptions may occur, the most common of which are empty rollback, idempotence, and suspense. Here I will explain how Seata handles these three types of exceptions.\\n\\n## How to handle empty rollback\\n\\nWhat is an empty rollback?\\n\\nAn empty rollback refers to a situation in a distributed transaction where the TM drives the second-phase rollback of the participant\'s Cancel method without calling the participant\'s Try method.\\n\\nHow does an empty rollback occur?\\n\\n<img src=\\"/img/blog/20220116201900.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nAs shown in the above figure, after the global transaction is opened, participant A will execute the first-phase RPC method after completing branch registration. If the machine where participant A is located crashes or there is a network anomaly at this time, the RPC call will fail, meaning that participant A\'s first-phase method did not execute successfully. However, the global transaction has already been opened, so Seata must progress to the final state. When the global transaction is rolled back, participant A\'s Cancel method will be called, resulting in an empty rollback.\\n\\nTo prevent empty rollback, it is necessary to identify it in the Cancel method. How does Seata do this?\\n\\nSeata\'s approach is to add a TCC transaction control table, which contains the XID and BranchID information of the transaction. A record is inserted when the Try method is executed, indicating that phase one has been executed. When the Cancel method is executed, this record is read. If the record does not exist, it means that the Try method was not executed.\\n\\n## How to Handle Idempotent Operations\\n\\nIdempotent operation refers to TC repeating the two-phase commit, so the Confirm/Cancel interface needs to support idempotent processing, which means that it will not cause duplicate resource submission or release.\\n\\nSo how does idempotent operation arise?\\n\\n<img src=\\"/img/blog/20220116203816.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nAs shown in the above figure, after participant A completes the two phases, network jitter or machine failure may cause TC not to receive the return result of participant A\'s execution of the two phases. TC will continue to make repeated calls until the two-phase execution result is successful.\\n\\nHow does Seata handle idempotent operations?\\n\\nSimilarly, a status field is added to the TCC transaction control table. This field has 3 values:\\n\\n1. tried: 1\\n2. committed: 2\\n3. rollbacked: 3\\n\\nAfter the execution of the two-phase Confirm/Cancel method, the status is changed to committed or rollbacked. When the two-phase Confirm/Cancel method is called repeatedly, checking the transaction status can solve the idempotent problem.\\n\\n## How to Handle Suspend\\n\\nSuspension refers to the two-phase Cancel method being executed before the phase Try method, because empty rollback is allowed. After the execution of the two-phase Cancel method, directly returning success, the global transaction has ended. However, because the Try method is executed later, this will cause the resources reserved by the phase Try method to never be committed or released.\\n\\nSo how does suspension arise?\\n\\n<img src=\\"/img/blog/20220116205241.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nAs shown in the above figure, when participant A\'s phase Try method is executed, network congestion occurs, and due to Seata\'s global transaction timeout limit, after the Try method times out, TM resolves to roll back the global transaction. After the rollback is completed, if the RPC request arrives at participant A at this time and the Try method is executed to reserve resources, it will cause suspension.\\n\\nHow does Seata handle suspension?\\n\\nAdd a status to the TCC transaction control table:\\n\\n1. suspended: 4\\n\\nWhen the two-phase Cancel method is executed, if it is found that there is no related record in the TCC transaction control table, it means that the two-phase Cancel method is executed before the phase Try method. Therefore, a record with status=4 is inserted. Then, when the phase Try method is executed, if status=4 is encountered, it means that the two-phase Cancel has been executed, and false is returned to prevent the phase Try method from succeeding.\\n\\n# Author Introduction\\n\\nZhang Chenghui, currently working at Ant Group, loves to share technology. He is the author of the WeChat public account \\"Advanced Backend,\\" the author of the technical blog (https://objcoding.com/), a Seata Committer, and his GitHub ID is: objcoding."},{"id":"/seata-at-lock","metadata":{"permalink":"/blog/seata-at-lock","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-at-lock.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-at-lock.md","title":"In-Depth Analysis of Seata AT Mode Transaction Isolation Levels and Global Lock Design","description":"The transaction isolation in Seata AT mode is built on the basis of local isolation levels of supporting transactions. Assuming a database local isolation level of Read Committed or higher, Seata designs a global write-exclusive lock maintained by the transaction coordinator to ensure write isolation between transactions. Meanwhile, the default isolation level for global transactions is defined at Read Uncommitted.","date":"2022-01-12T00:00:00.000Z","formattedDate":"January 12, 2022","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"chenghui.zhang"}],"frontMatter":{"title":"In-Depth Analysis of Seata AT Mode Transaction Isolation Levels and Global Lock Design","author":"chenghui.zhang","keywords":["Seata","distributed transaction","AT mode","Transaction","GlobalLock"],"description":"The transaction isolation in Seata AT mode is built on the basis of local isolation levels of supporting transactions. Assuming a database local isolation level of Read Committed or higher, Seata designs a global write-exclusive lock maintained by the transaction coordinator to ensure write isolation between transactions. Meanwhile, the default isolation level for global transactions is defined at Read Uncommitted.","date":"2022/01/12"},"prevItem":{"title":"In-Depth Analysis of Seata TCC Mode (1)","permalink":"/blog/seata-tcc"},"nextItem":{"title":"Q&A on the New Version of Snowflake Algorithm","permalink":"/blog/seata-snowflake-explain"}},"content":""},{"id":"/seata-snowflake-explain","metadata":{"permalink":"/blog/seata-snowflake-explain","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-snowflake-explain.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-snowflake-explain.md","title":"Q&A on the New Version of Snowflake Algorithm","description":"In the previous analysis of the new version of the Snowflake algorithm, we mentioned two changes made in the new version:","date":"2021-06-21T00:00:00.000Z","formattedDate":"June 21, 2021","tags":[],"readingTime":7.315,"hasTruncateMarker":false,"authors":[{"name":"selfishlover"}],"frontMatter":{"title":"Q&A on the New Version of Snowflake Algorithm","author":"selfishlover","keywords":["Seata","snowflake","UUID","page split"],"date":"2021/06/21"},"prevItem":{"title":"In-Depth Analysis of Seata AT Mode Transaction Isolation Levels and Global Lock Design","permalink":"/blog/seata-at-lock"},"nextItem":{"title":"Analysis of Seata\'s Distributed UUID Generator Based on Improved Snowflake Algorithm","permalink":"/blog/seata-analysis-UUID-generator"}},"content":"In the previous analysis of the new version of the Snowflake algorithm, we mentioned two changes made in the new version:\\n1. The timestamp no longer constantly follows the system clock.\\n2. The exchange of positions between node ID and timestamp. From the original:\\n   ![Original Bit Allocation Strategy](/img/blog/seata/uuid/before.png)\\n   to:\\n   ![Improved Bit Allocation Strategy](/img/blog/seata/uuid/after.png)\\n\\nA careful student raised a question: In the new version, the algorithm is indeed monotonically increasing within a single node, but in a multi-instance deployment, it is no longer globally monotonically increasing! Because it is obvious that the node ID is in the high bits, so the generated ID with a larger node ID will definitely be greater than the ID with a smaller node ID, regardless of the chronological order. In contrast, the original algorithm, with the timestamp in the high bits and always following the system clock, can ensure that IDs generated earlier are smaller than those generated later. Only when two nodes happen to generate IDs at the same timestamp, the order of the two IDs is determined by the node ID. So, does it mean that the new version of the algorithm is wrong?\\n\\nThis is a great question! The fact that students can raise this question indicates a deep understanding of the essential differences between the standard Snowflake algorithm and the new version. This is commendable! Here, let\'s first state the conclusion: indeed, the new version of the algorithm does not possess global monotonicity, but this does not affect our original intention (to reduce database page splits). This conclusion may seem counterintuitive but can be proven.\\n\\nBefore providing the proof, let\'s briefly review some knowledge about page splits in databases. Taking the classic MySQL InnoDB as an example, InnoDB uses a B+ tree index where the leaf nodes of the primary key index also store the complete records of data rows. The leaf nodes are linked together in the form of a doubly linked list. The physical storage form of the leaf nodes is a data page, and each data page can store up to N rows of records (where N is inversely proportional to the size of each row). As shown in the diagram:\\n![Data Page](/img/blog/seata/uuid/page1.png)\\nThe characteristics of the B+ tree require that the left node should be smaller than the right node. What happens if we want to insert a record with an ID of 25 at this point (assuming each data page can only hold 4 records)? The answer is that it will cause a page split, as shown in the diagram:\\n![Page Split](/img/blog/seata/uuid/page2.png)\\nPage splits are unfriendly to I/O, requiring the creation of new data pages, copying and transferring part of the records from the old data page, etc., and should be avoided as much as possible.\\n\\nIdeally, the primary key ID should be sequentially increasing (for example, setting the primary key as auto_increment). This way, a new page will only be needed when the current data page is full, and the doubly linked list will always grow sequentially at the tail, avoiding any mid-node splits.\\n\\nIn the worst-case scenario, if the primary key ID is randomly generated and unordered (for example, a UUID string in Java), new records will be randomly assigned to any data page. If the page is already full, it will trigger a page split.\\n\\nIf the primary key ID is generated by the standard Snowflake algorithm, in the best-case scenario, only one node is generating IDs within each timestamp. In this case, the algorithm\'s effect is equivalent to the ideal situation of sequential incrementation, similar to auto_increment. In the worst-case scenario, all nodes within each timestamp are generating IDs, and the algorithm\'s effect is close to unordered (but still much better than completely unordered UUIDs, as the workerId with only 10 bits limits the nodes to a maximum of 1024). In actual production, the algorithm\'s effectiveness depends on business traffic, and the lower the concurrency, the closer the algorithm is to the ideal scenario.\\n\\nSo, how does it fare with the new version of the algorithm?  \\n\\n\\nThe new version of the algorithm, from a global perspective, produces IDs in an unordered manner. However, for each workerId, the generated IDs are strictly monotonically increasing. Additionally, since workerId is finite, it can divide into a maximum of 1024 subsequences, each of which is monotonically increasing.\\n\\nFor a database, initially, the received IDs may be unordered, coming from various subsequences, as illustrated here:\\n![Initial State](/img/blog/seata/uuid/page3.png)\\n\\nIf, at this point, a worker1-seq2 arrives, it will clearly cause a page split:\\n![First Split](/img/blog/seata/uuid/page4.png)\\n\\nHowever, after the split, interesting things happen. For worker1, subsequent seq3, seq4 will not cause page splits anymore (because there is still space), and seq5 only needs to link to a new page for sequential growth (the difference is that this new page is not at the tail of the doubly linked list). Note that the subsequent IDs of worker1 will not be placed after any nodes from worker2 or beyond (thus avoiding page splits for later nodes) because they are always smaller than the IDs of worker2; nor will they be placed before the current node of worker1 (thus avoiding page splits for previous nodes) because the subsequences of worker1 are always monotonically increasing. Here, we refer to such subsequences as reaching a steady state, meaning that the subsequence has \\"stabilized,\\" and its subsequent growth will only occur at the end of the subsequence without causing page splits for other nodes.\\n\\nThe same principle can be extended to all subsequences. Regardless of how chaotic the IDs received by the database are initially, after a finite number of page splits, the doubly linked list can always reach a stable state:\\n![Steady State](/img/blog/seata/uuid/page5.png)\\n\\nAfter reaching the steady state, subsequent IDs will only grow sequentially within their respective subsequences, without causing page splits. The difference between this sequential growth and the sequential growth of auto_increment is that the former has 1024 growth points (the ends of various subsequences), while the latter only has one at the end.\\n\\nAt this point, we can answer the question posed at the beginning: indeed, the new algorithm is not globally monotonically increasing, but the algorithm **converges**. After reaching a steady state, the new algorithm can achieve the same effect as global sequential incrementation.\\n\\n## Further Considerations\\n\\nThe discussion so far has focused on the continuous growth of sequences. However, in practical production, there is not only the insertion of new data but also the deletion of old data. Data deletion may lead to page merging (InnoDB, if it finds that the space utilization of two adjacent data pages is both less than 50%, it will merge them). How does this affect the new algorithm?\\n\\nAs we have seen in the above process, the essence of the new algorithm is to utilize early page splits to gradually separate different subsequences, allowing the algorithm to continuously converge to a steady state. Page merging, on the other hand, may reverse this process by merging different subsequences back into the same data page, hindering the convergence of the algorithm. Especially in the early stages of convergence, frequent page merging may even prevent the algorithm from converging forever (I just separated them, and now I\'m merging them back together, back to square one~)! However, after convergence, only page merging at the end nodes of each subsequence has the potential to disrupt the steady state (merging the end node of one subsequence with the head node of the next subsequence). Merging on the remaining nodes of the subsequence does not affect the steady state because the subsequence remains ordered, albeit with a shorter length.\\n\\nTaking Seata\'s server as an example, the data in the three tables of the server has a relatively short lifecycle. After a global transaction ends, the data is cleared. This is not friendly to the new algorithm, as it does not provide enough time for convergence. However, there is already a pull request (PR) for delayed deletion in the review process, and with this PR, the effect will be much better. For example, periodic weekly cleanup allows sufficient time for the algorithm to converge in the early stages, and for most of the time, the database can benefit from it. At the time of cleanup, the worst-case result is that the table is cleared, and the algorithm starts from scratch.\\n\\nIf you wish to apply the new algorithm to a business system, make sure to ensure that the algorithm has time to converge. For example, for user tables or similar, where data is intended to be stored for a long time, the algorithm can naturally converge. Alternatively, implement a mechanism for delayed deletion, providing enough time for the algorithm to converge.\\n\\nIf you have better opinions and suggestions, feel free to contact the Seata community!"},{"id":"/seata-analysis-UUID-generator","metadata":{"permalink":"/blog/seata-analysis-UUID-generator","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-UUID-generator.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-UUID-generator.md","title":"Analysis of Seata\'s Distributed UUID Generator Based on Improved Snowflake Algorithm","description":"Seata incorporates a distributed UUID generator to assist in generating global transaction IDs and branch transaction IDs. The desired characteristics for this generator include high performance, global uniqueness, and trend incrementation.","date":"2021-05-08T00:00:00.000Z","formattedDate":"May 8, 2021","tags":[],"readingTime":5.71,"hasTruncateMarker":false,"authors":[{"name":"selfishlover"}],"frontMatter":{"title":"Analysis of Seata\'s Distributed UUID Generator Based on Improved Snowflake Algorithm","author":"selfishlover","keywords":["Seata","snowflake","UUID"],"date":"2021/05/08"},"prevItem":{"title":"Q&A on the New Version of Snowflake Algorithm","permalink":"/blog/seata-snowflake-explain"},"nextItem":{"title":"Seata New Feature Support -- Undo_Log Compression","permalink":"/blog/seata-feature-undo-log-compress"}},"content":"Seata incorporates a distributed UUID generator to assist in generating global transaction IDs and branch transaction IDs. The desired characteristics for this generator include high performance, global uniqueness, and trend incrementation.\\n\\nHigh performance is self-explanatory, and global uniqueness is crucial to prevent confusion between different global transactions or branch transactions. Additionally, trend incrementation is valuable for users employing databases as the storage tool for TC clusters, as it can reduce the frequency of data page splits, thereby minimizing database IO pressure (the `branch_table` table uses the branch transaction ID as the primary key).\\n\\nIn the older version of Seata (prior to 1.4), the implementation of this generator was based on the standard version of the Snowflake algorithm. The standard Snowflake algorithm has been well-documented online, so we won\'t delve into it here. If you\'re unfamiliar with it, consider referring to existing resources before continuing with this article.\\n\\nHere, we discuss some drawbacks of the standard Snowflake algorithm:\\n1. **Clock Sensitivity:** Since ID generation is always tied to the current operating system\'s timestamp (leveraging the monotonicity of time), a clock rollback may result in repeated IDs. Seata\'s strategy to handle this is by recording the last timestamp and rejecting service if the current timestamp is less than the recorded value (indicating a clock rollback). The service waits until the timestamp catches up with the recorded value. However, this means the TC will be in an unavailable state during this period.\\n\\n2. **Burst Performance Limit:** The standard Snowflake algorithm claims a high QPS, approximately 4 million/s. However, strictly speaking, this is a bit misleading. The timestamp unit of the algorithm is milliseconds, and the bit length allocated to the sequence number is 12, allowing for 4096 sequence spaces per millisecond. So, a more accurate description would be 4096/ms. The distinction between 4 million/s and 4096/ms lies in the fact that the former doesn\'t require every millisecond\'s concurrency to be below 4096. Seata also adheres to this limitation. If the sequence space for the current timestamp is exhausted, it will spin-wait for the next timestamp.\\n\\nIn newer versions (1.4 and beyond), the generator has undergone optimizations and improvements to address these issues effectively. The core idea of the improvement is to decouple from the operating system\'s timestamp, with the generator obtaining the system\'s current timestamp only during initialization as the initial timestamp. Subsequently, it no longer synchronizes with the system timestamp. The incrementation is solely driven by the incrementation of the sequence number. For example, when the sequence number reaches its maximum value (4095), the next request causes an overflow of the 12-bit space. The sequence number resets to zero, and the overflow carry is added to the timestamp, incrementing it by 1. Thus, the timestamp and sequence number can be considered as a single entity. In practice, we adjusted the bit allocation strategy for the 64-bit ID, swapping the positions of the timestamp and node ID for easier handling of this overflow carry:\\n\\nOriginal Bit Allocation Strategy:\\n![Original Bit Allocation Strategy](/img/blog/seata/uuid/before.png)\\n\\nModified Bit Allocation Strategy (swapping timestamp and node ID):\\n![Modified Bit Allocation Strategy](/img/blog/seata/uuid/after.png)\\n\\nThis arrangement allows the timestamp and sequence number to be contiguous in memory, making it easy to use an `AtomicLong` to simultaneously store them.\\n\\n```\\n/**\\n * timestamp and sequence mix in one Long\\n * highest 11 bit: not used\\n * middle  41 bit: timestamp\\n * lowest  12 bit: sequence\\n */\\nprivate AtomicLong timestampAndSequence;\\n```\\nThe highest 11 bits can be determined during initialization and remain unchanged thereafter:\\n```\\n/**\\n * business meaning: machine ID (0 ~ 1023)\\n * actual layout in memory:\\n * highest 1 bit: 0\\n * middle 10 bit: workerId\\n * lowest 53 bit: all 0\\n */\\nprivate long workerId;\\n```\\nProducing an ID is then straightforward\uff1a\\n```\\npublic long nextId() {\\n   // Obtain the incremented timestamp and sequence number\\n   long next = timestampAndSequence.incrementAndGet();\\n   // Extract the lowest 53 bits\\n   long timestampWithSequence = next & timestampAndSequenceMask;\\n   // Perform a bitwise OR operation with the previously saved top 11 bits\\n   return workerId | timestampWithSequence;\\n}\\n```\\n\\nAt this point, we can observe the following:\\n\\n1. The generator no longer has a burst performance limit of 4096/ms. If the sequence number space for a timestamp is exhausted, it will directly advance to the next timestamp, \\"borrowing\\" the sequence number space of the next timestamp (there is no need to worry about serious consequences of this \\"advance consumption,\\" as the reasons will be explained below).\\n\\n2. The generator has a weak dependency on the operating system clock. During runtime, the generator is not affected by clock backtracking (whether it is manually backtracked or due to machine clock drift) because the generator only fetches the system clock once at startup, and thereafter, they no longer stay synchronized. The only possible scenario for duplicate IDs is a significant clock backtracking during restart (either deliberate human backtracking or modification of the operating system time zone, such as changing Beijing time to London time~ Machine clock drift is typically in the millisecond range and won\'t have such a large impact).\\n\\n3. Will continuous \\"advance consumption\\" cause the generator\'s timestamps to be significantly ahead of the system timestamps, resulting in ID duplicates upon restart? In theory, yes, but practically almost impossible. To achieve this effect, it would mean that the generator\'s QPS received must be consistently stable at over 400w/s~ To be honest, even TC can\'t handle such high traffic, so, the bottleneck is definitely not in the generator.\\n\\nIn addition, we also adjusted the strategy for generating node IDs. In the original version, when the user did not manually specify a node ID, it would take the low 10 bits of the local IPv4 address as the node ID. In practical production, it was found that there were occasional occurrences of duplicate node IDs (mostly users deploying with k8s). For example, the following IPs would result in duplicates:\\n- 192.168.4.10\\n- 192.168.8.10\\n\\nMeaning, as long as the low 2 bits of the fourth byte and the third byte of the IP are the same, duplicates would occur. The new version\'s strategy is to prioritize taking the low 10 bits from the MAC address of the local network card. If the local machine does not have a valid network card configuration, it randomly picks one from [0, 1023] as the node ID. After this adjustment, it seems that new version users are no longer reporting the same issue (of course, it remains to be tested over time, but in any case, it won\'t be worse than the IP extraction strategy).\\n\\nThe above is a brief analysis of Seata\'s distributed UUID generator. If you find this generator useful, you can directly use it in your project. Its class declaration is `public`, and the full class name is:\\n`io.seata.common.util.IdWorker`\\n\\nOf course, if you have better ideas, you are also welcome to discuss them with the Seata community."},{"id":"/seata-feature-undo-log-compress","metadata":{"permalink":"/blog/seata-feature-undo-log-compress","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-feature-undo-log-compress.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-feature-undo-log-compress.md","title":"Seata New Feature Support -- Undo_Log Compression","description":"Current Situation & Pain Points","date":"2021-05-07T00:00:00.000Z","formattedDate":"May 7, 2021","tags":[],"readingTime":3.785,"hasTruncateMarker":false,"authors":[{"name":"chd"}],"frontMatter":{"title":"Seata New Feature Support -- Undo_Log Compression","author":"chd","keywords":["Seata","undo_log","compress"],"date":"2021/05/07"},"prevItem":{"title":"Analysis of Seata\'s Distributed UUID Generator Based on Improved Snowflake Algorithm","permalink":"/blog/seata-analysis-UUID-generator"},"nextItem":{"title":"Seata Deadlock Issue Caused by ConcurrentHashMap","permalink":"/blog/seata-dsproxy-deadlock"}},"content":"## Current Situation & Pain Points\\n\\nFor Seata, it records the before and after data of DML operations to perform possible rollback operations, and stores this data in a blob field in the database. For batch operations such as insert, update, delete, etc., the number of affected rows may be significant, concatenated into a large field inserted into the database, which may lead to the following issues:\\n\\n1. Exceeding the maximum write limit for a single database operation (such as the `max_allowed_package` parameter in MySQL).\\n2. Significant network IO and database disk IO overhead due to a large amount of data.\\n\\n## Brainstorming\\n\\nFor the first issue, the `max_allowed_package` parameter limit can be increased based on the actual situation of the business to avoid the \\"query is too large\\" problem. For the second issue, increasing bandwidth and using high-performance SSD as the database storage medium can help.\\n\\nThe above solutions involve external or costly measures. Is there a framework-level solution to address the pain points mentioned above?\\n\\nConsidering the root cause of the pain points mentioned above, the problem lies in the generation of excessively large data fields. Therefore, if the corresponding data can be compressed at the business level before data transmission and storage, theoretically, it can solve the problems mentioned above.\\n\\n## Feasibility Analysis\\n\\nCombining the brainstorming above, in practical development, when large batch operations are required, they are often scheduled during periods of relatively low user activity and low concurrency. At such times, CPU and memory resources can be relatively more utilized to quickly complete the corresponding operations. Therefore, by consuming CPU and memory resources to compress rollback data, the size of data transmission and storage can be reduced.\\n\\nAt this point, two things need to be demonstrated:\\n\\n1. After compression, it can reduce the pressure on network IO and database disk IO. This can be measured by the total time taken for data compression + storage in the database.\\n2. After compression, the efficiency of compression compared to the original data size. This can be measured by the data size before and after compression.\\n\\nTesting the time spent on compressing network usage:\\n\\n![image](https://user-images.githubusercontent.com/22959373/95567752-f55ddf80-0a55-11eb-8092-1f1d99855bdd.png)\\n\\n## Compression Ratio Test:\\n\\n![image](https://user-images.githubusercontent.com/22959373/95567834-0ad30980-0a56-11eb-9d7e-48b74babbea4.png)\\n\\nThe test results clearly indicate that using gzip or zip compression can significantly reduce the pressure on the database and network transmission. At the same time, it can substantially decrease the size of the stored data.\\n\\n### Implementation\\n\\n#### Implementation Approach\\n\\n![Compression](https://user-images.githubusercontent.com/22959373/116281711-8f039900-a7bc-11eb-91f8-82afdbb9f932.png)\\n\\n#### Partial Code\\n\\n```properties\\n# Whether to enable undo_log compression, default is true\\nseata.client.undo.compress.enable=true\\n\\n# Compressor type, default is zip, generally recommended to be zip\\nseata.client.undo.compress.type=zip\\n\\n# Compression threshold for enabling compression, default is 64k\\nseata.client.undo.compress.threshold=64k\\n```\\n\\nDetermining Whether the Undo_Log Compression Feature is Enabled and if the Compression Threshold is Reached\\n\\n```java\\nprotected boolean needCompress(byte[] undoLogContent) {\\n// 1. Check whether undo_log compression is enabled (1.4.2 Enabled by Default).\\n// 2. Check whether the compression threshold has been reached (64k by default).\\n// If both return requirements are met, the corresponding undoLogContent is compressed\\n    return ROLLBACK_INFO_COMPRESS_ENABLE \\n        && undoLogContent.length > ROLLBACK_INFO_COMPRESS_THRESHOLD;\\n}\\n```\\n\\nInitiating Compression for Undo_Log After Determining the Need\\n\\n\\n```java\\n// If you need to compress, compress undo_log\\nif (needCompress(undoLogContent)) {\\n    // Gets the compression type, default zip\\n    compressorType = ROLLBACK_INFO_COMPRESS_TYPE;\\n    //Get the corresponding compressor and compress it\\n    undoLogContent = CompressorFactory.getCompressor(compressorType.getCode()).compress(undoLogContent);\\n}\\n// else does not need to compress and does not need to do anything\\n```\\n\\nSave the compression type synchronously to the database for use when rolling back:\\n\\n```java\\nprotected String buildContext(String serializer, CompressorType compressorType) {\\n    Map<String, String> map = new HashMap<>();\\n    map.put(UndoLogConstants.SERIALIZER_KEY, serializer);\\n    // Save the compression type to the database\\n    map.put(UndoLogConstants.COMPRESSOR_TYPE_KEY, compressorType.name());\\n    return CollectionUtils.encodeMap(map);\\n}\\n```\\n\\nDecompress the corresponding information when rolling back:\\n\\n```java\\nprotected byte[] getRollbackInfo(ResultSet rs) throws SQLException  {\\n    // Gets a byte array of rollback information saved to the database\\n    byte[] rollbackInfo = rs.getBytes(ClientTableColumnsName.UNDO_LOG_ROLLBACK_INFO);\\n    // Gets the compression type\\n    // getOrDefault uses the default value CompressorType.NONE to directly upgrade 1.4.2+ to compatible versions earlier than 1.4.2\\n    String rollbackInfoContext = rs.getString(ClientTableColumnsName.UNDO_LOG_CONTEXT);\\n    Map<String, String> context = CollectionUtils.decodeMap(rollbackInfoContext);\\n    CompressorType compressorType = CompressorType.getByName(context.getOrDefault(UndoLogConstants.COMPRESSOR_TYPE_KEY,\\n    CompressorType.NONE.name()));\\n    // Get the corresponding compressor and uncompress it\\n    return CompressorFactory.getCompressor(compressorType.getCode())\\n        .decompress(rollbackInfo);\\n}\\n```\\n\\n\\n\\n### peroration\\n\\nBy compressing undo_log, Seata can further improve its performance when processing large amounts of data at the framework level. At the same time, it also provides the corresponding switch and relatively reasonable default value, which is convenient for users to use out of the box, but also convenient for users to adjust according to actual needs, so that the corresponding function is more suitable for the actual use scenario."},{"id":"/seata-dsproxy-deadlock","metadata":{"permalink":"/blog/seata-dsproxy-deadlock","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-dsproxy-deadlock.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-dsproxy-deadlock.md","title":"Seata Deadlock Issue Caused by ConcurrentHashMap","description":"This article primarily discusses an online issue, a Seata dynamic data source proxy deadlock caused by a ConcurrentHashMap bug.","date":"2021-03-13T00:00:00.000Z","formattedDate":"March 13, 2021","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"xiaoyong.luo"}],"frontMatter":{"title":"Seata Deadlock Issue Caused by ConcurrentHashMap","keywords":["Seata","dynamic data source","DataSource","ConcurrentHashMap","computeIfAbsent"],"description":"This article primarily discusses an online issue, a Seata dynamic data source proxy deadlock caused by a ConcurrentHashMap bug.","author":"xiaoyong.luo","date":"2021/03/13"},"prevItem":{"title":"Seata New Feature Support -- Undo_Log Compression","permalink":"/blog/seata-feature-undo-log-compress"},"nextItem":{"title":"Seata Application-Side Startup Process Analysis \u2014 Registry and Configuration Module","permalink":"/blog/seata-client-start-analysis-02"}},"content":""},{"id":"/seata-client-start-analysis-02","metadata":{"permalink":"/blog/seata-client-start-analysis-02","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-client-start-analysis-02.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-client-start-analysis-02.md","title":"Seata Application-Side Startup Process Analysis \u2014 Registry and Configuration Module","description":"","date":"2021-03-04T01:35:01.000Z","formattedDate":"March 4, 2021","tags":[{"label":"Seata","permalink":"/blog/tags/seata"}],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"booogu"}],"frontMatter":{"layout":"post","comments":true,"title":"Seata Application-Side Startup Process Analysis \u2014 Registry and Configuration Module","date":"2021-03-04T01:35:01.000Z","author":"booogu","catalog":true,"tags":["Seata"]},"prevItem":{"title":"Seata Deadlock Issue Caused by ConcurrentHashMap","permalink":"/blog/seata-dsproxy-deadlock"},"nextItem":{"title":"Seata Application-Side Startup Process Analysis \u2014 How RM & TM Connect with TC","permalink":"/blog/seata-client-start-analysis-01"}},"content":""},{"id":"/seata-client-start-analysis-01","metadata":{"permalink":"/blog/seata-client-start-analysis-01","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-client-start-analysis-01.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-client-start-analysis-01.md","title":"Seata Application-Side Startup Process Analysis \u2014 How RM & TM Connect with TC","description":"","date":"2021-02-28T21:08:00.000Z","formattedDate":"February 28, 2021","tags":[{"label":"Seata","permalink":"/blog/tags/seata"}],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"booogu"}],"frontMatter":{"layout":"post","comments":true,"title":"Seata Application-Side Startup Process Analysis \u2014 How RM & TM Connect with TC","date":"2021-02-28T21:08:00.000Z","author":"booogu","catalog":true,"tags":["Seata"]},"prevItem":{"title":"Seata Application-Side Startup Process Analysis \u2014 Registry and Configuration Module","permalink":"/blog/seata-client-start-analysis-02"},"nextItem":{"title":"Integration of Spring Cloud with Seata for Distributed Transaction - TCC Mode","permalink":"/blog/integrate-seata-tcc-mode-with-spring-cloud"}},"content":""},{"id":"/integrate-seata-tcc-mode-with-spring-cloud","metadata":{"permalink":"/blog/integrate-seata-tcc-mode-with-spring-cloud","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/integrate-seata-tcc-mode-with-spring-cloud.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/integrate-seata-tcc-mode-with-spring-cloud.md","title":"Integration of Spring Cloud with Seata for Distributed Transaction - TCC Mode","description":"This article mainly introduces the integration of Spring Cloud with Seata for distributed transaction using the TCC mode.","date":"2021-01-23T00:00:00.000Z","formattedDate":"January 23, 2021","tags":[],"readingTime":5.76,"hasTruncateMarker":false,"authors":[{"name":"gongxing(zhijian.tan)"}],"frontMatter":{"title":"Integration of Spring Cloud with Seata for Distributed Transaction - TCC Mode","keywords":["TCC","Seata","Spring Cloud","Distributed","Transaction"],"description":"This article mainly introduces the integration of Spring Cloud with Seata for distributed transaction using the TCC mode.","author":"gongxing(zhijian.tan)","date":"2021-01-23T00:00:00.000Z"},"prevItem":{"title":"Seata Application-Side Startup Process Analysis \u2014 How RM & TM Connect with TC","permalink":"/blog/seata-client-start-analysis-01"},"nextItem":{"title":"Analysis of Seata Configuration Management Principles","permalink":"/blog/seata-config-manager"}},"content":"This article will introduce how to integrate Seata (1.4.0) with Spring Cloud and Feign using the TCC mode. In practice, Seata\'s AT mode can meet about 80% of our distributed transaction needs. However, when dealing with operations on databases and middleware (such as Redis) that do not support transactions, or when using databases that are not currently supported by the AT mode (currently AT supports MySQL, Oracle, and PostgreSQL), cross-company service invocations, cross-language application invocations, or the need for manual control of the entire two-phase commit process, we need to combine the TCC mode. Moreover, the TCC mode also supports mixed usage with the AT mode.\\n\\n\\n# \u4e00\u3001The concept of TCC mode\\n\\nIn Seata, a distributed global transaction follows a two-phase commit model with a Try-[Confirm/Cancel] pattern. Both the AT (Automatic Transaction) mode and the TCC (Try-Confirm-Cancel) mode in Seata are implementations of the two-phase commit. The main differences between them are as follows:\\n\\nAT mode is based on relational databases that support local ACID transactions (currently supporting MySQL, Oracle, and PostgreSQL):\\n\\nThe first phase, prepare: In the local transaction, it combines the submission of business data updates and the recording of corresponding rollback logs.\\nThe second phase, commit: It immediately completes successfully and automatically asynchronously cleans up the rollback logs.\\nThe second phase, rollback: It automatically generates compensation operations through the rollback logs to complete data rollback.\\n\\nOn the other hand, TCC mode does not rely on transaction support from underlying data resources:\\n\\nThe first phase, prepare: It calls a custom-defined prepare logic.\\nThe second phase, commit: It calls a custom-defined commit logic.\\nThe second phase, rollback: It calls a custom-defined rollback logic.\\n\\nTCC mode refers to the ability to include custom-defined branch transactions in the management of global transactions.\\n\\nIn summary, Seata\'s TCC mode is a manual implementation of the AT mode that allows you to define the processing logic for the two phases without relying on the undo_log used in the AT mode.\\n\\n# \u4e8c\u3001prepare\\n\\n- regist center [nacos](https://nacos.io/zh-cn/ \\"nacos\\") \\n- [seata server(TC\uff09](/docs/ops/deploy-guide-beginner/ \\"seata\u670d\u52a1\u7aef(TC\uff09\\")\\n\\n\\n# \u4e09\u3001Building TM and TCC-RM\\n\\nThis chapter focuses on the implementation of TCC using Spring Cloud + Feign. For the project setup, please refer to the source code (this project provides demos for both AT mode and TCC mode).\\n\\n[DEMO](https://github.com/tanzzj/springcloud-seata-feign \\"\u670d\u52a1\u7aef\u642d\u5efa\u6587\u6863\\")\\n\\n## 3.1 build seata server \\n\\n[build server doc](/docs/ops/deploy-guide-beginner/ \\"\u670d\u52a1\u7aef\u642d\u5efa\u6587\u6863\\")\\n\\n## 3.2 build TM\\n\\n[service-tm](https://github.com/tanzzj/springcloud-seata-feign/tree/master/service-tm)\\n\\n## 3.3 build RM-TCC\\n\\n### 3.3.1 Defining TCC Interface\\n\\nSince we are using Spring Cloud + Feign, which relies on HTTP for communication, we can use @LocalTCC here. It is important to note that @LocalTCC must be annotated on the interface. This interface can be a regular business interface as long as it implements the corresponding methods for the two-phase commit in TCC. The TCC-related annotations are as follows:\\n\\n- @LocalTCC: Used for TCC in the Spring Cloud + Feign mode.\\n- @TwoPhaseBusinessAction: Annotates the try method. The name attribute represents the bean name of the current TCC method, which can be the method name (globally unique). The commitMethod attribute points to the commit method, and the rollbackMethod attribute points to the transaction rollback method. After specifying these three methods, Seata will automatically invoke the commit or rollback method based on the success or failure of the global transaction.\\n- @BusinessActionContextParameter: Annotates the parameters to be passed to the second phase (commitMethod/rollbackMethod) methods.\\n- BusinessActionContext: Represents the TCC transaction context.\\n\\nHere is an example:\\n\\n```java\\n/**\\n * Here we define the TCC interface.\\n * It must be defined on the interface.\\n * We are using Spring Cloud for remote invocation.\\n * Therefore, we can use LocalTCC here.\\n *\\n */\\n@LocalTCC\\npublic interface TccService {\\n \\n    /**\\n     * Define the two-phase commit.\\n     * name = The bean name of this TCC, globally unique.\\n     * commitMethod = The method for the second phase confirmation.\\n     * rollbackMethod = The method for the second phase cancellation.\\n     * Use the BusinessActionContextParameter annotation to pass parameters to the second phase.\\n     *\\n     * @param params  \\n     * @return String\\n     */\\n    @TwoPhaseBusinessAction(name = \\"insert\\", commitMethod = \\"commitTcc\\", rollbackMethod = \\"cancel\\")\\n    String insert(\\n            @BusinessActionContextParameter(paramName = \\"params\\") Map<String, String> params\\n    );\\n \\n    /**\\n     *  The confirmation method can be named differently, but it must be consistent with the commitMethod.\\n     *  The context can be used to pass the parameters from the try method.\\n     * @param context \\n     * @return boolean\\n     */\\n    boolean commitTcc(BusinessActionContext context);\\n \\n    /**\\n     * two phase cancel\\n     *\\n     * @param context \\n     * @return boolean\\n     */\\n    boolean cancel(BusinessActionContext context);\\n}\\n```\\n\\n### 3.3.2 Business Implementation of TCC Interface\\n\\nTo keep the code concise, we will combine the routing layer with the business layer for explanation here. However, in actual projects, this may not be the case.\\n\\n- Using @Transactional in the try method allows for direct rollback of operations in relational databases through Spring transactions. The rollback of operations in non-relational databases or other middleware can be handled in the rollbackMethod.\\n- By using context.getActionContext(\\"params\\"), you can retrieve the parameters defined in the try phase and perform business rollback operations on these parameters in the second phase.\\n- Note 1: It is not advisable to catch exceptions here (similarly, handle exceptions with aspects), as doing so would cause TCC to recognize the operation as successful, and the second phase would directly execute the commitMethod.\\n- Note 2: In TCC mode, it is the responsibility of the developer to ensure idempotence and transaction suspension prevention.\\n\\n```java\\n@Slf4j\\n@RestController\\npublic class TccServiceImpl implements  TccService {\\n \\n    @Autowired\\n    TccDAO tccDAO;\\n \\n    /**\\n     * tcc t\uff08try\uff09method\\n     * Choose the actual business execution logic or resource reservation logic based on the actual business scenario.\\n     *\\n     * @param params - name\\n     * @return String\\n     */\\n    @Override\\n    @PostMapping(\\"/tcc-insert\\")\\n    @Transactional(rollbackFor = Exception.class, propagation = Propagation.REQUIRED)\\n    public String insert(@RequestBody Map<String, String> params) {\\n        log.info(\\"xid = \\" + RootContext.getXID());\\n        //todo Perform actual operations or operations on MQ, Redis, etc.\\n        tccDAO.insert(params);\\n        //Remove the following annotations to throw an exception\\n        //throw new RuntimeException(\\"\u670d\u52a1tcc\u6d4b\u8bd5\u56de\u6eda\\");\\n        return \\"success\\";\\n    }\\n \\n    /**\\n     * TCC service confirm method\\n     * If resource reservation is used in the first phase, the reserved resources should be committed during the second phase confirmation\\n     * @param context \\n     * @return boolean\\n     */\\n    @Override\\n    public boolean commitTcc(BusinessActionContext context) {\\n        log.info(\\"xid = \\" + context.getXid() + \\"\u63d0\u4ea4\u6210\u529f\\");\\n        //todo If resource reservation is used in the first phase, resources should be committed here.\\n        return true;\\n    }\\n \\n    /**\\n     * tcc  cancel method\\n     *\\n     * @param context \\n     * @return boolean\\n     */\\n    @Override\\n    public boolean cancel(BusinessActionContext context) {\\n        //todo Here, write the rollback operations for middleware or non-relational databases.\\n        System.out.println(\\"please manually rollback this data:\\" + context.getActionContext(\\"params\\"));\\n        return true;\\n    }\\n}\\n```\\n\\n### 3.3.3 Starting a Global Transaction in TM and Invoking RM-TCC Interface\\n\\nPlease refer to the project source code in section 3.2.\\n\\nWith this, the integration of TCC mode with Spring Cloud is complete."},{"id":"/seata-config-manager","metadata":{"permalink":"/blog/seata-config-manager","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-config-manager.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-config-manager.md","title":"Analysis of Seata Configuration Management Principles","description":"This article primarily introduces the core implementation of Seata configuration management and the interaction process with Spring configuration.","date":"2021-01-10T00:00:00.000Z","formattedDate":"January 10, 2021","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"xiaoyong.luo"}],"frontMatter":{"title":"Analysis of Seata Configuration Management Principles","keywords":["Seata","configuration center","configuration management","Spring configuration"],"description":"This article primarily introduces the core implementation of Seata configuration management and the interaction process with Spring configuration.","author":"xiaoyong.luo","date":"2021/01/10"},"prevItem":{"title":"Integration of Spring Cloud with Seata for Distributed Transaction - TCC Mode","permalink":"/blog/integrate-seata-tcc-mode-with-spring-cloud"},"nextItem":{"title":"Detailed Explanation of seata-golang Communication Model","permalink":"/blog/seata-golang-communication-mode"}},"content":""},{"id":"/seata-golang-communication-mode","metadata":{"permalink":"/blog/seata-golang-communication-mode","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-golang-communication-mode.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-golang-communication-mode.md","title":"Detailed Explanation of seata-golang Communication Model","description":"This article provides a detailed explanation of the underlying RPC communication implementation in seata-golang.","date":"2021-01-04T00:00:00.000Z","formattedDate":"January 4, 2021","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"xiaomin.liu"}],"frontMatter":{"title":"Detailed Explanation of seata-golang Communication Model","keywords":["seata","seata-golang","seata-go","getty","distributed transaction"],"description":"This article provides a detailed explanation of the underlying RPC communication implementation in seata-golang.","author":"xiaomin.liu","date":"2021/01/04"},"prevItem":{"title":"Analysis of Seata Configuration Management Principles","permalink":"/blog/seata-config-manager"},"nextItem":{"title":"Analysis of Seata Data Source Proxy","permalink":"/blog/seata-datasource-proxy"}},"content":""},{"id":"/seata-datasource-proxy","metadata":{"permalink":"/blog/seata-datasource-proxy","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-datasource-proxy.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-datasource-proxy.md","title":"Analysis of Seata Data Source Proxy","description":"This article primarily introduces the implementation principles of Seata data source proxy and potential issues that may arise during usage.","date":"2020-10-16T00:00:00.000Z","formattedDate":"October 16, 2020","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"xiaoyong.luo"}],"frontMatter":{"title":"Analysis of Seata Data Source Proxy","keywords":["Seata","data source","data source proxy","multiple data sources"],"description":"This article primarily introduces the implementation principles of Seata data source proxy and potential issues that may arise during usage.","author":"xiaoyong.luo","date":"2020/10/16"},"prevItem":{"title":"Detailed Explanation of seata-golang Communication Model","permalink":"/blog/seata-golang-communication-mode"},"nextItem":{"title":"Seata Source Code - Client Startup Process in Distributed Transactions","permalink":"/blog/seata-sourcecode-client-bootstrap"}},"content":""},{"id":"/seata-sourcecode-client-bootstrap","metadata":{"permalink":"/blog/seata-sourcecode-client-bootstrap","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-sourcecode-client-bootstrap.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-sourcecode-client-bootstrap.md","title":"Seata Source Code - Client Startup Process in Distributed Transactions","description":"","date":"2020-08-25T00:00:00.000Z","formattedDate":"August 25, 2020","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"xiaobin.yang"}],"frontMatter":{"title":"Seata Source Code - Client Startup Process in Distributed Transactions","author":"xiaobin.yang","date":"2020/08/25","keywords":["fescar","seata","distributed transaction"]},"prevItem":{"title":"Analysis of Seata Data Source Proxy","permalink":"/blog/seata-datasource-proxy"},"nextItem":{"title":"Setting Up Seata Demo Environment on Mac (AT Mode)","permalink":"/blog/seata-at-demo-in-mac"}},"content":""},{"id":"/seata-at-demo-in-mac","metadata":{"permalink":"/blog/seata-at-demo-in-mac","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-at-demo-in-mac.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-at-demo-in-mac.md","title":"Setting Up Seata Demo Environment on Mac (AT Mode)","description":"","date":"2020-07-20T00:00:00.000Z","formattedDate":"July 20, 2020","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"portman xu"}],"frontMatter":{"title":"Setting Up Seata Demo Environment on Mac (AT Mode)","author":"portman xu","date":"2020/07/20","keywords":["seata","distributed transaction","demo","mac","at"]},"prevItem":{"title":"Seata Source Code - Client Startup Process in Distributed Transactions","permalink":"/blog/seata-sourcecode-client-bootstrap"},"nextItem":{"title":"The Refactoring Journey of Seata RPC Module","permalink":"/blog/seata-rpc-refactor"}},"content":""},{"id":"/seata-rpc-refactor","metadata":{"permalink":"/blog/seata-rpc-refactor","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-rpc-refactor.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-rpc-refactor.md","title":"The Refactoring Journey of Seata RPC Module","description":"","date":"2020-07-13T00:00:00.000Z","formattedDate":"July 13, 2020","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"chenghui.zhang"}],"frontMatter":{"title":"The Refactoring Journey of Seata RPC Module","author":"chenghui.zhang","keywords":["Seata","RPC module","refactoring"],"date":"2020/07/13"},"prevItem":{"title":"Setting Up Seata Demo Environment on Mac (AT Mode)","permalink":"/blog/seata-at-demo-in-mac"},"nextItem":{"title":"Seata Source Code - Server Startup Process in Distributed Transactions","permalink":"/blog/seata-sourcecode-server-bootstrap"}},"content":""},{"id":"/seata-sourcecode-server-bootstrap","metadata":{"permalink":"/blog/seata-sourcecode-server-bootstrap","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-sourcecode-server-bootstrap.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-sourcecode-server-bootstrap.md","title":"Seata Source Code - Server Startup Process in Distributed Transactions","description":"","date":"2020-07-02T00:00:00.000Z","formattedDate":"July 2, 2020","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"xiaobing.yang"}],"frontMatter":{"title":"Seata Source Code - Server Startup Process in Distributed Transactions","author":"xiaobing.yang","date":"2020/07/02","keywords":["fescar","seata","distributed transaction"]},"prevItem":{"title":"The Refactoring Journey of Seata RPC Module","permalink":"/blog/seata-rpc-refactor"},"nextItem":{"title":"How is distributed transaction realized? In-depth interpretation of Seata\'s XA mode","permalink":"/blog/seata-xa-introduce"}},"content":""},{"id":"/seata-xa-introduce","metadata":{"permalink":"/blog/seata-xa-introduce","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-xa-introduce.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-xa-introduce.md","title":"How is distributed transaction realized? In-depth interpretation of Seata\'s XA mode","description":"In-depth interpretation of Seata\'s XA mode","date":"2020-04-28T00:00:00.000Z","formattedDate":"April 28, 2020","tags":[],"readingTime":14.365,"hasTruncateMarker":false,"authors":[{"name":"Xuan Yi"}],"frontMatter":{"title":"How is distributed transaction realized? In-depth interpretation of Seata\'s XA mode","keywords":["Seata","distributed transaction","XA","AT"],"description":"In-depth interpretation of Seata\'s XA mode","author":"Xuan Yi","date":"2020/04/28"},"prevItem":{"title":"Seata Source Code - Server Startup Process in Distributed Transactions","permalink":"/blog/seata-sourcecode-server-bootstrap"},"nextItem":{"title":"Seata Quick Start","permalink":"/blog/seata-quick-start"}},"content":"Author profile: Xuan Yi, GitHub ID: sharajava, responsible for the GTS development team of Alibaba Middleware, initiator of the SEATA open source project, worked for many years at Oracle Beijing R&D Center, and was engaged in WebLogic core development. He has long been focused on middleware, especially technical practices in the field of distributed transactions.\\n\\nThe 1.2.0 version of Seata has released a new transaction mode: XA mode, which supports the XA protocol.\\n\\nHere, we will interpret this new feature in depth from three aspects:\\n\\n- What: What is XA mode?\\n- Why: Why support XA?\\n- How: How is XA mode implemented and how to use it?\\n\\n# 1. What is XA mode?\\n\\nThere are two basic preliminary concepts here:\\n\\n1. What is XA?\\n2. What is the so-called transaction mode defined by Seata?\\n\\nBased on these two points, understanding XA mode becomes quite natural.\\n\\n## 1.1 What is XA?\\n\\nThe XA specification is a standard for distributed transaction processing (DTP) defined by the X/Open organization.\\n\\nThe XA specification describes the interface between the global transaction manager(TM) and the local resource manager(RM). The purpose of the XA specification is to allow multiple resources (such as databases, application servers, message queues, etc.) to access the same transaction, thus maintaining ACID properties across applications.\\n\\nThe XA specification uses the Two-Phase Commit (2PC) to ensure that all resources are committed or rolled back at the same time for any specific transaction.\\n\\nThe XA specification was proposed in the early 1990s. Currently, almost all mainstream databases support the XA specification.\\n\\n## 1.2 What is Seata\'s transaction mode?\\n\\nSeata defines the framework for global transactions.\\n\\nA global transaction is defined as the overall coordination of several branch transactions:\\n\\n1. The Transaction Manager (TM) requests the Transaction Coordinator (TC) to initiate (Begin), commit (Commit), or rollback (Rollback) the global transaction.\\n2. The TM binds the XID representing the global transaction to the branch transaction.\\n3. The Resource Manager (RM) registers with the TC, associating the branch transaction with the global transaction represented by XID.\\n4. The RM reports the execution result of the branch transaction to the TC. (optional)\\n5. The TC sends a branch commit or branch rollback command to the RM.\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB19qmhOrY1gK0jSZTEXXXDQVXa-1330-924.png\\" alt=\\"seata-mod\\" style={{ zoom:\'50%\' }} />\\n\\nSeata\'s global transaction processing process is divided into two phases:\\n\\n- Execution phase: Execute branch transactions and ensure that the execution results are *rollbackable* and *durable*.\\n- Completion phase: Based on the resolution of the execution phase, the application sends a request for global commit or rollback to the TC through the TM, and the TC commands the RM to drive the branch transaction to commit or rollback.\\n\\nSeata\'s so-called transaction mode refers to the behavior mode of branch transactions running under the Seata global transaction framework. More precisely, it should be called the branch transaction mode.\\n\\nThe difference between different transaction modes lies in the different ways branch transactions achieve the goals of the two phases of the global transaction.  That is, answering the following two questions:\\n\\n- Execution phase: How to execute and ensure that the execution results are *rollbackable* and *durable*.\\n- Completion phase: After receiving the command from the TC, how to submit or rollback the branch transaction?\\n\\nTaking our Seata AT mode and TCC mode as examples:\\n\\nAT mode\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB1NTuzOBr0gK0jSZFnXXbRRXXa-1330-924.png\\" alt=\\"at-mod\\" style={{ zoom:\'50%\' }} />\\n\\n- Execution phase:\\n  - Rollbackable: Record rollback logs according to the SQL parsing result\\n  - Durable: Rollback logs and business SQL are committed to the database in the same local transaction\\n\\n- Completion phase:\\n  - Branch commit: Asynchronously delete rollback log records\\n  - Branch rollback: Compensate and update according to the rollback log\\n\\nTCC mode\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB1m59pOEY1gK0jSZFCXXcwqXXa-1330-924.png\\" alt=\\"tcc-mod\\" style={{ zoom:\'50%\' }} />\\n\\n- Execution Phase:\\n\\n  - Call the Try method defined by the business (guaranteed *rollback* and *persistence* entirely by the business layer)\\n\\n- Completion Phase:\\n\\n  - Branch Commit: Call the Confirm method defined for each transaction branch\\n  - Branch Rollback: Call the Cancel method defined for each transaction branch\\n\\n## 1.3 What is XA mode in Seata?\\n\\nXA mode:\\n\\nWithin the distributed transaction framework defined by Seata, it is a transaction mode that uses XA protocol mechanisms to manage branch transactions with the support of transaction resources (databases, message services, etc.) for the XA protocol.\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB1hSpccIVl614jSZKPXXaGjpXa-1330-924.png\\" alt=\\"xa-mod\\" style={{ zoom:\'50%\' }} />\\n\\n- Execution Phase:\\n\\n  - Rollback: Business SQL operations are performed in an XA branch, and the support of resources for the XA protocol ensures *rollback*\\n  - Persistence: After the XA branch is completed, XA prepare is executed, and similarly, the support of resources for the XA protocol ensures *persistence* (i.e., any unexpected occurrences will not cause situations where rollback is not possible)\\n\\n- Completion Phase:\\n\\n  - Branch Commit: Perform commit for XA branch\\n  - Branch Rollback: Perform rollback for XA branch\\n\\n# 2. Why support XA?\\n\\nWhy add XA mode in Seata? What is the significance of supporting XA?\\n\\n## 2.1 Problems with Compensatory Transaction Mode\\n\\nEssentially, the 3 major transaction modes that Seata already supports: AT, TCC, and Saga, are all compensatory in nature.\\n\\nCompensatory transaction processing mechanisms are built on top of transaction resources (either in the middleware layer or in the application layer), and the transaction resources themselves are unaware of distributed transactions.\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB1z.qyOET1gK0jSZFrXXcNCXXa-602-460.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nThe fundamental problem with transaction resources being unaware of distributed transactions is the inability to achieve true *global consistency*.\\n\\nFor example, in a compensatory transaction processing process, a stock record is reduced from 100 to 50. At this point, the warehouse administrator connects to the database and sees the current quantity as 50. Later, the transaction is rolled back due to an unexpected occurrence, and the stock is compensated back to 100. Clearly, the warehouse administrator\'s query finding 50 is *dirty data*.\\n\\nIt can be seen that because compensatory distributed transaction mechanisms do not require the mechanism of transaction resources (such as a database), they cannot guarantee data consistency from a global perspective outside the transaction framework.\\n\\n## 2.2 Value of XA\\n\\nUnlike compensatory transaction modes, the XA protocol requires transaction resources to provide support for standards and protocols.\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB1vs9kOvb2gK0jSZK9XXaEgFXa-602-486.png\\" alt=\\"nct\\" style={{ zoom:\'50%\' }} />\\n\\nBecause transaction resources are aware of and participate in the distributed transaction processing process, they (such as databases) can guarantee effective isolation of data from any perspective and satisfy global data consistency.\\n\\nFor example, in the scenario of stock updates mentioned in the previous section, during the XA transaction processing process, the intermediate state of the database holding 50 is guaranteed by the database itself and will not be *seen* in the warehouse administrator\'s query statistics. (Of course, the isolation level needs to be READ_COMMITTED or higher.)\\n\\nIn addition to the fundamental value of *global consistency*, supporting XA also has the following benefits:\\n\\n1. Non-invasive business: Like AT, XA mode will be non-invasive for businesses, without bringing additional burden to application design and development.\\n2. Wide support for databases: XA protocol is widely supported by mainstream relational databases and can be used without additional adaptation.\\n3. Easy multi-language support: Because it does not involve SQL parsing, the XA mode has lower requirements for Seata\'s RM, making it easier for different language development SDKs compared to the AT mode.\\n4. Migration of traditional XA-based applications: Traditional applications based on the XA protocol can be smoothly migrated to the Seata platform using the XA mode.\\n\\n## 2.3 Widely Questioned Issues of XA\\n\\nThere is no distributed transaction mechanism that can perfectly adapt to all scenarios and meet all requirements.\\n\\nThe XA specification was proposed as early as the early 1990s to solve the problems in the field of distributed transaction processing.\\n\\nNow, whether it\'s the AT mode, TCC mode, or the Saga mode, the essence of these modes\' proposals stems from the inability of the XA specification to meet certain scenario requirements.\\n\\nThe distributed transaction processing mechanism defined by the XA specification has some widely questioned issues. What is our thinking regarding these issues?\\n\\n1. **Data Locking**: Data is locked throughout the entire transaction processing until it is finished, and reads and writes are constrained according to the definition of isolation levels.\\n\\n> Thinking:\\n>\\n> Data locking is the cost to obtain higher isolation and global consistency.\\n>\\n> In compensatory transaction processing mechanisms, the completion of branch (local) transactions is done during the execution stage, and data is not locked at the resource level. However, this is done at the cost of sacrificing isolation.\\n>\\n> Additionally, the AT mode uses *global locks* to ensure basic *write isolation*, effectively locking data, but the lock is managed centrally on the TC side, with high unlock efficiency and no blocking issues.\\n\\n2. **Protocol Blocking**: After XA prepare, the branch transaction enters a blocking stage and must wait for XA commit or XA rollback.\\n\\n> Thinking:\\n>\\n> The blocking mechanism of the protocol itself is not the problem. The key issue is the combination of protocol blocking and data locking.\\n>\\n> If a resource participating in the global transaction is \\"offline\\" (does not receive commands to end branch transactions), the data it locks will remain locked. This may even lead to deadlocks.\\n>\\n> This is the core pain point of the XA protocol and is the key problem that Seata aims to solve by introducing the XA mode.\\n>\\n> The basic idea is twofold: avoiding \\"loss of connection\\" and adding a \\"self-release\\" mechanism. (This involves a lot of technical details, which will not be discussed at the moment. They will be specifically discussed in the subsequent evolution of the XA mode.)\\n\\n3. **Poor Performance**: Performance loss mainly comes from two aspects: on one hand, the transaction coordination process increases the RT of individual transactions; on the other hand, concurrent transaction data lock conflicts reduce throughput.\\n\\n> Thinking:\\n>\\n> Compared to running scenarios without distributed transaction support, performance will certainly decline, there is no doubt about that.\\n>\\n> Essentially, the transaction mechanism (whether local or distributed) sacrifices some performance to achieve a simple programming model.\\n>\\n> Compared to the AT mode, which is also *non-invasive for businesses*:\\n>\\n> Firstly, because XA mode also runs under Seata\'s defined distributed transaction framework, it does not generate additional transaction coordination communication overhead.\\n>\\n> Secondly, in concurrent transactions, if data has hotspots and lock conflicts occur, this situation also exists in the AT mode (which defaults to using a global lock).\\n>\\n> Therefore, in the two main aspects affecting performance, the XA mode does not have a significantly obvious disadvantage compared to the AT mode.\\n>\\n> The performance advantage of the AT mode mainly lies in: centralized management of global data locks, where the release of locks does not require RM involvement and is very fast; in addition, the asynchronous completion of the global commit stage.\\n\\n# 3. How Does XA Mode Work and How to Use It?\\n\\n## 3.1 Design of XA Mode\\n\\n### 3.1.1 Design Objectives\\n\\nThe basic design objectives of XA mode mainly focus on two main aspects:\\n\\n1. From the perspective of *scenarios*, it meets the requirement of *global consistency*.\\n2. From the perspective of *applications*, it maintains the non-invasive nature consistent with the AT mode.\\n3. From the perspective of *mechanisms*, it adapts to the characteristics of distributed microservice architecture.\\n\\nOverall idea:\\n\\n1. Same as the AT mode: Construct branch transactions from local transactions in the application program.\\n2. Through data source proxy, wrap the interaction mechanism of the XA protocol at the framework level outside the scope of local transactions in the application program, making the XA programming model transparent.\\n3. Split the 2PC of XA and perform XA prepare at the end of the execution stage of branch transactions, seamlessly integrating the XA protocol into Seata\'s transaction framework, reducing one round of RPC interaction.\\n\\n### 3.1.2 Core Design\\n\\n#### 1. Overall Operating Mechanism\\n\\nXA mode runs within the transaction framework defined by Seata:\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB1uM2OaSslXu8jSZFuXXXg7FXa-1330-958.png\\" alt=\\"xa-fw\\" style={{ zoom:\'50%\' }} />\\n\\n- Execution phase (Execute):\\n\\n  - XA start/XA end/XA prepare + SQL + Branch registration\\n\\n- Completion phase (Finish):\\n\\n  - XA commit/XA rollback\\n\\n#### 2. Data Source Proxy\\n\\nXA mode requires XAConnection.\\n\\nThere are two ways to obtain XAConnection:\\n\\n- Method 1: Requires developers to configure XADataSource\\n- Method 2: Creation based on the developer\'s normal DataSource\\n\\nThe first method adds cognitive burden to developers, as they need to learn and use XA data sources specifically for XA mode, which contradicts the design goal of transparent XA programming model.\\n\\nThe second method is more user-friendly, similar to the AT mode, where developers do not need to worry about any XA-related issues and can maintain a local programming model.\\n\\nWe prioritize the implementation of the second method: the data source proxy creates the corresponding XAConnection based on the normal JDBC connection obtained from the normal data source.\\n\\nComparison with the data source proxy mechanism of the AT mode:\\n\\n<img src=\\"/img/xa/pics/ds1.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nHowever, the second method has limitations: it cannot guarantee compatibility correctness.\\n\\nIn fact, this method is what database drivers should do. Different vendors and different versions of database driver implementation mechanisms are vendor-specific, and we can only guarantee correctness on fully tested driver versions, as differences in the driver versions used by developers can lead to the failure of the mechanism.\\n\\nThis is particularly evident in Oracle. See Druid issue: [https://github.com/alibaba/druid/issues/3707](https://github.com/alibaba/druid/issues/3707)\\n\\nTaking everything into account, the data source proxy design for XA mode needs to support the first method: proxy based on XA data source.\\n\\nComparison with the data source proxy mechanism of the AT mode:\\n\\n<img src=\\"/img/xa/pics/ds2.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\n#### 3. Branch Registration\\n\\nXA start requires the Xid parameter.\\n\\nThis Xid needs to be associated with the XID and BranchId of the Seata global transaction, so that the TC can drive the XA branch to commit or rollback.\\n\\nCurrently, the BranchId in Seata is generated uniformly by the TC during the branch registration process, so the timing of the XA mode branch registration needs to be before XA start.\\n\\nA possible optimization in the future:\\n\\nDelay branch registration as much as possible. Similar to the AT mode, register the branch before the local transaction commit to avoid meaningless branch registration in case of branch execution failure.\\n\\nThis optimization direction requires a change in the BranchId generation mechanism to cooperate. BranchId will not be generated through the branch registration process, but will be generated and then used to register the branch.\\n\\n#### 4. Summary\\n\\nHere, only a few important core designs of the XA mode are explained to illustrate its basic operation mechanism.\\n\\nIn addition, important aspects such as *connection maintenance* and *exception handling* are also important and can be further understood from the project code.\\n\\nMore information and exchange will be written and shared with everyone in the future.\\n\\n### 3.1.3 Evolution Plan\\n\\nThe overall evolution plan of the XA mode is as follows:\\n\\n1. Step 1 (already completed): The first version (1.2.0) runs the prototype mechanism of the XA mode. Ensure only addition, no modification, and no new issues introduced to other modes.\\n2. Step 2 (planned to be completed in May): Necessary integration and refactoring with the AT mode.\\n3. Step 3 (planned to be completed in July): Refine the exception handling mechanism and polish for production readiness.\\n4. Step 4 (planned to be completed in August): Performance optimization.\\n5. Step 5 (planned to be completed in 2020): Integrate with Seata project\'s ongoing design for cloud-native Transaction Mesh to create cloud-native capabilities.\\n\\n## 3.2 Usage of XA Mode\\n\\nFrom a programming model perspective, XA mode is exactly the same as the AT mode.\\n\\nYou can refer to the Seata official website sample: [seata-xa](https://github.com/seata/seata-xa)\\n\\nThe example scenario is the classic Seata example, involving the product ordering business of three microservices: inventory, orders, and accounts.\\n\\nIn the example, the upper programming model is the same as the AT mode. By simply modifying the data source proxy, you can switch between XA mode and AT mode.\\n\\n```java\\n@Bean(\\"dataSource\\")\\npublic DataSource dataSource(DruidDataSource druidDataSource) {\\n    // DataSourceProxy for AT mode\\n    // return new DataSourceProxy(druidDataSource);\\n\\n    // DataSourceProxyXA for XA mode\\n    return new DataSourceProxyXA(druidDataSource);\\n}\\n```\\n\\n# 4. Summary\\n\\nAt the current stage of technological development, there is no distributed transaction processing mechanism that can perfectly meet all scenarios\' requirements.\\n\\nConsistency, reliability, ease of use, performance, and many other aspects of system design constraints require different transaction processing mechanisms to meet them.\\n\\nThe core value of the Seata project is to build a standardized platform that comprehensively addresses the distributed transaction problem.\\n\\nBased on Seata, the upper application architecture can flexibly choose the appropriate distributed transaction solution according to the actual scenario\'s needs.\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB1lTSoOqL7gK0jSZFBXXXZZpXa-1028-528.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nThe addition of XA mode fills the gap in Seata in the global consistency scenario, forming a landscape of four major transaction modes: AT, TCC, Saga, and XA, which can basically meet all scenarios\' demands for distributed transaction processing.\\n\\nOf course, both XA mode and the Seata project itself are not yet perfect, and there are many areas that need improvement and enhancement. We warmly welcome everyone to participate in the project\'s development and contribute to building a standardized distributed transaction platform together."},{"id":"/seata-quick-start","metadata":{"permalink":"/blog/seata-quick-start","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-quick-start.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-quick-start.md","title":"Seata Quick Start","description":"Getting started with Seata from scratch, setting up Seata services, and integrating distributed transactions into Java projects.","date":"2020-04-19T00:00:00.000Z","formattedDate":"April 19, 2020","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"yudaoyuanma"}],"frontMatter":{"title":"Seata Quick Start","description":"Getting started with Seata from scratch, setting up Seata services, and integrating distributed transactions into Java projects.","keywords":["fescar","seata","distributed transaction"],"author":"yudaoyuanma","date":"2020/04/19"},"prevItem":{"title":"How is distributed transaction realized? In-depth interpretation of Seata\'s XA mode","permalink":"/blog/seata-xa-introduce"},"nextItem":{"title":"Seata High Availability Deployment Practice","permalink":"/blog/seata-ha-practice"}},"content":""},{"id":"/seata-ha-practice","metadata":{"permalink":"/blog/seata-ha-practice","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-ha-practice.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-ha-practice.md","title":"Seata High Availability Deployment Practice","description":"Seata High Availability Deployment Practice","date":"2020-04-10T00:00:00.000Z","formattedDate":"April 10, 2020","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"helloworlde"}],"frontMatter":{"hidden":true,"title":"Seata High Availability Deployment Practice","keywords":["kubernetes","ops"],"description":"Seata High Availability Deployment Practice","author":"helloworlde","date":"2020-04-10T00:00:00.000Z"},"prevItem":{"title":"Seata Quick Start","permalink":"/blog/seata-quick-start"},"nextItem":{"title":"Seata Config Module Source Code Analysis","permalink":"/blog/seata-analysis-config-modular"}},"content":""},{"id":"/seata-analysis-config-modular","metadata":{"permalink":"/blog/seata-analysis-config-modular","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-config-modular.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-config-modular.md","title":"Seata Config Module Source Code Analysis","description":"","date":"2020-01-11T00:00:00.000Z","formattedDate":"January 11, 2020","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"runze.zhao"}],"frontMatter":{"title":"Seata Config Module Source Code Analysis","author":"runze.zhao","keywords":["Seata","distributed transaction"],"date":"2020/1/11"},"prevItem":{"title":"Seata High Availability Deployment Practice","permalink":"/blog/seata-ha-practice"},"nextItem":{"title":"Source Code Analysis of Seata-XID Propagation in Dubbo","permalink":"/blog/seata-analysis-dubbo-transmit-xid"}},"content":""},{"id":"/seata-analysis-dubbo-transmit-xid","metadata":{"permalink":"/blog/seata-analysis-dubbo-transmit-xid","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-dubbo-transmit-xid.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-dubbo-transmit-xid.md","title":"Source Code Analysis of Seata-XID Propagation in Dubbo","description":"This article explores the propagation of XID in Seata-Dubbo through source code analysis.","date":"2020-01-01T00:00:00.000Z","formattedDate":"January 1, 2020","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"FUNKYE"}],"frontMatter":{"title":"Source Code Analysis of Seata-XID Propagation in Dubbo","keywords":["Seata","Dubbo","distributed transaction","spring"],"description":"This article explores the propagation of XID in Seata-Dubbo through source code analysis.","author":"FUNKYE","date":"2020/01/01"},"prevItem":{"title":"Seata Config Module Source Code Analysis","permalink":"/blog/seata-analysis-config-modular"},"nextItem":{"title":"Seata TCC Module Source Code Analysis","permalink":"/blog/seata-analysis-tcc-modular"}},"content":""},{"id":"/seata-analysis-tcc-modular","metadata":{"permalink":"/blog/seata-analysis-tcc-modular","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-tcc-modular.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-tcc-modular.md","title":"Seata TCC Module Source Code Analysis","description":"","date":"2019-12-25T00:00:00.000Z","formattedDate":"December 25, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"runze.zhao"}],"frontMatter":{"title":"Seata TCC Module Source Code Analysis","author":"runze.zhao","keywords":["Seata","distributed transaction"],"date":"2019/12/25"},"prevItem":{"title":"Source Code Analysis of Seata-XID Propagation in Dubbo","permalink":"/blog/seata-analysis-dubbo-transmit-xid"},"nextItem":{"title":"Seata Community Meetup\xb7Hangzhou Station","permalink":"/blog/seata-community-meetup-hangzhou-ready"}},"content":""},{"id":"/seata-community-meetup-hangzhou-ready","metadata":{"permalink":"/blog/seata-community-meetup-hangzhou-ready","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-community-meetup-hangzhou-ready.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-community-meetup-hangzhou-ready.md","title":"Seata Community Meetup\xb7Hangzhou Station","description":"Seata Community Meetup\xb7Hangzhou Station, officially held on December 21st at Zhejiang Youth Innovation Space in Dream Town, Hangzhou.","date":"2019-12-25T00:00:00.000Z","formattedDate":"December 25, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Seata Community Meetup\xb7Hangzhou Station","keywords":["Seata","Hangzhou","meetup"],"date":"2019/12/25","description":"Seata Community Meetup\xb7Hangzhou Station, officially held on December 21st at Zhejiang Youth Innovation Space in Dream Town, Hangzhou."},"prevItem":{"title":"Seata TCC Module Source Code Analysis","permalink":"/blog/seata-analysis-tcc-modular"},"nextItem":{"title":"Seata Core Module Source Code Analysis","permalink":"/blog/seata-analysis-core-modular"}},"content":""},{"id":"/seata-analysis-core-modular","metadata":{"permalink":"/blog/seata-analysis-core-modular","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-core-modular.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-core-modular.md","title":"Seata Core Module Source Code Analysis","description":"","date":"2019-12-23T00:00:00.000Z","formattedDate":"December 23, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"runze.zhao"}],"frontMatter":{"title":"Seata Core Module Source Code Analysis","author":"runze.zhao","keywords":["Seata","distributed transaction"],"date":"2019/12/23"},"prevItem":{"title":"Seata Community Meetup\xb7Hangzhou Station","permalink":"/blog/seata-community-meetup-hangzhou-ready"},"nextItem":{"title":"Dynamically Creating/Closing Seata Distributed Transactions through AOP","permalink":"/blog/seata-spring-boot-aop-aspectj"}},"content":""},{"id":"/seata-spring-boot-aop-aspectj","metadata":{"permalink":"/blog/seata-spring-boot-aop-aspectj","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-spring-boot-aop-aspectj.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-spring-boot-aop-aspectj.md","title":"Dynamically Creating/Closing Seata Distributed Transactions through AOP","description":"This article explains how to dynamically create/close Seata distributed transactions using AOP.","date":"2019-12-23T00:00:00.000Z","formattedDate":"December 23, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"FUNKYE"}],"frontMatter":{"title":"Dynamically Creating/Closing Seata Distributed Transactions through AOP","keywords":["Seata","Nacos","distributed transaction","spring"],"description":"This article explains how to dynamically create/close Seata distributed transactions using AOP.","author":"FUNKYE","date":"2019/12/23"},"prevItem":{"title":"Seata Core Module Source Code Analysis","permalink":"/blog/seata-analysis-core-modular"},"nextItem":{"title":"Seata Dynamic Configuration Subscription and Degradation Implementation Principles","permalink":"/blog/seata-dynamic-config-and-dynamic-disable"}},"content":""},{"id":"/seata-dynamic-config-and-dynamic-disable","metadata":{"permalink":"/blog/seata-dynamic-config-and-dynamic-disable","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-dynamic-config-and-dynamic-disable.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-dynamic-config-and-dynamic-disable.md","title":"Seata Dynamic Configuration Subscription and Degradation Implementation Principles","description":"This article explains how Seata adapts to different dynamic configuration subscriptions and implements degradation functionality with support for multiple configuration centers.","date":"2019-12-17T00:00:00.000Z","formattedDate":"December 17, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"chenghui.zhang"}],"frontMatter":{"title":"Seata Dynamic Configuration Subscription and Degradation Implementation Principles","author":"chenghui.zhang","keywords":["Seata","Dynamic","Config"],"description":"This article explains how Seata adapts to different dynamic configuration subscriptions and implements degradation functionality with support for multiple configuration centers.","date":"2019/12/17"},"prevItem":{"title":"Dynamically Creating/Closing Seata Distributed Transactions through AOP","permalink":"/blog/seata-spring-boot-aop-aspectj"},"nextItem":{"title":"Seata Configuration Center Implementation Principles","permalink":"/blog/seata-config-center"}},"content":""},{"id":"/seata-config-center","metadata":{"permalink":"/blog/seata-config-center","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-config-center.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-config-center.md","title":"Seata Configuration Center Implementation Principles","description":"Seata supports multiple third-party configuration centers, but how does Seata simultaneously accommodate so many configuration centers?","date":"2019-12-12T00:00:00.000Z","formattedDate":"December 12, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"chenghui.zhang"}],"frontMatter":{"title":"Seata Configuration Center Implementation Principles","author":"chenghui.zhang","keywords":["Seata","Config"],"description":"Seata supports multiple third-party configuration centers, but how does Seata simultaneously accommodate so many configuration centers?","date":"2019/12/12"},"prevItem":{"title":"Seata Dynamic Configuration Subscription and Degradation Implementation Principles","permalink":"/blog/seata-dynamic-config-and-dynamic-disable"},"nextItem":{"title":"Docker Deployment of Seata Integration with Nacos","permalink":"/blog/seata-nacos-docker"}},"content":""},{"id":"/seata-nacos-docker","metadata":{"permalink":"/blog/seata-nacos-docker","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-nacos-docker.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-nacos-docker.md","title":"Docker Deployment of Seata Integration with Nacos","description":"This article explains how to deploy Seata integrated with Nacos configuration using Docker.","date":"2019-12-03T00:00:00.000Z","formattedDate":"December 3, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"FUNKYE"}],"frontMatter":{"title":"Docker Deployment of Seata Integration with Nacos","keywords":["Seata","Nacos","distributed transaction"],"description":"This article explains how to deploy Seata integrated with Nacos configuration using Docker.","author":"FUNKYE","date":"2019/12/03"},"prevItem":{"title":"Seata Configuration Center Implementation Principles","permalink":"/blog/seata-config-center"},"nextItem":{"title":"Configuring Seata Distributed Transaction with Nacos as the Configuration Center","permalink":"/blog/seata-nacos-analysis"}},"content":""},{"id":"/seata-nacos-analysis","metadata":{"permalink":"/blog/seata-nacos-analysis","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-nacos-analysis.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-nacos-analysis.md","title":"Configuring Seata Distributed Transaction with Nacos as the Configuration Center","description":"This article explains how to integrate Seata with Nacos for configuration.","date":"2019-12-02T00:00:00.000Z","formattedDate":"December 2, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"FUNKYE"}],"frontMatter":{"title":"Configuring Seata Distributed Transaction with Nacos as the Configuration Center","keywords":["Seata","Nacos","distributed transaction"],"description":"This article explains how to integrate Seata with Nacos for configuration.","author":"FUNKYE","date":"2019/12/02"},"prevItem":{"title":"Docker Deployment of Seata Integration with Nacos","permalink":"/blog/seata-nacos-docker"},"nextItem":{"title":"Resolving the Issue of Losing Mybatis-Plus Features in Seata AT Mode Integration through Source Code","permalink":"/blog/seata-mybatisplus-analysis"}},"content":""},{"id":"/seata-mybatisplus-analysis","metadata":{"permalink":"/blog/seata-mybatisplus-analysis","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-mybatisplus-analysis.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-mybatisplus-analysis.md","title":"Resolving the Issue of Losing Mybatis-Plus Features in Seata AT Mode Integration through Source Code","description":"This article explains how to resolve the issue of losing Mybatis-Plus features in Seata integration through source code.","date":"2019-11-30T00:00:00.000Z","formattedDate":"November 30, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"FUNKYE"}],"frontMatter":{"title":"Resolving the Issue of Losing Mybatis-Plus Features in Seata AT Mode Integration through Source Code","keywords":["Seata","Mybatis-Plus","distributed transaction"],"description":"This article explains how to resolve the issue of losing Mybatis-Plus features in Seata integration through source code.","author":"FUNKYE","date":"2019/11/30"},"prevItem":{"title":"Configuring Seata Distributed Transaction with Nacos as the Configuration Center","permalink":"/blog/seata-nacos-analysis"},"nextItem":{"title":"Integrating Seata Distributed Transaction with SpringBoot+Dubbo+MybatisPlus","permalink":"/blog/springboot-dubbo-mybatisplus-seata"}},"content":""},{"id":"/springboot-dubbo-mybatisplus-seata","metadata":{"permalink":"/blog/springboot-dubbo-mybatisplus-seata","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/springboot-dubbo-mybatisplus-seata.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/springboot-dubbo-mybatisplus-seata.md","title":"Integrating Seata Distributed Transaction with SpringBoot+Dubbo+MybatisPlus","description":"This article explains how to build the integration of Seata with SpringBoot+Dubbo+MybatisPlus using the direct connection approach.","date":"2019-11-29T00:00:00.000Z","formattedDate":"November 29, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"FUNKYE"}],"frontMatter":{"title":"Integrating Seata Distributed Transaction with SpringBoot+Dubbo+MybatisPlus","keywords":["Seata","dubbo","mybatis","distributed transaction"],"description":"This article explains how to build the integration of Seata with SpringBoot+Dubbo+MybatisPlus using the direct connection approach.","author":"FUNKYE","date":"2019/11/29"},"prevItem":{"title":"Resolving the Issue of Losing Mybatis-Plus Features in Seata AT Mode Integration through Source Code","permalink":"/blog/seata-mybatisplus-analysis"},"nextItem":{"title":"Does Seata Client Need to Start RM and TM Simultaneously?","permalink":"/blog/seata-at-mode-start-rm-tm"}},"content":""},{"id":"/seata-at-mode-start-rm-tm","metadata":{"permalink":"/blog/seata-at-mode-start-rm-tm","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-at-mode-start-rm-tm.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-at-mode-start-rm-tm.md","title":"Does Seata Client Need to Start RM and TM Simultaneously?","description":"A discussion point regarding future optimizations for Seata","date":"2019-11-28T00:00:00.000Z","formattedDate":"November 28, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"chenghui.zhang"}],"frontMatter":{"title":"Does Seata Client Need to Start RM and TM Simultaneously?","author":"chenghui.zhang","keywords":["Seata","distributed transaction","AT mode","RM","TM"],"description":"A discussion point regarding future optimizations for Seata","date":"2019/11/28"},"prevItem":{"title":"Integrating Seata Distributed Transaction with SpringBoot+Dubbo+MybatisPlus","permalink":"/blog/springboot-dubbo-mybatisplus-seata"},"nextItem":{"title":"Seata AT Mode Startup Source Code Analysis","permalink":"/blog/seata-at-mode-start"}},"content":""},{"id":"/seata-at-mode-start","metadata":{"permalink":"/blog/seata-at-mode-start","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-at-mode-start.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-at-mode-start.md","title":"Seata AT Mode Startup Source Code Analysis","description":"Seata Source Code Analysis Series","date":"2019-11-27T00:00:00.000Z","formattedDate":"November 27, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"chenghui.zhang"}],"frontMatter":{"title":"Seata AT Mode Startup Source Code Analysis","author":"chenghui.zhang","keywords":["Seata","distributed transaction","AT mode"],"description":"Seata Source Code Analysis Series","date":"2019/11/27"},"prevItem":{"title":"Does Seata Client Need to Start RM and TM Simultaneously?","permalink":"/blog/seata-at-mode-start-rm-tm"},"nextItem":{"title":"Designing More Flexible Financial Applications with Seata Saga","permalink":"/blog/design-more-flexable-application-by-saga"}},"content":""},{"id":"/design-more-flexable-application-by-saga","metadata":{"permalink":"/blog/design-more-flexable-application-by-saga","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/design-more-flexable-application-by-saga.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/design-more-flexable-application-by-saga.md","title":"Designing More Flexible Financial Applications with Seata Saga","description":"This article delves into the pain points of developing distributed financial applications, analyzing solutions from both theoretical and practical perspectives. It explains how to design more flexible financial applications using Seata Saga.","date":"2019-11-04T00:00:00.000Z","formattedDate":"November 4, 2019","tags":[],"readingTime":19.94,"hasTruncateMarker":false,"authors":[{"name":"long187"}],"frontMatter":{"title":"Designing More Flexible Financial Applications with Seata Saga","keywords":["Saga","Seata","Consistency","Financial","Flexibility","Distributed","Transaction"],"description":"This article delves into the pain points of developing distributed financial applications, analyzing solutions from both theoretical and practical perspectives. It explains how to design more flexible financial applications using Seata Saga.","author":"long187","date":"2019-11-04T00:00:00.000Z"},"prevItem":{"title":"Seata AT Mode Startup Source Code Analysis","permalink":"/blog/seata-at-mode-start"},"nextItem":{"title":"Comprehensive Explanation of Distributed Transaction Seata and Its Three Modes","permalink":"/blog/seata-at-tcc-saga"}},"content":"Seata, short for Simple Extensible Autonomous Transaction Architecture, is an all-in-one distributed transaction solution. It provides AT, TCC, Saga, and XA transaction modes. This article provides a detailed explanation of the Saga mode within Seata, with the project hosted on [GitHub](https://github.com/apache/incubator-seata).\\n\\nAuthor: Yiyuan (Chen Long), Core Developer of Distributed Transactions at Ant Financial, Seata Committer.\\n\\n\\n<a name=\\"uTwja\\"></a>\\n\\n# Pain Points in Financial Distributed Application Development\\n\\nDistributed systems face a prominent challenge where a business process requires a composition of various services. This challenge becomes even more pronounced in a microservices architecture, as it necessitates consistency guarantees at the business level. In other words, if a step fails, it either needs to roll back to the previous service invocation or continuously retry to ensure the success of all steps. - From \\"Left Ear Wind - Resilient Design: Compensation Transaction\\"\\n\\nIn the domain of financial microservices architecture, business processes are often more complex. Processes are lengthy, such as a typical internet microloan business process involving calls to more than ten services. When combined with exception handling processes, the complexity increases further. Developers with experience in financial business development can relate to these challenges.\\n\\nDuring the development of financial distributed applications, we encounter several pain points:\\n\\n- **Difficulty Ensuring Business Consistency**<br />\\n\\n    In many of the systems we encounter (e.g., in channel layers, product layers, and integration layers), ensuring eventual business consistency often involves adopting a \\"compensation\\" approach. Without a coordinator to support this, the development difficulty is significant. Each step requires handling \\"rollback\\" operations in catch blocks, resulting in a code structure resembling an \\"arrow,\\" with poor readability and maintainability. Alternatively, retrying exceptional operations, if unsuccessful, might lead to asynchronous retries or even manual intervention. These challenges impose a significant burden on developers, reducing development efficiency and increasing the likelihood of errors.\\n\\n- **Difficulty Managing Business State**<br />\\n\\n    With numerous business entities and their corresponding states, developers often update the entity\'s state in the database after completing a business activity. Lack of a state machine to manage the entire state transition process results in a lack of intuitiveness, increases the likelihood of errors, and causes the business to enter an incorrect state.\\n\\n- **Difficulty Ensuring Idempotence**<br />\\n\\n    Idempotence of services is a fundamental requirement in a distributed environment. Ensuring the idempotence of services often requires developers to design each service individually, using unique keys in databases or distributed caches. There is no unified solution, creating a significant burden on developers and increasing the chances of oversight, leading to financial losses.\\n\\n- **Challenges in Business Monitoring and Operations; Lack of Unified Error Guardian Capability**<br />\\n\\n    Monitoring the execution of business operations is usually done by logging, and monitoring platforms are based on log analysis. While this is generally sufficient, in the case of business errors, these monitors lack immediate access to the business context and require additional database queries. Additionally, the reliance on developers for log printing makes it prone to omissions. For compensatory transactions, there is often a need for \\"error guardian triggering compensation\\" and \\"worker-triggered compensation\\" operations. The lack of a unified error guardian and processing standard requires developers to implement these individually, resulting in a heavy development burden.\\n\\n\\n<a name=\\"hvEU6\\"></a>\\n\\n# Theoretical Foundation\\n\\nIn certain scenarios where strong consistency is required for data, we may adopt distributed transaction schemes like \\"Two-Phase Commit\\" at the business layer. However, in other scenarios, where such strong consistency is not necessary, ensuring eventual consistency is sufficient.\\n\\nFor example, Ant Financial currently employs the TCC (Try, Confirm, Cancel) pattern in its financial core systems. The characteristics of financial core systems include high consistency requirements (business isolation), short processes, and high concurrency.\\n\\nOn the other hand, in many business systems above the financial core (e.g., systems in the channel layer, product layer, and integration layer), the emphasis is on achieving eventual consistency. These systems typically have complex processes, long flows, and may need to call services from other companies (such as financial networks). Developing Try, Confirm, Cancel methods for each service in these scenarios incurs high costs. Additionally, when there are services from other companies in the transaction, it is impractical to require those services to follow the TCC development model. Long processes can negatively impact performance if transaction boundaries are too extensive.\\n\\nWhen it comes to transactions, we are familiar with ACID, and we are also acquainted with the CAP theorem, which states that at most two out of three\u2014Consistency (C), Availability (A), and Partition Tolerance (P)\u2014can be achieved simultaneously. To enhance performance, a variant of ACID known as BASE emerged. While ACID emphasizes consistency (C in CAP), BASE emphasizes availability (A in CAP). Achieving strong consistency (ACID) is often challenging, especially when dealing with multiple systems that are not provided by a single company. BASE systems are designed to create more resilient systems. In many situations, particularly when dealing with multiple systems and providers, BASE systems acknowledge the risk of data inconsistency in the short term. This allows new transactions to occur, with potentially problematic transactions addressed later through compensatory means to ensure eventual consistency.\\n\\nTherefore, in practical development, we make trade-offs. For many business systems above the financial core, compensatory transactions can be adopted. The concept of compensatory transactions has been proposed for about 30 years, with the Saga theory emerging as a solution for long transactions. With the recent rise of microservices, Saga has gradually gained attention in recent years. Currently, the industry generally recognizes Saga as a solution for handling long transactions.\\n\\n> [https://github.com/aphyr/dist-sagas/blob/master/sagas.pdf](https://github.com/aphyr/dist-sagas/blob/master/sagas.pdf)[1]\\n> [http://microservices.io/patterns/data/saga.html](http://microservices.io/patterns/data/saga.html)[2]\\n\\n\\n<a name=\\"k8kbY\\"></a>\\n\\n# Community and Industry Solutions\\n\\n<a name=\\"Oc5Er\\"></a>\\n## Apache Camel Saga\\n\\nCamel is an open-source product that implements Enterprise Integration Patterns (EIP). It is based on an event-driven architecture and offers good performance and throughput. In version 2.21, Camel introduced the Saga EIP.\\n\\nThe Saga EIP provides a way to define a series of related actions through Camel routes. These actions either all succeed or all roll back. Saga can coordinate distributed services or local services using any communication protocol, achieving global eventual consistency. Saga does not require the entire process to be completed in a short time because it does not occupy any database locks. It can support requests that require long processing times, ranging from seconds to days. Camel\'s Saga EIP is based on [MicroProfile\'s LRA](https://github.com/eclipse/microprofile-sandbox/tree/master/proposals/0009-LRA)[3] (Long Running Action). It also supports the coordination of distributed services implemented in any language using any communication protocol.\\n\\nThe implementation of Saga does not lock data. Instead, it defines \\"compensating operations\\" for each operation. When an error occurs during the normal process execution, the \\"compensating operations\\" for the operations that have already been executed are triggered to roll back the process. \\"Compensating operations\\" can be defined on Camel routes using Java or XML DSL (Definition Specific Language).\\n\\nHere is an example of Java DSL:\\n\\n```java\\n// Java DSL example goes here\\n\\n```java\\n// action\\nfrom(\\"direct:reserveCredit\\")\\n  .bean(idService, \\"generateCustomId\\") // generate a custom Id and set it in the body\\n  .to(\\"direct:creditReservation\\")\\n\\n// delegate action\\nfrom(\\"direct:creditReservation\\")\\n  .saga()\\n  .propagation(SagaPropagation.SUPPORTS)\\n  .option(\\"CreditId\\", body()) // mark the current body as needed in the compensating action\\n  .compensation(\\"direct:creditRefund\\")\\n    .bean(creditService, \\"reserveCredit\\")\\n    .log(\\"Credit ${header.amount} reserved. Custom Id used is ${body}\\");\\n\\n// called only if the saga is cancelled\\nfrom(\\"direct:creditRefund\\")\\n  .transform(header(\\"CreditId\\")) // retrieve the CreditId option from headers\\n  .bean(creditService, \\"refundCredit\\")\\n  .log(\\"Credit for Custom Id ${body} refunded\\");\\n```\\n\\nXML DSL sample:\\n```xml\\n<route>\\n  <from uri=\\"direct:start\\"/>\\n  <saga>\\n    <compensation uri=\\"direct:compensation\\" />\\n    <completion uri=\\"direct:completion\\" />\\n    <option optionName=\\"myOptionKey\\">\\n      <constant>myOptionValue</constant>\\n    </option>\\n    <option optionName=\\"myOptionKey2\\">\\n      <constant>myOptionValue2</constant>\\n    </option>\\n  </saga>\\n  <to uri=\\"direct:action1\\" />\\n  <to uri=\\"direct:action2\\" />\\n</route>\\n```\\n\\n<a name=\\"pQWuF\\"></a>\\n\\n## Eventuate Tram Saga\\n\\n[Eventuate Tram Saga](https://github.com/eventuate-tram/eventuate-tram-sagas)[4]\xa0The framework is a Saga framework for Java microservices using JDBC/JPA. Similar to Camel Saga, it also adopts Java DSL to define compensating operations:\\n\\n\\n```java\\npublic class CreateOrderSaga implements SimpleSaga<CreateOrderSagaData> {\\n\\n  private SagaDefinition<CreateOrderSagaData> sagaDefinition =\\n          step()\\n            .withCompensation(this::reject)\\n          .step()\\n            .invokeParticipant(this::reserveCredit)\\n          .step()\\n            .invokeParticipant(this::approve)\\n          .build();\\n\\n\\n  @Override\\n  public SagaDefinition<CreateOrderSagaData> getSagaDefinition() {\\n    return this.sagaDefinition;\\n  }\\n\\n\\n  private CommandWithDestination reserveCredit(CreateOrderSagaData data) {\\n    long orderId = data.getOrderId();\\n    Long customerId = data.getOrderDetails().getCustomerId();\\n    Money orderTotal = data.getOrderDetails().getOrderTotal();\\n    return send(new ReserveCreditCommand(customerId, orderId, orderTotal))\\n            .to(\\"customerService\\")\\n            .build();\\n\\n...\\n```\\n\\n<a name=\\"scN9h\\"></a>\\n\\n## Apache ServiceComb Saga\\n\\n[ServiceComb Saga](https://github.com/apache/incubator-servicecomb-saga)[5] is also a solution for achieving data eventual consistency in microservices applications. In contrast to [TCC](http://design.inf.usi.ch/sites/default/files/biblio/rest-tcc.pdf), Saga directly commits transactions in the try phase, and the subsequent rollback phase is completed through compensating operations in reverse. What sets it apart is the use of Java annotations and interceptors to define \\"compensating\\" services.<br />\\n\\n#### Architecture:\\n\\nSaga consists of **alpha** and **omega**, where:\\n\\n- Alpha acts as the coordinator, primarily responsible for managing and coordinating transactions;<br />\\n- Omega is an embedded agent in microservices, responsible for intercepting network requests and reporting transaction events to alpha;<br />\\n\\nThe diagram below illustrates the relationship between alpha, omega, and microservices:<br />\\n\\n![ServiceComb Saga](/img/saga/service-comb-saga.png?raw=true)\\n\\n<a name=\\"ggflbq\\"></a>\\n\\n#### sample\uff1a\\n```java\\npublic class ServiceA extends AbsService implements IServiceA {\\n\\n  private static final Logger LOG = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\\n\\n  @Autowired\\n  private IServiceB serviceB;\\n\\n  @Autowired\\n  private IServiceC serviceC;\\n\\n  @Override\\n  public String getServiceName() {\\n    return \\"servicea\\";\\n  }\\n\\n  @Override\\n  public String getTableName() {\\n    return \\"testa\\";\\n  }\\n\\n  @Override\\n  @SagaStart\\n  @Compensable(compensationMethod = \\"cancelRun\\")\\n  @Transactional(rollbackFor = Exception.class)\\n  public Object run(InvokeContext invokeContext) throws Exception {\\n    LOG.info(\\"A.run called\\");\\n    doRunBusi();\\n    if (invokeContext.isInvokeB(getServiceName())) {\\n      serviceB.run(invokeContext);\\n    }\\n    if (invokeContext.isInvokeC(getServiceName())) {\\n      serviceC.run(invokeContext);\\n    }\\n    if (invokeContext.isException(getServiceName())) {\\n      LOG.info(\\"A.run exception\\");\\n      throw new Exception(\\"A.run exception\\");\\n    }\\n    return null;\\n  }\\n\\n  public void cancelRun(InvokeContext invokeContext) {\\n    LOG.info(\\"A.cancel called\\");\\n    doCancelBusi();\\n  }\\n```\\n\\n<a name=\\"CnD8r\\"></a>\\n\\n## Ant Financial\'s Practice\\n\\nAnt Financial extensively uses the TCC mode for distributed transactions, mainly in scenarios where high consistency and performance are required, such as in financial core systems. In upper-level business systems with complex and lengthy processes, developing TCC can be costly. In such cases, most businesses opt for the Saga mode to achieve eventual business consistency. Due to historical reasons, different business units have their own set of \\"compensating\\" transaction solutions, basically falling into two categories:\\n\\n1. When a service needs to \\"retry\\" or \\"compensate\\" in case of failure, a record is inserted into the database with the status before executing the service. When an exception occurs, a scheduled task queries the database record and performs \\"retry\\" or \\"compensation.\\" If the business process is successful, the record is deleted.\\n\\n2. Designing a state machine engine and a simple DSL to orchestrate business processes and record business states. The state machine engine can define \\"compensating services.\\" In case of an exception, the state machine engine invokes \\"compensating services\\" in reverse. There is also an \\"error guardian\\" platform that monitors failed or uncompensated business transactions and continuously performs \\"compensation\\" or \\"retry.\\"\\n\\n## Solution Comparison\\n\\nGenerally, there are two common solutions in the community and industry: one is based on a state machine or a process engine that orchestrates processes and defines compensation through DSL; the other is based on Java annotations and interceptors to implement compensation. What are the advantages and disadvantages of these two approaches?\\n\\n| Approach             | Pros                                                                                                                                                                                                                                                                                                  | Cons                                                                                                                                                                            |\\n| -------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| State Machine + DSL  | <br />- Business processes can be defined using visual tools, standardized, readable, and can achieve service orchestration functionality<br />- Improves communication efficiency between business analysts and developers<br />- Business state management: Processes are essentially state machines, reflecting the flow of business states<br />- Enhances flexibility in exception handling: Can implement \\"forward retry\\" or \\"backward compensation\\" after recovery from a crash<br />- Naturally supports asynchronous processing engines such as Actor model or SEDA architecture, improving overall throughput<br /> | <br />- Business processes are composed of JAVA programs and DSL configurations, making development relatively cumbersome<br />- High intrusiveness into existing business if it is a transformation<br />- High implementation cost of the engine<br />          |\\n| Interceptor + Java Annotation | <br />- Programs and annotations are integrated, simple development, low learning curve<br />- Easy integration into existing businesses<br />- Low framework implementation cost                                                                                                                                                                              | <br />- The framework cannot provide asynchronous processing modes such as the Actor model or SEDA architecture to improve system throughput<br />- The framework cannot provide business state management<br />- Difficult to achieve \\"forward retry\\" after crash recovery due to the inability to restore thread context<br />          |\\n\\n## Seata Saga Approach\\n\\nThe introduction of Seata Saga can be found in [Seata Saga Official Documentation](http://seata.io/zh-cn/docs/user/saga.html)[6].\\n\\nSeata Saga adopts the state machine + DSL approach for the following reasons:\\n\\n- The state machine + DSL approach is more widely used in practical production scenarios.\\n- Can use asynchronous processing engines such as the Actor model or SEDA architecture to improve overall throughput.\\n- Typically, business systems above the core system have \\"service orchestration\\" requirements, and service orchestration has transactional eventual consistency requirements. These two are challenging to separate. The state machine + DSL approach can simultaneously meet these two requirements.\\n- Because Saga mode theoretically does not guarantee isolation, in extreme cases, it may not complete the rollback operation due to dirty writing. For example, in a distributed transaction, if you recharge user A first and then deduct the balance from user B, if A user consumes the balance before the transaction is committed, and the transaction is rolled back, there is no way to compensate. Some business scenarios may allow the business to eventually succeed, and in cases where rollback is impossible, it can continue to retry the subsequent process. The state machine + DSL approach can achieve the ability to \\"forward\\" recover context and continue execution, making the business eventually successful and achieving eventual consistency.\\n\\n> In cases where isolation is not guaranteed: When designing business processes, follow the principle of \\"prefer long\u6b3e, not short\u6b3e.\\" Long\u6b3e means fewer funds for customers and more funds for institutions. Institutions can refund customers based on their credibility. Conversely, short\u6b3e means less funding for institutions, and the funds may not be recovered. Therefore, in business process design, deduction should be done first.\\n\\n### State Definition Language (Seata State Language)\\n\\n1. Define the service call process through a state diagram and generate a JSON state language definition file.\\n2. In the state diagram, a node can be a service call, and the node can configure its compensating node.\\n3. The JSON state diagram is driven by the state machine engine. When an exception occurs, the state engine executes the compensating node corresponding to the successfully executed node to roll back the transaction.\\n> Note: Whether to compensate when an exception occurs can also be user-defined.\\n\\n4. It can meet service orchestration requirements, supporting one-way selection, concurrency, asynchronous, sub-state machine, parameter conversion, parameter mapping, service execution status judgment, exception capture, and other functions.\\n\\nAssuming a business process calls two services, deducting inventory (InventoryService) and deducting balance (BalanceService), to ensure that in a distributed scenario, either both succeed or both roll back. Both participant services have a `reduce` method for inventory deduction or balance deduction, and a `compensateReduce` method for compensating deduction operations. Let\'s take a look at the interface definition of InventoryService:\\n\\n```java\\npublic interface InventoryService {\\n\\n    /**\\n     * reduce\\n     * @param businessKey\\n     * @param amount\\n     * @param params\\n     * @return\\n     */\\n    boolean reduce(String businessKey, BigDecimal amount, Map<String, Object> params);\\n\\n    /**\\n     * compensateReduce\\n     * @param businessKey\\n     * @param params\\n     * @return\\n     */\\n    boolean compensateReduce(String businessKey, Map<String, Object> params);\\n}\\n```\\n\\n## This is the state diagram corresponding to the business process:\\n\\n![Example State Diagram](/img/saga/demo_statelang.png?raw=true)\\n<br />Corresponding JSON\\n\\n\\n```json\\n{\\n    \\"Name\\": \\"reduceInventoryAndBalance\\",\\n    \\"Comment\\": \\"reduce inventory then reduce balance in a transaction\\",\\n    \\"StartState\\": \\"ReduceInventory\\",\\n    \\"Version\\": \\"0.0.1\\",\\n    \\"States\\": {\\n        \\"ReduceInventory\\": {\\n            \\"Type\\": \\"ServiceTask\\",\\n            \\"ServiceName\\": \\"inventoryAction\\",\\n            \\"ServiceMethod\\": \\"reduce\\",\\n            \\"CompensateState\\": \\"CompensateReduceInventory\\",\\n            \\"Next\\": \\"ChoiceState\\",\\n            \\"Input\\": [\\n                \\"$.[businessKey]\\",\\n                \\"$.[count]\\"\\n            ],\\n            \\"Output\\": {\\n                \\"reduceInventoryResult\\": \\"$.#root\\"\\n            },\\n            \\"Status\\": {\\n                \\"#root == true\\": \\"SU\\",\\n                \\"#root == false\\": \\"FA\\",\\n                \\"$Exception{java.lang.Throwable}\\": \\"UN\\"\\n            }\\n        },\\n        \\"ChoiceState\\":{\\n            \\"Type\\": \\"Choice\\",\\n            \\"Choices\\":[\\n                {\\n                    \\"Expression\\":\\"[reduceInventoryResult] == true\\",\\n                    \\"Next\\":\\"ReduceBalance\\"\\n                }\\n            ],\\n            \\"Default\\":\\"Fail\\"\\n        },\\n        \\"ReduceBalance\\": {\\n            \\"Type\\": \\"ServiceTask\\",\\n            \\"ServiceName\\": \\"balanceAction\\",\\n            \\"ServiceMethod\\": \\"reduce\\",\\n            \\"CompensateState\\": \\"CompensateReduceBalance\\",\\n            \\"Input\\": [\\n                \\"$.[businessKey]\\",\\n                \\"$.[amount]\\",\\n                {\\n                    \\"throwException\\" : \\"$.[mockReduceBalanceFail]\\"\\n                }\\n            ],\\n            \\"Output\\": {\\n                \\"compensateReduceBalanceResult\\": \\"$.#root\\"\\n            },\\n            \\"Status\\": {\\n                \\"#root == true\\": \\"SU\\",\\n                \\"#root == false\\": \\"FA\\",\\n                \\"$Exception{java.lang.Throwable}\\": \\"UN\\"\\n            },\\n            \\"Catch\\": [\\n                {\\n                    \\"Exceptions\\": [\\n                        \\"java.lang.Throwable\\"\\n                    ],\\n                    \\"Next\\": \\"CompensationTrigger\\"\\n                }\\n            ],\\n            \\"Next\\": \\"Succeed\\"\\n        },\\n        \\"CompensateReduceInventory\\": {\\n            \\"Type\\": \\"ServiceTask\\",\\n            \\"ServiceName\\": \\"inventoryAction\\",\\n            \\"ServiceMethod\\": \\"compensateReduce\\",\\n            \\"Input\\": [\\n                \\"$.[businessKey]\\"\\n            ]\\n        },\\n        \\"CompensateReduceBalance\\": {\\n            \\"Type\\": \\"ServiceTask\\",\\n            \\"ServiceName\\": \\"balanceAction\\",\\n            \\"ServiceMethod\\": \\"compensateReduce\\",\\n            \\"Input\\": [\\n                \\"$.[businessKey]\\"\\n            ]\\n        },\\n        \\"CompensationTrigger\\": {\\n            \\"Type\\": \\"CompensationTrigger\\",\\n            \\"Next\\": \\"Fail\\"\\n        },\\n        \\"Succeed\\": {\\n            \\"Type\\":\\"Succeed\\"\\n        },\\n        \\"Fail\\": {\\n            \\"Type\\":\\"Fail\\",\\n            \\"ErrorCode\\": \\"PURCHASE_FAILED\\",\\n            \\"Message\\": \\"purchase failed\\"\\n        }\\n    }\\n}\\n```\\n\\n## This is the state language to some extent referring to [AWS Step Functions](https://docs.aws.amazon.com/zh_cn/step-functions/latest/dg/tutorial-creating-lambda-state-machine.html)[7].\\n\\n<a name=\\"2de9b28a\\"></a>\\n\\n#### Introduction to \\"State Machine\\" Attributes:\\n\\n- Name: Represents the name of the state machine, must be unique;\\n- Comment: Description of the state machine;\\n- Version: Version of the state machine definition;\\n- StartState: The first \\"state\\" to run when starting;\\n- States: List of states, a map structure, where the key is the name of the \\"state,\\" which must be unique within the state machine;\\n\\n<a name=\\"2b956670\\"></a>\\n\\n#### Introduction to \\"State\\" Attributes:\\n\\n- Type: The type of the \\"state,\\" such as:\\n  - ServiceTask: Executes the service task;\\n  - Choice: Single conditional choice route;\\n  - CompensationTrigger: Triggers the compensation process;\\n  - Succeed: Normal end of the state machine;\\n  - Fail: Exceptional end of the state machine;\\n  - SubStateMachine: Calls a sub-state machine;\\n- ServiceName: Service name, usually the beanId of the service;\\n- ServiceMethod: Service method name;\\n- CompensateState: Compensatory \\"state\\" for this state;\\n- Input: List of input parameters for the service call, an array corresponding to the parameter list of the service method, $. represents using an expression to retrieve parameters from the state machine context. The expression uses [SpringEL](https://docs.spring.io/spring/docs/4.3.10.RELEASE/spring-framework-reference/html/expressions.html)[8], and if it is a constant, write the value directly;\\n- Output: Assigns the parameters returned by the service to the state machine context, a map structure, where the key is the key when placing it in the state machine context (the state machine context is also a map), and the value uses $. as a SpringEL expression, indicating the value is taken from the return parameters of the service, #root represents the entire return parameters of the service;\\n- Status: Mapping of the service execution status, the framework defines three statuses, SU success, FA failure, UN unknown. We need to map the execution status of the service into these three statuses, helping the framework judge the overall consistency of the transaction. It is a map structure, where the key is a condition expression, usually based on the return value of the service or the exception thrown for judgment. The default is a SpringEL expression to judge the return parameters of the service. Those starting with $Exception{ indicate judging the exception type, and the value is mapped to this value when this condition expression is true;\\n- Catch: Route after catching an exception;\\n- Next: The next \\"state\\" to execute after the service is completed;\\n- Choices: List of optional branches in the Choice type \\"state,\\" where Expression is a SpringEL expression, and Next is the next \\"state\\" to execute when the expression is true;\\n- ErrorCode: Error code for the Fail type \\"state\\";\\n- Message: Error message for the Fail type \\"state\\";\\n\\nFor more detailed explanations of the state language, please refer to [Seata Saga Official Documentation](http://seata.io/zh-cn/docs/user/saga.html)[6[http://seata.io/zh-cn/docs/user/saga.html](http://seata.io/zh-cn/docs/user/saga.html)].\\n\\n<a name=\\"209f0e37\\"></a>\\n\\n### State Machine Engine Principle:\\n\\n![State Machine Engine Principle](/img/saga/saga_engine_mechanism.png?raw=true)\\n\\n- The state diagram in the image first executes stateA, then executes stateB, and then executes stateC;\\n- The execution of \\"states\\" is based on an event-driven model. After stateA is executed, a routing message is generated and placed in the EventQueue. The event consumer takes the message from the EventQueue and executes stateB;\\n- When the entire state machine is started, Seata Server is called to start a distributed transaction, and the xid is generated. Then, the start event of the \\"state machine instance\\" is recorded in the local database;\\n- When a \\"state\\" is executed, Seata Server is called to register a branch transaction, and the branchId is generated. Then, the start event of the \\"state instance\\" is recorded in the local database;\\n- After a \\"state\\" is executed, the end event of the \\"state instance\\" is recorded in the local database, and Seata Server is called to report the status of the branch transaction;\\n- When the entire state machine is executed, the completion event of the \\"state machine instance\\" is recorded in the local database, and Seata Server is called to commit or roll back the distributed transaction;\\n\\n\\n<a name=\\"808e95dc\\"></a>\\n\\n### Design of State Machine Engine:\\n\\n![Design of State Machine Engine](/img/saga/saga_engine.png?raw=true)\\n\\nThe design of the state machine engine is mainly divided into three layers, with the upper layer depending on the lower layer. From bottom to top, they are:\\n\\n- Eventing Layer:\\n  - Implements an event-driven architecture that can push events and be consumed by a consumer. This layer does not care about what the event is or what the consumer executes; it is implemented by the upper layer.\\n\\n- ProcessController Layer:\\n  - Driven by the above Eventing to execute a \\"empty\\" process. The behavior and routing of \\"states\\" are not implemented. It is implemented by the upper layer.\\n> Based on the above two layers, theoretically, any \\"process\\" engine can be customly extended. The design of these two layers is based on the internal design of the financial network platform.\\n\\n\\n- StateMachineEngine Layer:\\n  - Implements the behavior and routing logic of each type of state in the state machine engine;\\n  - Provides API and state machine language repository;\\n\\n<a name=\\"73a9fddd\\"></a>\\n\\n### Practical Experience in Service Design under Saga Mode\\n\\nBelow are some practical experiences summarized in the design of microservices under Saga mode. Of course, these are recommended practices, not necessarily to be followed 100%. There are \\"workaround\\" solutions even if not followed.\\n> Good news: Seata Saga mode has no specific requirements for the interface parameters of microservices, making Saga mode suitable for integrating legacy systems or services from external institutions.\\n\\n\\n<a name=\\"d64c5051\\"></a>\\n\\n#### Allow Empty Compensation\\n\\n- Empty Compensation: The original service was not executed, but the compensation service was executed;\\n- Reasons:\\n  - Timeout (packet loss) of the original service;\\n  - Saga transaction triggers a rollback;\\n  - The request of the original service is not received, but the compensation request is received first;\\n\\nTherefore, when designing services, it is necessary to allow empty compensation, that is, if the business primary key to be compensated is not found, return compensation success and record the original business primary key.\\n\\n<a name=\\"88a92b17\\"></a>\\n\\n#### Hang Prevention Control\\n\\n- Hang: Compensation service is executed before the original service;\\n- Reasons:\\n  - Timeout (congestion) of the original service;\\n  - Saga transaction rollback triggers a rollback;\\n  - Congested original service arrives;\\n\\nTherefore, check whether the current business primary key already exists in the business primary keys recorded by empty compensation. If it exists, reject the execution of the service.\\n\\n<a name=\\"ce766631\\"></a>\\n\\n#### Idempotent Control\\n\\n- Both the original service and the compensation service need to ensure idempotence. Due to possible network timeouts, a retry strategy can be set. When a retry occurs, idempotent control should be used to avoid duplicate updates to business data.\\n\\n<a name=\\"FO5YS\\"></a>\\n\\n# Summary\\n\\nMany times, we don\'t need to emphasize strong consistency. We design more resilient systems based on the BASE and Saga theories to achieve better performance and fault tolerance in distributed architecture. There is no silver bullet in distributed architecture, only solutions suitable for specific scenarios. In fact, Seata Saga is a product with the capabilities of \\"service orchestration\\" and \\"Saga distributed transactions.\\" Summarizing, its applicable scenarios are:\\n\\n- Suitable for handling \\"long transactions\\" in a microservices architecture;\\n- Suitable for \\"service orchestration\\" requirements in a microservices architecture;\\n- Suitable for business systems with a large number of composite services above the financial core system (such as systems in the channel layer, product layer, integration layer);\\n- Suitable for scenarios where integration with services provided by legacy systems or external institutions is required (these services are immutable and cannot be required to be modified).\\n\\n<a name=\\"3X7vO\\"></a>\\n\\n## Related Links Mentioned in the Article\\n\\n[1][https://github.com/aphyr/dist-sagas/blob/master/sagas.pdf](https://github.com/aphyr/dist-sagas/blob/master/sagas.pdf)<br />[2][http://microservices.io/patterns/data/saga.html](http://microservices.io/patterns/data/saga.html)<br />[3][Microprofile \u7684 LRA](https://github.com/eclipse/microprofile-sandbox/tree/master/proposals/0009-LRA)\uff1a[https://github.com/eclipse/microprofile-sandbox/tree/master/proposals/0009-LRA](https://github.com/eclipse/microprofile-sandbox/tree/master/proposals/0009-LRA)<br />[4][Eventuate Tram Saga](https://github.com/eventuate-tram/eventuate-tram-sagas)\uff1a[https://github.com/eventuate-tram/eventuate-tram-sagas](https://github.com/eventuate-tram/eventuate-tram-sagas)<br />[5][ServiceComb Saga](https://github.com/apache/incubator-servicecomb-saga)\uff1a[https://github.com/apache/servicecomb-pack](https://github.com/apache/servicecomb-pack)<br />[6][Seata Saga \u5b98\u7f51\u6587\u6863](http://seata.io/zh-cn/docs/user/saga.html)\uff1a[http://seata.io/zh-cn/docs/user/saga.html](http://seata.io/zh-cn/docs/user/saga.html)<br />[7][AWS Step Functions](https://docs.aws.amazon.com/zh_cn/step-functions/latest/dg/tutorial-creating-lambda-state-machine.html)\uff1a[https://docs.aws.amazon.com/zh_cn/step-functions/latest/dg/tutorial-creating-lambda-state-machine.html](https://docs.aws.amazon.com/zh_cn/step-functions/latest/dg/tutorial-creating-lambda-state-machine.html)<br />[8][SpringEL](https://docs.spring.io/spring/docs/4.3.10.RELEASE/spring-framework-reference/html/expressions.html)\uff1a[https://docs.spring.io/spring/docs/4.3.10.RELEASE/spring-framework-reference/html/expressions.html](https://docs.spring.io/spring/docs/4.3.10.RELEASE/spring-framework-reference/html/expressions.html)<br />"},{"id":"/seata-at-tcc-saga","metadata":{"permalink":"/blog/seata-at-tcc-saga","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-at-tcc-saga.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-at-tcc-saga.md","title":"Comprehensive Explanation of Distributed Transaction Seata and Its Three Modes","description":"This article focuses on sharing the background and theoretical foundation of distributed transactions, as well as the principles of Seata distributed transactions and the implementation of three modes (AT, TCC, Saga) of distributed transactions.","date":"2019-08-11T00:00:00.000Z","formattedDate":"August 11, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"long187"}],"frontMatter":{"title":"Comprehensive Explanation of Distributed Transaction Seata and Its Three Modes","keywords":["Saga","Seata","AT","TCC","consistency","finance","distributed","transaction"],"description":"This article focuses on sharing the background and theoretical foundation of distributed transactions, as well as the principles of Seata distributed transactions and the implementation of three modes (AT, TCC, Saga) of distributed transactions.","author":"long187","date":"2019-08-11T00:00:00.000Z"},"prevItem":{"title":"Designing More Flexible Financial Applications with Seata Saga","permalink":"/blog/design-more-flexable-application-by-saga"},"nextItem":{"title":"Design Principles of Distributed Transaction Middleware Seata","permalink":"/blog/seata-at-mode-design"}},"content":""},{"id":"/seata-at-mode-design","metadata":{"permalink":"/blog/seata-at-mode-design","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-at-mode-design.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-at-mode-design.md","title":"Design Principles of Distributed Transaction Middleware Seata","description":"Design principles of AT mode","date":"2019-07-11T00:00:00.000Z","formattedDate":"July 11, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"chenghui.zhang"}],"frontMatter":{"title":"Design Principles of Distributed Transaction Middleware Seata","author":"chenghui.zhang","keywords":["Seata","distributed transaction","AT mode"],"description":"Design principles of AT mode","date":"2019/07/11"},"prevItem":{"title":"Comprehensive Explanation of Distributed Transaction Seata and Its Three Modes","permalink":"/blog/seata-at-tcc-saga"},"nextItem":{"title":"Seata Distributed Go Server Officially Open Source - Introduction to TaaS Design","permalink":"/blog/seata-analysis-go-server"}},"content":""},{"id":"/seata-analysis-go-server","metadata":{"permalink":"/blog/seata-analysis-go-server","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-go-server.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-go-server.md","title":"Seata Distributed Go Server Officially Open Source - Introduction to TaaS Design","description":"","date":"2019-04-23T00:00:00.000Z","formattedDate":"April 23, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"fagongzi(zhangxu19830126@gmail.com)"}],"frontMatter":{"title":"Seata Distributed Go Server Officially Open Source - Introduction to TaaS Design","author":"fagongzi(zhangxu19830126@gmail.com)","date":"2019/04/23","keywords":["seata","distributed transaction","high availability"]},"prevItem":{"title":"Design Principles of Distributed Transaction Middleware Seata","permalink":"/blog/seata-at-mode-design"},"nextItem":{"title":"Fescar Integration with Spring Cloud In-Depth Analysis of Source Code","permalink":"/blog/how-to-support-spring-cloud"}},"content":""},{"id":"/how-to-support-spring-cloud","metadata":{"permalink":"/blog/how-to-support-spring-cloud","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/how-to-support-spring-cloud.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/how-to-support-spring-cloud.md","title":"Fescar Integration with Spring Cloud In-Depth Analysis of Source Code","description":"Fescar","date":"2019-04-15T00:00:00.000Z","formattedDate":"April 15, 2019","tags":[],"readingTime":16.17,"hasTruncateMarker":false,"authors":[{"name":"shukang.guo min.ji"}],"frontMatter":{"title":"Fescar Integration with Spring Cloud In-Depth Analysis of Source Code","author":"shukang.guo min.ji","date":"2019/04/15","keywords":["fescar","seata","Distributed","transaction"]},"prevItem":{"title":"Seata Distributed Go Server Officially Open Source - Introduction to TaaS Design","permalink":"/blog/seata-analysis-go-server"},"nextItem":{"title":"Integrating Seata (formerly Fescar) Distributed Transaction with Spring Cloud","permalink":"/blog/integrate-seata-with-spring-cloud"}},"content":"### Fescar \\n\\nCommon distributed transaction approaches include XA based on 2PC (e.g., Atomikos), TCC (e.g., ByteTCC) focusing on the business layer, and transactional messaging (e.g., RocketMQ Half Message). XA is a protocol for distributed transactions that requires support from local databases. However, the resource locking at the database level can lead to poor performance. On the other hand, TCC, introduced by Alibaba as a preacher, requires a significant amount of business code to ensure transactional consistency, resulting in higher development and maintenance costs.\\n\\nDistributed transactions are a widely discussed topic in the industry, and this is one of the reasons why Fescar has gained 6k stars in a short period of time. The name \\"Fescar\\" stands for Fast & Easy Commit And Rollback. In simple terms, Fescar drives global transactions by coordinating local RDBMS branch transactions. It is a middleware that operates at the application layer. The main advantages of Fescar are better performance compared to XA, as it does not occupy connection resources for a long time, and lower development cost and business invasiveness compared to TCC.\\n\\nSimilar to XA, Fescar divides roles into TC (Transaction Coordinator), RM (Resource Manager), and TM (Transaction Manager). The overall transaction process model of Fescar is as follows:\\n\\n![Fescar\u4e8b\u52a1\u8fc7\u7a0b](/img/blog/fescar-microservices.png)\\n\\n```\\n1.The TM (Transaction Manager) requests the TC (Transaction Coordinator) to start a global transaction. The global transaction is successfully created, and a globally unique XID (Transaction ID) is generated.\\n2.The XID is propagated in the context of the microservice invocation chain.\\n3.The RM (Resource Manager) registers the branch transaction with the TC, bringing it under the jurisdiction of the global transaction corresponding to the XID.\\n4.The TM initiates a global commit or rollback resolution for the XID with the TC.\\n5.The TC schedules the completion of commit or rollback requests for all branch transactions under the jurisdiction of the XID.\\n```\\n\\nIn the current implementation version, the TC (Transaction Coordinator) is deployed as a separate process. It is responsible for maintaining the operation records and global lock records of the global transaction, as well as coordinating and driving the global transaction\'s commit or rollback. On the other hand, the TM (Transaction Manager) and RM (Resource Manager) work in the same application process as the application.\\n\\nThe RM manages the underlying database through proxying the JDBC data source. It uses syntax parsing to retain snapshots and generate undo logs during transaction execution. This ensures that the transaction can be rolled back to its previous state if needed.\\n\\nThis covers the general flow and model division of Fescar. Now, let\'s proceed with the analysis of Fescar\'s transaction propagation mechanism.\\n\\n\\n### Fescar Transaction Propagation Mechanism\\n\\nThe transaction propagation in Fescar includes both nested transaction calls within an application and transaction propagation across different services. So, how does Fescar propagate transactions in a microservices call chain? Fescar provides a transaction API that allows users to manually bind a transaction\'s XID and join it to the global transaction. Therefore, depending on the specific service framework mechanism, we can propagate the XID in the call chain to achieve transaction propagation.\\n\\nThe RPC request process consists of two parts: the caller and the callee. We need to handle the XID during the request and response. The general process is as follows: the caller (or the requester) retrieves the XID from the current transaction context and passes it to the callee through the RPC protocol. The callee extracts the XID from the request and binds it to its own transaction context, thereby participating in the global transaction. Common microservices frameworks usually provide corresponding Filter and Interceptor mechanisms. Now, let\'s analyze the integration process of Spring Cloud and Fescar in more detail.\\n\\n### Partial Source Code Analysis of Fescar Integration with Spring Cloud Alibaba\\n\\nThis section of the source code is entirely from spring-cloud-alibaba-fescar. The source code analysis mainly includes three parts: AutoConfiguration, the microservice provider, and the microservice consumer. Regarding the microservice consumer, it can be further divided into two specific approaches: RestTemplate and Feign. For the Feign request approach, it is further categorized into usage patterns that integrate with Hystrix and Sentine.\\n\\n#### Fescar AutoConfiguration\\nFor the AutoConfiguration analysis, this section will only cover the parts related to the startup of Fescar. The analysis of other parts will be interspersed in the \'Microservice Provider\' and \'Microservice Consumer\' sections.\\n\\nThe startup of Fescar requires the configuration of GlobalTransactionScanner. The GlobalTransactionScanner is responsible for initializing Fescar\'s RM client, TM client, and automatically proxying classes annotated with the GlobalTransactional annotation. The startup of the GlobalTransactionScanner bean is loaded and injected through GlobalTransactionAutoConfiguration, which also injects FescarProperties.\\n\\nFescarProperties contains important properties of Fescar, such as txServiceGroup. The value of this property can be read from the application.properties file using the key \'spring.cloud.alibaba.fescar.txServiceGroup\', with a default value of \'${spring.application.name}-fescar-service-group\'. txServiceGroup represents the logical transaction group name in Fescar. This group name is obtained from the configuration center (currently supporting file and Apollo) to retrieve the TC cluster name corresponding to the logical transaction group name. The TC cluster\'s service name is then constructed based on the cluster name. The RM client, TM client, and TC interact through RPC by using the registry center (currently supporting Nacos, Redis, ZooKeeper, and Eureka) and the service name to find available TC service nodes.\\n\\n#### Microservice Provider\\n\\nSince the logic of the consumer is a bit more complex, let\'s first analyze the logic of the provider. For Spring Cloud projects, the default RPC transport protocol is HTTP, so the HandlerInterceptor mechanism is used to intercept HTTP requests.\\n\\nHandlerInterceptor is an interface provided by Spring, and it has three methods that can be overridden.\\n\\n```java\\n    /**\\n\\t * Intercept the execution of a handler. Called after HandlerMapping determined\\n\\t * an appropriate handler object, but before HandlerAdapter invokes the handler.\\n\\t */\\n\\tdefault boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler)\\n\\t\\t\\tthrows Exception {\\n\\n\\t\\treturn true;\\n\\t}\\n\\n\\t/**\\n\\t * Intercept the execution of a handler. Called after HandlerAdapter actually\\n\\t * invoked the handler, but before the DispatcherServlet renders the view.\\n\\t * Can expose additional model objects to the view via the given ModelAndView.\\n\\t */\\n\\tdefault void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler,\\n\\t\\t\\t@Nullable ModelAndView modelAndView) throws Exception {\\n\\t}\\n\\n\\t/**\\n\\t * Callback after completion of request processing, that is, after rendering\\n\\t * the view. Will be called on any outcome of handler execution, thus allows\\n\\t * for proper resource cleanup.\\n\\t */\\n\\tdefault void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler,\\n\\t\\t\\t@Nullable Exception ex) throws Exception {\\n\\t}\\n```\\n\\nAccording to the comments, we can clearly see the timing and common use cases of each method. For Fescar integration, it overrides the preHandle and afterCompletion methods as needed.\\n\\nThe purpose of FescarHandlerInterceptor is to bind the XID passed from the service chain to the transaction context of the service node and clean up related resources after the request is completed. FescarHandlerInterceptorConfiguration is responsible for configuring the interception of all URLs. This interceptor will be executed for all incoming requests to perform XID conversion and transaction binding.\\n\\n```java\\n/**\\n * @author xiaojing\\n *\\n * Fescar HandlerInterceptor, Convert Fescar information into\\n * @see com.alibaba.fescar.core.context.RootContext from http request\'s header in\\n * {@link org.springframework.web.servlet.HandlerInterceptor#preHandle(HttpServletRequest , HttpServletResponse , Object )},\\n * And clean up Fescar information after servlet method invocation in\\n * {@link org.springframework.web.servlet.HandlerInterceptor#afterCompletion(HttpServletRequest, HttpServletResponse, Object, Exception)}\\n */\\npublic class FescarHandlerInterceptor implements HandlerInterceptor {\\n\\n\\tprivate static final Logger log = LoggerFactory\\n\\t\\t\\t.getLogger(FescarHandlerInterceptor.class);\\n\\n\\t@Override\\n\\tpublic boolean preHandle(HttpServletRequest request, HttpServletResponse response,\\n\\t\\t\\tObject handler) throws Exception {\\n\\n\\t\\tString xid = RootContext.getXID();\\n\\t\\tString rpcXid = request.getHeader(RootContext.KEY_XID);\\n\\t\\tif (log.isDebugEnabled()) {\\n\\t\\t\\tlog.debug(\\"xid in RootContext {} xid in RpcContext {}\\", xid, rpcXid);\\n\\t\\t}\\n\\n\\t\\tif (xid == null && rpcXid != null) {\\n\\t\\t\\tRootContext.bind(rpcXid);\\n\\t\\t\\tif (log.isDebugEnabled()) {\\n\\t\\t\\t\\tlog.debug(\\"bind {} to RootContext\\", rpcXid);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\treturn true;\\n\\t}\\n\\n\\t@Override\\n\\tpublic void afterCompletion(HttpServletRequest request, HttpServletResponse response,\\n\\t\\t\\tObject handler, Exception e) throws Exception {\\n\\n\\t\\tString rpcXid = request.getHeader(RootContext.KEY_XID);\\n\\n\\t\\tif (StringUtils.isEmpty(rpcXid)) {\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\n\\t\\tString unbindXid = RootContext.unbind();\\n\\t\\tif (log.isDebugEnabled()) {\\n\\t\\t\\tlog.debug(\\"unbind {} from RootContext\\", unbindXid);\\n\\t\\t}\\n\\t\\tif (!rpcXid.equalsIgnoreCase(unbindXid)) {\\n\\t\\t\\tlog.warn(\\"xid in change during RPC from {} to {}\\", rpcXid, unbindXid);\\n\\t\\t\\tif (unbindXid != null) {\\n\\t\\t\\t\\tRootContext.bind(unbindXid);\\n\\t\\t\\t\\tlog.warn(\\"bind {} back to RootContext\\", unbindXid);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n}\\n\\n```\\n\\n\\n\\nThe preHandle method is called before the request is executed. The xid parameter represents the unique identifier of the global transaction already bound to the current transaction context, while rpcXid represents the global transaction identifier that needs to be bound to the request and is passed through the HTTP header. In the preHandle method, it checks if there is no XID in the current transaction context and if rpcXid is not empty. If so, it binds rpcXid to the current transaction context.\\n\\nThe afterCompletion method is called after the request is completed and is used to perform resource cleanup actions. Fescar uses the RootContext.unbind() method to unbind the XID involved in the transaction context. The logic in the if statement is for code robustness. If rpcXid and unbindXid are not equal, it rebinds unbindXid.\\n\\nFor Spring Cloud, the default RPC method is HTTP. Therefore, for the provider, there is no need to differentiate the request interception method. It only needs to extract the XID from the header and bind it to its own transaction context. However, for the consumer, due to the variety of request components, including circuit breakers and isolation mechanisms, different situations need to be distinguished and handled. We will analyze this in more detail later.\\n\\n\\n#### Microservice Consumer\\n\\nFescar categorizes the request methods into RestTemplate, Feign, Feign+Hystrix, and Feign+Sentinel. Different components are automatically configured through Spring Boot\'s Auto Configuration. The specific configuration classes can be found in the spring.factories file, and we will also discuss the relevant configuration classes later in this document.\\n\\n#####  RestTemplate\\n\\nLet\'s take a look at how Fescar passes XID if the consumer is using RestTemplate for requests.\\n\\n```java\\npublic class FescarRestTemplateInterceptor implements ClientHttpRequestInterceptor {\\n\\t@Override\\n\\tpublic ClientHttpResponse intercept(HttpRequest httpRequest, byte[] bytes,\\n\\t\\t\\tClientHttpRequestExecution clientHttpRequestExecution) throws IOException {\\n\\t\\tHttpRequestWrapper requestWrapper = new HttpRequestWrapper(httpRequest);\\n\\n\\t\\tString xid = RootContext.getXID();\\n\\n\\t\\tif (!StringUtils.isEmpty(xid)) {\\n\\t\\t\\trequestWrapper.getHeaders().add(RootContext.KEY_XID, xid);\\n\\t\\t}\\n\\t\\treturn clientHttpRequestExecution.execute(requestWrapper, bytes);\\n\\t}\\n}\\n```\\n\\nThe FescarRestTemplateInterceptor implements the intercept method of the ClientHttpRequestInterceptor interface. It wraps the outgoing request and, if there is an existing Fescar transaction context XID, retrieves it and adds it to the HTTP headers of the request.\\n\\nFescarRestTemplateInterceptor is configured in RestTemplate through FescarRestTemplateAutoConfiguration.\\n\\n```java\\n@Configuration\\npublic class FescarRestTemplateAutoConfiguration {\\n\\n\\t@Bean\\n\\tpublic FescarRestTemplateInterceptor fescarRestTemplateInterceptor() {\\n\\t\\treturn new FescarRestTemplateInterceptor();\\n\\t}\\n\\n\\t@Autowired(required = false)\\n\\tprivate Collection<RestTemplate> restTemplates;\\n\\n\\t@Autowired\\n\\tprivate FescarRestTemplateInterceptor fescarRestTemplateInterceptor;\\n\\n\\t@PostConstruct\\n\\tpublic void init() {\\n\\t\\tif (this.restTemplates != null) {\\n\\t\\t\\tfor (RestTemplate restTemplate : restTemplates) {\\n\\t\\t\\t\\tList<ClientHttpRequestInterceptor> interceptors = new ArrayList<ClientHttpRequestInterceptor>(\\n\\t\\t\\t\\t\\t\\trestTemplate.getInterceptors());\\n\\t\\t\\t\\tinterceptors.add(this.fescarRestTemplateInterceptor);\\n\\t\\t\\t\\trestTemplate.setInterceptors(interceptors);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n}\\n```\\n\\nThe init method iterates through all the RestTemplate instances, retrieves the original interceptors from each RestTemplate, adds the fescarRestTemplateInterceptor, and then reorders the interceptors.\\n\\n##### Feign\\n\\n![Feign \u7c7b\u5173\u7cfb\u56fe](/img/blog/20190305184812.png)\\n\\nNext, let\'s take a look at the code related to Feign. There are quite a few classes in this package, so let\'s start with its AutoConfiguration.\\n\\n```java\\n@Configuration\\n@ConditionalOnClass(Client.class)\\n@AutoConfigureBefore(FeignAutoConfiguration.class)\\npublic class FescarFeignClientAutoConfiguration {\\n\\n\\t@Bean\\n\\t@Scope(\\"prototype\\")\\n\\t@ConditionalOnClass(name = \\"com.netflix.hystrix.HystrixCommand\\")\\n\\t@ConditionalOnProperty(name = \\"feign.hystrix.enabled\\", havingValue = \\"true\\")\\n\\tFeign.Builder feignHystrixBuilder(BeanFactory beanFactory) {\\n\\t\\treturn FescarHystrixFeignBuilder.builder(beanFactory);\\n\\t}\\n\\n\\t@Bean\\n\\t@Scope(\\"prototype\\")\\n\\t@ConditionalOnClass(name = \\"com.alibaba.csp.sentinel.SphU\\")\\n\\t@ConditionalOnProperty(name = \\"feign.sentinel.enabled\\", havingValue = \\"true\\")\\n\\tFeign.Builder feignSentinelBuilder(BeanFactory beanFactory) {\\n\\t\\treturn FescarSentinelFeignBuilder.builder(beanFactory);\\n\\t}\\n\\n\\t@Bean\\n\\t@ConditionalOnMissingBean\\n\\t@Scope(\\"prototype\\")\\n\\tFeign.Builder feignBuilder(BeanFactory beanFactory) {\\n\\t\\treturn FescarFeignBuilder.builder(beanFactory);\\n\\t}\\n\\n\\t@Configuration\\n\\tprotected static class FeignBeanPostProcessorConfiguration {\\n\\n\\t\\t@Bean\\n\\t\\tFescarBeanPostProcessor fescarBeanPostProcessor(\\n\\t\\t\\t\\tFescarFeignObjectWrapper fescarFeignObjectWrapper) {\\n\\t\\t\\treturn new FescarBeanPostProcessor(fescarFeignObjectWrapper);\\n\\t\\t}\\n\\n\\t\\t@Bean\\n\\t\\tFescarContextBeanPostProcessor fescarContextBeanPostProcessor(\\n\\t\\t\\t\\tBeanFactory beanFactory) {\\n\\t\\t\\treturn new FescarContextBeanPostProcessor(beanFactory);\\n\\t\\t}\\n\\n\\t\\t@Bean\\n\\t\\tFescarFeignObjectWrapper fescarFeignObjectWrapper(BeanFactory beanFactory) {\\n\\t\\t\\treturn new FescarFeignObjectWrapper(beanFactory);\\n\\t\\t}\\n\\t}\\n\\n}\\n```\\n\\nThe FescarFeignClientAutoConfiguration is enabled when the Client.class exists and requires it to be applied before FeignAutoConfiguration. Since FeignClientsConfiguration is responsible for generating the FeignContext and is enabled by FeignAutoConfiguration, based on the dependency relationship, FescarFeignClientAutoConfiguration is also applied before FeignClientsConfiguration.\\n\\nFescarFeignClientAutoConfiguration customizes the Feign.Builder and adapts it for feign.sentinel, feign.hystrix, and regular feign cases. The purpose is to customize the actual implementation of the Client in Feign to be FescarFeignClient.\\n\\n```java\\nHystrixFeign.builder().retryer(Retryer.NEVER_RETRY)\\n      .client(new FescarFeignClient(beanFactory))\\n```\\n\\n```java\\nSentinelFeign.builder().retryer(Retryer.NEVER_RETRY)\\n\\t\\t\\t\\t.client(new FescarFeignClient(beanFactory));\\n```\\n\\n```java\\nFeign.builder().client(new FescarFeignClient(beanFactory));\\n```\\n\\n\\nFescarFeignClient is an enhancement of the original Feign client proxy.\\n\\n```java\\npublic class FescarFeignClient implements Client {\\n\\n\\tprivate final Client delegate;\\n\\tprivate final BeanFactory beanFactory;\\n\\n\\tFescarFeignClient(BeanFactory beanFactory) {\\n\\t\\tthis.beanFactory = beanFactory;\\n\\t\\tthis.delegate = new Client.Default(null, null);\\n\\t}\\n\\n\\tFescarFeignClient(BeanFactory beanFactory, Client delegate) {\\n\\t\\tthis.delegate = delegate;\\n\\t\\tthis.beanFactory = beanFactory;\\n\\t}\\n\\n\\t@Override\\n\\tpublic Response execute(Request request, Request.Options options) throws IOException {\\n\\n\\t\\tRequest modifiedRequest = getModifyRequest(request);\\n\\n\\t\\ttry {\\n\\t\\t\\treturn this.delegate.execute(modifiedRequest, options);\\n\\t\\t}\\n\\t\\tfinally {\\n\\n\\t\\t}\\n\\t}\\n\\n\\tprivate Request getModifyRequest(Request request) {\\n\\n\\t\\tString xid = RootContext.getXID();\\n\\n\\t\\tif (StringUtils.isEmpty(xid)) {\\n\\t\\t\\treturn request;\\n\\t\\t}\\n\\n\\t\\tMap<String, Collection<String>> headers = new HashMap<>();\\n\\t\\theaders.putAll(request.headers());\\n\\n\\t\\tList<String> fescarXid = new ArrayList<>();\\n\\t\\tfescarXid.add(xid);\\n\\t\\theaders.put(RootContext.KEY_XID, fescarXid);\\n\\n\\t\\treturn Request.create(request.method(), request.url(), headers, request.body(),\\n\\t\\t\\t\\trequest.charset());\\n\\t}\\n\\n```\\n\\n\\nIn the above process, we can see that FescarFeignClient modifies the original Request. It first retrieves the XID from the current transaction context and, if the XID is not empty, adds it to the request\'s header.\\n\\nFeignBeanPostProcessorConfiguration defines three beans: FescarContextBeanPostProcessor, FescarBeanPostProcessor, and FescarFeignObjectWrapper. FescarContextBeanPostProcessor and FescarBeanPostProcessor both implement the Spring BeanPostProcessor interface.\\n\\nHere is the implementation of FescarContextBeanPostProcessor\\n\\n```java\\n    @Override\\n\\tpublic Object postProcessBeforeInitialization(Object bean, String beanName)\\n\\t\\t\\tthrows BeansException {\\n\\t\\tif (bean instanceof FeignContext && !(bean instanceof FescarFeignContext)) {\\n\\t\\t\\treturn new FescarFeignContext(getFescarFeignObjectWrapper(),\\n\\t\\t\\t\\t\\t(FeignContext) bean);\\n\\t\\t}\\n\\t\\treturn bean;\\n\\t}\\n\\n\\t@Override\\n\\tpublic Object postProcessAfterInitialization(Object bean, String beanName)\\n\\t\\t\\tthrows BeansException {\\n\\t\\treturn bean;\\n\\t}\\n```\\n\\nThe two methods in BeanPostProcessor allow for pre- and post-processing of beans in the Spring container. The postProcessBeforeInitialization method is called before initialization, while the postProcessAfterInitialization method is called after initialization. The return value of these methods can be the original bean instance or a wrapped instance using a wrapper.\\n\\nFescarContextBeanPostProcessor wraps FeignContext into FescarFeignContext. FescarBeanPostProcessor wraps FeignClient into FescarLoadBalancerFeignClient and FescarFeignClient, depending on whether it inherits from LoadBalancerFeignClient.\\n\\nIn FeignAutoConfiguration, the FeignContext does not have any ConditionalOnXXX conditions. Therefore, Fescar uses a pre-processing approach to wrap FeignContext into FescarFeignContext.\\n\\n```java\\n    @Bean\\n\\tpublic FeignContext feignContext() {\\n\\t\\tFeignContext context = new FeignContext();\\n\\t\\tcontext.setConfigurations(this.configurations);\\n\\t\\treturn context;\\n\\t}\\n```\\n\\nFor Feign Clients, the FeignClientFactoryBean retrieves an instance of FeignContext. For custom Feign Client objects configured by developers using the @Configuration annotation, they are configured into the builder, which causes the enhanced FescarFeignClient in FescarFeignBuilder to become ineffective. The key code in FeignClientFactoryBean is as follows\\n\\n```java\\n\\t/**\\n\\t * @param <T> the target type of the Feign client\\n\\t * @return a {@link Feign} client created with the specified data and the context information\\n\\t */\\n\\t<T> T getTarget() {\\n\\t\\tFeignContext context = applicationContext.getBean(FeignContext.class);\\n\\t\\tFeign.Builder builder = feign(context);\\n\\n\\t\\tif (!StringUtils.hasText(this.url)) {\\n\\t\\t\\tif (!this.name.startsWith(\\"http\\")) {\\n\\t\\t\\t\\turl = \\"http://\\" + this.name;\\n\\t\\t\\t}\\n\\t\\t\\telse {\\n\\t\\t\\t\\turl = this.name;\\n\\t\\t\\t}\\n\\t\\t\\turl += cleanPath();\\n\\t\\t\\treturn (T) loadBalance(builder, context, new HardCodedTarget<>(this.type,\\n\\t\\t\\t\\t\\tthis.name, url));\\n\\t\\t}\\n\\t\\tif (StringUtils.hasText(this.url) && !this.url.startsWith(\\"http\\")) {\\n\\t\\t\\tthis.url = \\"http://\\" + this.url;\\n\\t\\t}\\n\\t\\tString url = this.url + cleanPath();\\n\\t\\tClient client = getOptional(context, Client.class);\\n\\t\\tif (client != null) {\\n\\t\\t\\tif (client instanceof LoadBalancerFeignClient) {\\n\\t\\t\\t\\t// not load balancing because we have a url,\\n\\t\\t\\t\\t// but ribbon is on the classpath, so unwrap\\n\\t\\t\\t\\tclient = ((LoadBalancerFeignClient)client).getDelegate();\\n\\t\\t\\t}\\n\\t\\t\\tbuilder.client(client);\\n\\t\\t}\\n\\t\\tTargeter targeter = get(context, Targeter.class);\\n\\t\\treturn (T) targeter.target(this, builder, context, new HardCodedTarget<>(\\n\\t\\t\\t\\tthis.type, this.name, url));\\n\\t}\\n```\\nThe above code determines whether to make a direct call to the specified URL or use load balancing based on whether the URL parameter is specified in the annotation. The targeter.target method creates the object through dynamic proxy. The general process is as follows: the parsed Feign methods are stored in a map, and then passed as a parameter to generate the InvocationHandler, which in turn generates the dynamic proxy object.\\n\\nThe presence of FescarContextBeanPostProcessor ensures that even if developers customize operations on FeignClient, the enhancement of global transactions required by Fescar can still be achieved.\\n\\nAs for FescarFeignObjectWrapper, let\'s focus on the Wrapper method:\\n\\n```java\\n\\tObject wrap(Object bean) {\\n\\t\\tif (bean instanceof Client && !(bean instanceof FescarFeignClient)) {\\n\\t\\t\\tif (bean instanceof LoadBalancerFeignClient) {\\n\\t\\t\\t\\tLoadBalancerFeignClient client = ((LoadBalancerFeignClient) bean);\\n\\t\\t\\t\\treturn new FescarLoadBalancerFeignClient(client.getDelegate(), factory(),\\n\\t\\t\\t\\t\\t\\tclientFactory(), this.beanFactory);\\n\\t\\t\\t}\\n\\t\\t\\treturn new FescarFeignClient(this.beanFactory, (Client) bean);\\n\\t\\t}\\n\\t\\treturn bean;\\n\\t}\\n```\\n\\nIn the wrap method, if the bean is an instance of LoadBalancerFeignClient, it first retrieves the actual Client object that the LoadBalancerFeignClient proxies using the client.getDelegate() method. It then wraps the Client object into FescarFeignClient and generates a subclass of LoadBalancerFeignClient called FescarLoadBalancerFeignClient. If the bean is an instance of Client and not FescarFeignClient or LoadBalancerFeignClient, it is directly wrapped and transformed into FescarFeignClient.\\n\\nThe above process design is quite clever. It controls the order of configuration based on Spring Boot\'s Auto Configuration and customizes the Feign Builder bean to ensure that all Clients are enhanced with FescarFeignClient. It also wraps the beans in the Spring container using BeanPostProcessor, ensuring that all beans in the container are enhanced with FescarFeignClient, thus avoiding the replacement action in the getTarget method of FeignClientFactoryBean.\\n\\n##### Hystrix Isolation\\n\\nNow let\'s take a look at the Hystrix part. Why do we separate Hystrix and implement a separate strategy class in Fescar? Currently, the default implementation of the transaction context RootContext is based on ThreadLocal, which means the context is bound to the thread. Hystrix itself has two isolation modes: semaphore-based isolation and thread pool-based isolation. Hystrix officially recommends using thread pool isolation for better separation, which is the commonly used mode:\\n\\n```\\nThread or Semaphore\\nThe default, and the recommended setting, is to run HystrixCommands using thread isolation (THREAD) and HystrixObservableCommands using semaphore isolation (SEMAPHORE).\\n\\nCommands executed in threads have an extra layer of protection against latencies beyond what network timeouts can offer.\\n\\nGenerally the only time you should use semaphore isolation for HystrixCommands is when the call is so high volume (hundreds per second, per instance) that the overhead of separate threads is too high; this typically only applies to non-network calls.\\n```\\n\\nYou are correct that the service layer\'s business code and the thread that sends the request are not the same. Therefore, the ThreadLocal approach cannot pass the XID to the Hystrix thread and subsequently to the callee. To address this issue, Hystrix provides a mechanism for developers to customize the concurrency strategy. This can be done by extending the HystrixConcurrencyStrategy class and overriding the wrapCallable method:\\n\\n```java\\npublic class FescarHystrixConcurrencyStrategy extends HystrixConcurrencyStrategy {\\n\\n\\tprivate HystrixConcurrencyStrategy delegate;\\n\\n\\tpublic FescarHystrixConcurrencyStrategy() {\\n\\t\\tthis.delegate = HystrixPlugins.getInstance().getConcurrencyStrategy();\\n\\t\\tHystrixPlugins.reset();\\n\\t\\tHystrixPlugins.getInstance().registerConcurrencyStrategy(this);\\n\\t}\\n\\n\\t@Override\\n\\tpublic <K> Callable<K> wrapCallable(Callable<K> c) {\\n\\t\\tif (c instanceof FescarContextCallable) {\\n\\t\\t\\treturn c;\\n\\t\\t}\\n\\n\\t\\tCallable<K> wrappedCallable;\\n\\t\\tif (this.delegate != null) {\\n\\t\\t\\twrappedCallable = this.delegate.wrapCallable(c);\\n\\t\\t}\\n\\t\\telse {\\n\\t\\t\\twrappedCallable = c;\\n\\t\\t}\\n\\t\\tif (wrappedCallable instanceof FescarContextCallable) {\\n\\t\\t\\treturn wrappedCallable;\\n\\t\\t}\\n\\n\\t\\treturn new FescarContextCallable<>(wrappedCallable);\\n\\t}\\n\\n\\tprivate static class FescarContextCallable<K> implements Callable<K> {\\n\\n\\t\\tprivate final Callable<K> actual;\\n\\t\\tprivate final String xid;\\n\\n\\t\\tFescarContextCallable(Callable<K> actual) {\\n\\t\\t\\tthis.actual = actual;\\n\\t\\t\\tthis.xid = RootContext.getXID();\\n\\t\\t}\\n\\n\\t\\t@Override\\n\\t\\tpublic K call() throws Exception {\\n\\t\\t\\ttry {\\n\\t\\t\\t\\tRootContext.bind(xid);\\n\\t\\t\\t\\treturn actual.call();\\n\\t\\t\\t}\\n\\t\\t\\tfinally {\\n\\t\\t\\t\\tRootContext.unbind();\\n\\t\\t\\t}\\n\\t\\t}\\n\\n\\t}\\n}\\n```\\n\\nFescar also provides a FescarHystrixAutoConfiguration, which generates the FescarHystrixConcurrencyStrategy when HystrixCommand is present.\\n\\n```java\\n@Configuration\\n@ConditionalOnClass(HystrixCommand.class)\\npublic class FescarHystrixAutoConfiguration {\\n\\n\\t@Bean\\n\\tFescarHystrixConcurrencyStrategy fescarHystrixConcurrencyStrategy() {\\n\\t\\treturn new FescarHystrixConcurrencyStrategy();\\n\\t}\\n\\n}\\n```\\n\\n### reference\\n\\n- Fescar: https://github.com/alibaba/fescar\\n\\n- Spring Cloud Alibaba: https://github.com/spring-cloud-incubator/spring-cloud-alibaba\\n\\n- spring-cloud-openfeign: https://github.com/spring-cloud/spring-cloud-openfeign\\n\\n ### author\\n\\n  kangshu.guo\uff0cCommunity nickname ywind, formerly employed at Huawei Terminal Cloud, currently a Java engineer at Sohu Intelligent Media Center. Mainly responsible for development related to Sohu accounts. Has a strong interest in distributed transactions, distributed systems, and microservices architecture.\\n  min.ji(qinming)\uff0cCommunity nickname slievrly, Fescar project leader, core developer of Alibaba middleware TXC/GTS. Engaged in core research and development work in distributed middleware for a long time. Has extensive technical expertise in the field of distributed transactions."},{"id":"/integrate-seata-with-spring-cloud","metadata":{"permalink":"/blog/integrate-seata-with-spring-cloud","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/integrate-seata-with-spring-cloud.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/integrate-seata-with-spring-cloud.md","title":"Integrating Seata (formerly Fescar) Distributed Transaction with Spring Cloud","description":"","date":"2019-04-15T00:00:00.000Z","formattedDate":"April 15, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"dafei.Fei"}],"frontMatter":{"title":"Integrating Seata (formerly Fescar) Distributed Transaction with Spring Cloud","author":"dafei.Fei","date":"2019/04/15","keywords":["fescar","seata","distributed transaction"]},"prevItem":{"title":"Fescar Integration with Spring Cloud In-Depth Analysis of Source Code","permalink":"/blog/how-to-support-spring-cloud"},"nextItem":{"title":"In-Depth Explanation of Seata-Client Principles and Processes in Distributed Transactions","permalink":"/blog/seata-analysis-java-client"}},"content":""},{"id":"/seata-analysis-java-client","metadata":{"permalink":"/blog/seata-analysis-java-client","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-java-client.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-java-client.md","title":"In-Depth Explanation of Seata-Client Principles and Processes in Distributed Transactions","description":"","date":"2019-04-15T00:00:00.000Z","formattedDate":"April 15, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"fangliangsheng"}],"frontMatter":{"title":"In-Depth Explanation of Seata-Client Principles and Processes in Distributed Transactions","author":"fangliangsheng","date":"2019/04/15","keywords":["fescar","seata","distributed transaction"]},"prevItem":{"title":"Integrating Seata (formerly Fescar) Distributed Transaction with Spring Cloud","permalink":"/blog/integrate-seata-with-spring-cloud"},"nextItem":{"title":"In-Depth Analysis of One-Stop Distributed Transaction Solution Seata-Server","permalink":"/blog/seata-analysis-java-server"}},"content":""},{"id":"/seata-analysis-java-server","metadata":{"permalink":"/blog/seata-analysis-java-server","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-java-server.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-java-server.md","title":"In-Depth Analysis of One-Stop Distributed Transaction Solution Seata-Server","description":"","date":"2019-04-08T00:00:00.000Z","formattedDate":"April 8, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"zhao.li,min.ji"}],"frontMatter":{"title":"In-Depth Analysis of One-Stop Distributed Transaction Solution Seata-Server","author":"zhao.li,min.ji","date":"2019/04/08","keywords":["fescar","seata","distributed transaction"]},"prevItem":{"title":"In-Depth Explanation of Seata-Client Principles and Processes in Distributed Transactions","permalink":"/blog/seata-analysis-java-client"},"nextItem":{"title":"Analysis of Applicable Models and Scenarios for TCC","permalink":"/blog/tcc-mode-applicable-scenario-analysis"}},"content":""},{"id":"/tcc-mode-applicable-scenario-analysis","metadata":{"permalink":"/blog/tcc-mode-applicable-scenario-analysis","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/tcc-mode-applicable-scenario-analysis.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/tcc-mode-applicable-scenario-analysis.md","title":"Analysis of Applicable Models and Scenarios for TCC","description":"","date":"2019-03-27T00:00:00.000Z","formattedDate":"March 27, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"zhangthen"}],"frontMatter":{"title":"Analysis of Applicable Models and Scenarios for TCC","author":"zhangthen","date":"2019/03/27","keywords":["seata","distributed transaction","TCC","roadmap"]},"prevItem":{"title":"In-Depth Analysis of One-Stop Distributed Transaction Solution Seata-Server","permalink":"/blog/seata-analysis-java-server"},"nextItem":{"title":"Introduction to TCC Theory and Design Implementation Guide","permalink":"/blog/tcc-mode-design-principle"}},"content":""},{"id":"/tcc-mode-design-principle","metadata":{"permalink":"/blog/tcc-mode-design-principle","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/tcc-mode-design-principle.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/tcc-mode-design-principle.md","title":"Introduction to TCC Theory and Design Implementation Guide","description":"","date":"2019-03-26T00:00:00.000Z","formattedDate":"March 26, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"zhangthen"}],"frontMatter":{"title":"Introduction to TCC Theory and Design Implementation Guide","author":"zhangthen","date":"2019/03/26","keywords":["fescar","distributed transaction","TCC","roadmap"]},"prevItem":{"title":"Analysis of Applicable Models and Scenarios for TCC","permalink":"/blog/tcc-mode-applicable-scenario-analysis"},"nextItem":{"title":"How to use Seata to ensure consistency between Dubbo Microservices","permalink":"/blog/quick-start-use-seata-and-dubbo-services"}},"content":""},{"id":"/quick-start-use-seata-and-dubbo-services","metadata":{"permalink":"/blog/quick-start-use-seata-and-dubbo-services","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/quick-start-use-seata-and-dubbo-services.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/quick-start-use-seata-and-dubbo-services.md","title":"How to use Seata to ensure consistency between Dubbo Microservices","description":"This article will introduce you how to use Seata to ensure consistency between Dubbo Microservices.","date":"2019-03-07T00:00:00.000Z","formattedDate":"March 7, 2019","tags":[],"readingTime":2.84,"hasTruncateMarker":false,"authors":[{"name":"slievrly"}],"frontMatter":{"title":"How to use Seata to ensure consistency between Dubbo Microservices","keywords":["Dubbo","Seata","Consistency"],"description":"This article will introduce you how to use Seata to ensure consistency between Dubbo Microservices.","author":"slievrly","date":"2019-03-07T00:00:00.000Z"},"prevItem":{"title":"Introduction to TCC Theory and Design Implementation Guide","permalink":"/blog/tcc-mode-design-principle"},"nextItem":{"title":"Unveiling the Principles of Fescar Distributed Transaction","permalink":"/blog/seata-analysis-simple"}},"content":"## Use case\\n\\nA business logic for user purchasing commodities. The whole business logic is powered by 3 microservices:\\n\\n- Storage service: deduct storage count on given commodity.\\n- Order service: create order according to purchase request.\\n- Account service: debit the balance of user\'s account.\\n\\n### Architecture\\n\\n![Architecture](/img/blog/seata/seata-1.png) \\n\\n\\n### StorageService\\n\\n```java\\npublic interface StorageService {\\n\\n    /**\\n     * deduct storage count\\n     */\\n    void deduct(String commodityCode, int count);\\n}\\n```\\n\\n### OrderService\\n\\n```java\\npublic interface OrderService {\\n\\n    /**\\n     * create order\\n     */\\n    Order create(String userId, String commodityCode, int orderCount);\\n}\\n```\\n\\n### AccountService\\n\\n```java\\npublic interface AccountService {\\n\\n    /**\\n     * debit balance of user\'s account\\n     */\\n    void debit(String userId, int money);\\n}\\n```\\n\\n### Main business logic\\n\\n```java\\npublic class BusinessServiceImpl implements BusinessService {\\n\\n    private StorageService storageService;\\n\\n    private OrderService orderService;\\n\\n    /**\\n     * purchase\\n     */\\n    public void purchase(String userId, String commodityCode, int orderCount) {\\n\\n        storageService.deduct(commodityCode, orderCount);\\n\\n        orderService.create(userId, commodityCode, orderCount);\\n    }\\n}\\n```\\n\\n```java\\npublic class StorageServiceImpl implements StorageService {\\n\\n  private StorageDAO storageDAO;\\n  \\n    @Override\\n    public void deduct(String commodityCode, int count) {\\n        Storage storage = new Storage();\\n        storage.setCount(count);\\n        storage.setCommodityCode(commodityCode);\\n        storageDAO.update(storage);\\n    }\\n}\\n```\\n\\n```java\\npublic class OrderServiceImpl implements OrderService {\\n\\n    private OrderDAO orderDAO;\\n\\n    private AccountService accountService;\\n\\n    public Order create(String userId, String commodityCode, int orderCount) {\\n\\n        int orderMoney = calculate(commodityCode, orderCount);\\n\\n        accountService.debit(userId, orderMoney);\\n\\n        Order order = new Order();\\n        order.userId = userId;\\n        order.commodityCode = commodityCode;\\n        order.count = orderCount;\\n        order.money = orderMoney;\\n\\n        return orderDAO.insert(order);\\n    }\\n}\\n```\\n\\n## Distributed Transaction Solution with Seata\\n\\n![undefined](/img/blog/seata/seata-2.png) \\n\\nWe just need an annotation `@GlobalTransactional` on business method: \\n\\n```java\\n\\n    @GlobalTransactional\\n    public void purchase(String userId, String commodityCode, int orderCount) {\\n        ......\\n    }\\n```\\n\\n## Example powered by Dubbo + Seata\\n\\n### Step 1: Setup database\\n\\n- Requirement: MySQL with InnoDB engine.\\n\\n**Note:** In fact, there should be 3 database for the 3 services in the example use case. However, we can just create one database and configure 3 data sources for simple. \\n\\nModify Spring XML with the database URL/username/password you just created.\\n\\ndubbo-account-service.xml\\ndubbo-order-service.xml\\ndubbo-storage-service.xml\\n\\n```xml\\n    <property name=\\"url\\" value=\\"jdbc:mysql://x.x.x.x:3306/xxx\\" />\\n    <property name=\\"username\\" value=\\"xxx\\" />\\n    <property name=\\"password\\" value=\\"xxx\\" />\\n```\\n### Step 2: Create UNDO_LOG table for Seata\\n\\n`UNDO_LOG` table is required by Seata AT mode.\\n\\n```sql\\nCREATE TABLE `undo_log` (\\n  `id` bigint(20) NOT NULL AUTO_INCREMENT,\\n  `branch_id` bigint(20) NOT NULL,\\n  `xid` varchar(100) NOT NULL,\\n  `rollback_info` longblob NOT NULL,\\n  `log_status` int(11) NOT NULL,\\n  `log_created` datetime NOT NULL,\\n  `log_modified` datetime NOT NULL,\\n  `ext` varchar(100) DEFAULT NULL,\\n  PRIMARY KEY (`id`),\\n  KEY `idx_unionkey` (`xid`,`branch_id`)\\n) ENGINE=InnoDB AUTO_INCREMENT=159 DEFAULT CHARSET=utf8\\n```\\n\\n### Step 3: Create tables for example business\\n\\n```sql\\n\\nDROP TABLE IF EXISTS `storage_tbl`;\\nCREATE TABLE `storage_tbl` (\\n  `id` int(11) NOT NULL AUTO_INCREMENT,\\n  `commodity_code` varchar(255) DEFAULT NULL,\\n  `count` int(11) DEFAULT 0,\\n  PRIMARY KEY (`id`),\\n  UNIQUE KEY (`commodity_code`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\\n\\n\\nDROP TABLE IF EXISTS `order_tbl`;\\nCREATE TABLE `order_tbl` (\\n  `id` int(11) NOT NULL AUTO_INCREMENT,\\n  `user_id` varchar(255) DEFAULT NULL,\\n  `commodity_code` varchar(255) DEFAULT NULL,\\n  `count` int(11) DEFAULT 0,\\n  `money` int(11) DEFAULT 0,\\n  PRIMARY KEY (`id`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\\n\\n\\nDROP TABLE IF EXISTS `account_tbl`;\\nCREATE TABLE `account_tbl` (\\n  `id` int(11) NOT NULL AUTO_INCREMENT,\\n  `user_id` varchar(255) DEFAULT NULL,\\n  `money` int(11) DEFAULT 0,\\n  PRIMARY KEY (`id`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\\n```\\n### Step 4: Start Seata-Server\\n\\n- Download server [package](https://github.com/apache/incubator-seata/releases), unzip it.\\n- Start Seata-Server\\n\\n```shell\\nsh seata-server.sh $LISTEN_PORT $PATH_FOR_PERSISTENT_DATA\\n\\ne.g.\\n\\nsh seata-server.sh 8091 /home/admin/seata/data/\\n```\\n\\n### Step 5: Run example\\n\\n- Start AccountService ([DubboAccountServiceStarter](https://github.com/apache/incubator-seata-samples/blob/master/dubbo/src/main/java/com/seata/seata/samples/dubbo/starter/DubboAccountServiceStarter.java)).\\n- Start StorageService ([DubboStorageServiceStarter](https://github.com/apache/incubator-seata-samples/blob/master/dubbo/src/main/java/com/seata/seata/samples/dubbo/starter/DubboStorageServiceStarter.java)).\\n- Start OrderService ([DubboOrderServiceStarter](https://github.com/apache/incubator-seata-samples/blob/master/dubbo/src/main/java/com/seata/seata/samples/dubbo/starter/DubboOrderServiceStarter.java)).\\n- Run BusinessService for test ([DubboBusinessTester](https://github.com/apache/incubator-seata-samples/blob/master/dubbo/src/main/java/com/seata/seata/samples/dubbo/starter/DubboBusinessTester.java)).\\n\\n### Related projects\\n* seata:          https://github.com/apache/incubator-seata/\\n* seata-samples : https://github.com/apache/incubator-seata-samples"},{"id":"/seata-analysis-simple","metadata":{"permalink":"/blog/seata-analysis-simple","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-simple.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-simple.md","title":"Unveiling the Principles of Fescar Distributed Transaction","description":"","date":"2019-02-18T00:00:00.000Z","formattedDate":"February 18, 2019","tags":[],"readingTime":0,"hasTruncateMarker":false,"authors":[{"name":"kailin.chen"}],"frontMatter":{"title":"Unveiling the Principles of Fescar Distributed Transaction","author":"kailin.chen","keywords":["Fescar","distributed transaction"],"date":"2019/02/18"},"prevItem":{"title":"How to use Seata to ensure consistency between Dubbo Microservices","permalink":"/blog/quick-start-use-seata-and-dubbo-services"},"nextItem":{"title":"MT mode","permalink":"/blog/manual-transaction-mode"}},"content":""},{"id":"/manual-transaction-mode","metadata":{"permalink":"/blog/manual-transaction-mode","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/manual-transaction-mode.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/manual-transaction-mode.md","title":"MT mode","description":"introduce MT mode","date":"2019-02-13T00:00:00.000Z","formattedDate":"February 13, 2019","tags":[],"readingTime":1.12,"hasTruncateMarker":false,"authors":[{"name":"kmmshmily"}],"frontMatter":{"title":"MT mode","keywords":["MT mode"],"description":"introduce MT mode","author":"kmmshmily","date":"2019-02-13T00:00:00.000Z"},"prevItem":{"title":"Unveiling the Principles of Fescar Distributed Transaction","permalink":"/blog/seata-analysis-simple"}},"content":"Review the description in the overview: a distributed global transaction, the whole is a model of **the two-phase commit**. A global transaction consists of several branch transactions that meet the model requirements of **the two-phase commit**, which requires each branch transaction to have its own:\\n\\n- One phase prepare behavior\\n- Two phase commit or rollback behavior\\n\\n![Overview of a global transaction](https://upload-images.jianshu.io/upload_images/4420767-e48f0284a037d1df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\\n\\nAccording to the two phase behavior pattern\uff0cWe divide the branch transaction into **Automatic (Branch) Transaction Mode** and **Manual (Branch) Transaction Mode**.\\n\\nThe AT mode is based on the **Relational Database** that **supports local ACID transactions**\uff1a\\n\\n- One phase prepare behavior: In the local transaction, the business data update and the corresponding rollback log record are submitted together.\\n- Two phase commit behavior: Immediately ended successfully, **Auto** asynchronous batch cleanup of the rollback log.\\n- Two phase rollback behavior: By rolling back the log, **automatic** generates a compensation operation to complete the data rollback.\\n\\nAccordingly, the MT mode does not rely on transaction support for the underlying data resources:\\n\\n- One phase prepare behavior: Call the prepare logic of **custom** .\\n- Two phase commit behavior:Call the commit logic of **custom** .\\n- Two phase rollback behavior:Call the rollback logic of **custom** .\\n\\nThe so-called MT mode refers to the support of the branch transaction of **custom** into the management of global transactions."}]}')}}]);