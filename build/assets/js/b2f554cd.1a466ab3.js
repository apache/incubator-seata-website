"use strict";(self.webpackChunkseata_website=self.webpackChunkseata_website||[]).push([[11477],{30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/integrate-seata-with-spring-cloud","metadata":{"permalink":"/blog/integrate-seata-with-spring-cloud","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/integrate-seata-with-spring-cloud.md","source":"@site/blog/integrate-seata-with-spring-cloud.md","title":"integrate-seata-with-spring-cloud","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"nextItem":{"title":"iscas2023","permalink":"/blog/iscas2023"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/iscas2023","metadata":{"permalink":"/blog/iscas2023","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/iscas2023.md","source":"@site/blog/iscas2023.md","title":"iscas2023","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"integrate-seata-with-spring-cloud","permalink":"/blog/integrate-seata-with-spring-cloud"},"nextItem":{"title":"seata-1.5.2","permalink":"/blog/seata-1.5.2"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-1.5.2","metadata":{"permalink":"/blog/seata-1.5.2","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-1.5.2.md","source":"@site/blog/seata-1.5.2.md","title":"seata-1.5.2","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"iscas2023","permalink":"/blog/iscas2023"},"nextItem":{"title":"seata-1.6.0","permalink":"/blog/seata-1.6.0"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-1.6.0","metadata":{"permalink":"/blog/seata-1.6.0","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-1.6.0.md","source":"@site/blog/seata-1.6.0.md","title":"seata-1.6.0","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-1.5.2","permalink":"/blog/seata-1.5.2"},"nextItem":{"title":"seata-analysis-config-modular","permalink":"/blog/seata-analysis-config-modular"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-analysis-config-modular","metadata":{"permalink":"/blog/seata-analysis-config-modular","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-analysis-config-modular.md","source":"@site/blog/seata-analysis-config-modular.md","title":"seata-analysis-config-modular","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-1.6.0","permalink":"/blog/seata-1.6.0"},"nextItem":{"title":"seata-analysis-core-modular","permalink":"/blog/seata-analysis-core-modular"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-analysis-core-modular","metadata":{"permalink":"/blog/seata-analysis-core-modular","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-analysis-core-modular.md","source":"@site/blog/seata-analysis-core-modular.md","title":"seata-analysis-core-modular","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-analysis-config-modular","permalink":"/blog/seata-analysis-config-modular"},"nextItem":{"title":"seata-analysis-dubbo-transmit-xid","permalink":"/blog/seata-analysis-dubbo-transmit-xid"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-analysis-dubbo-transmit-xid","metadata":{"permalink":"/blog/seata-analysis-dubbo-transmit-xid","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-analysis-dubbo-transmit-xid.md","source":"@site/blog/seata-analysis-dubbo-transmit-xid.md","title":"seata-analysis-dubbo-transmit-xid","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-analysis-core-modular","permalink":"/blog/seata-analysis-core-modular"},"nextItem":{"title":"seata-analysis-go-server","permalink":"/blog/seata-analysis-go-server"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-analysis-go-server","metadata":{"permalink":"/blog/seata-analysis-go-server","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-analysis-go-server.md","source":"@site/blog/seata-analysis-go-server.md","title":"seata-analysis-go-server","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-analysis-dubbo-transmit-xid","permalink":"/blog/seata-analysis-dubbo-transmit-xid"},"nextItem":{"title":"seata-analysis-java-client","permalink":"/blog/seata-analysis-java-client"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-analysis-java-client","metadata":{"permalink":"/blog/seata-analysis-java-client","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-analysis-java-client.md","source":"@site/blog/seata-analysis-java-client.md","title":"seata-analysis-java-client","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-analysis-go-server","permalink":"/blog/seata-analysis-go-server"},"nextItem":{"title":"seata-analysis-java-server","permalink":"/blog/seata-analysis-java-server"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-analysis-java-server","metadata":{"permalink":"/blog/seata-analysis-java-server","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-analysis-java-server.md","source":"@site/blog/seata-analysis-java-server.md","title":"seata-analysis-java-server","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-analysis-java-client","permalink":"/blog/seata-analysis-java-client"},"nextItem":{"title":"seata-analysis-simple","permalink":"/blog/seata-analysis-simple"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-analysis-simple","metadata":{"permalink":"/blog/seata-analysis-simple","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-analysis-simple.md","source":"@site/blog/seata-analysis-simple.md","title":"seata-analysis-simple","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-analysis-java-server","permalink":"/blog/seata-analysis-java-server"},"nextItem":{"title":"seata-analysis-tcc-modular","permalink":"/blog/seata-analysis-tcc-modular"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-analysis-tcc-modular","metadata":{"permalink":"/blog/seata-analysis-tcc-modular","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-analysis-tcc-modular.md","source":"@site/blog/seata-analysis-tcc-modular.md","title":"seata-analysis-tcc-modular","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-analysis-simple","permalink":"/blog/seata-analysis-simple"},"nextItem":{"title":"seata-at-demo-in-mac","permalink":"/blog/seata-at-demo-in-mac"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-at-demo-in-mac","metadata":{"permalink":"/blog/seata-at-demo-in-mac","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-at-demo-in-mac.md","source":"@site/blog/seata-at-demo-in-mac.md","title":"seata-at-demo-in-mac","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-analysis-tcc-modular","permalink":"/blog/seata-analysis-tcc-modular"},"nextItem":{"title":"seata-at-lock","permalink":"/blog/seata-at-lock"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-at-lock","metadata":{"permalink":"/blog/seata-at-lock","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-at-lock.md","source":"@site/blog/seata-at-lock.md","title":"seata-at-lock","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-at-demo-in-mac","permalink":"/blog/seata-at-demo-in-mac"},"nextItem":{"title":"seata-at-mode-design","permalink":"/blog/seata-at-mode-design"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-at-mode-design","metadata":{"permalink":"/blog/seata-at-mode-design","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-at-mode-design.md","source":"@site/blog/seata-at-mode-design.md","title":"seata-at-mode-design","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-at-lock","permalink":"/blog/seata-at-lock"},"nextItem":{"title":"seata-at-mode-start-rm-tm","permalink":"/blog/seata-at-mode-start-rm-tm"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-at-mode-start-rm-tm","metadata":{"permalink":"/blog/seata-at-mode-start-rm-tm","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-at-mode-start-rm-tm.md","source":"@site/blog/seata-at-mode-start-rm-tm.md","title":"seata-at-mode-start-rm-tm","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-at-mode-design","permalink":"/blog/seata-at-mode-design"},"nextItem":{"title":"seata-at-mode-start","permalink":"/blog/seata-at-mode-start"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-at-mode-start","metadata":{"permalink":"/blog/seata-at-mode-start","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-at-mode-start.md","source":"@site/blog/seata-at-mode-start.md","title":"seata-at-mode-start","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-at-mode-start-rm-tm","permalink":"/blog/seata-at-mode-start-rm-tm"},"nextItem":{"title":"seata-at-tcc-saga","permalink":"/blog/seata-at-tcc-saga"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-at-tcc-saga","metadata":{"permalink":"/blog/seata-at-tcc-saga","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-at-tcc-saga.md","source":"@site/blog/seata-at-tcc-saga.md","title":"seata-at-tcc-saga","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-at-mode-start","permalink":"/blog/seata-at-mode-start"},"nextItem":{"title":"seata-client-start-analysis-01","permalink":"/blog/seata-client-start-analysis-01"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-client-start-analysis-01","metadata":{"permalink":"/blog/seata-client-start-analysis-01","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-client-start-analysis-01.md","source":"@site/blog/seata-client-start-analysis-01.md","title":"seata-client-start-analysis-01","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-at-tcc-saga","permalink":"/blog/seata-at-tcc-saga"},"nextItem":{"title":"seata-client-start-analysis-02","permalink":"/blog/seata-client-start-analysis-02"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-client-start-analysis-02","metadata":{"permalink":"/blog/seata-client-start-analysis-02","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-client-start-analysis-02.md","source":"@site/blog/seata-client-start-analysis-02.md","title":"seata-client-start-analysis-02","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-client-start-analysis-01","permalink":"/blog/seata-client-start-analysis-01"},"nextItem":{"title":"seata-community-meetup-hangzhou-ready","permalink":"/blog/seata-community-meetup-hangzhou-ready"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-community-meetup-hangzhou-ready","metadata":{"permalink":"/blog/seata-community-meetup-hangzhou-ready","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-community-meetup-hangzhou-ready.md","source":"@site/blog/seata-community-meetup-hangzhou-ready.md","title":"seata-community-meetup-hangzhou-ready","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-client-start-analysis-02","permalink":"/blog/seata-client-start-analysis-02"},"nextItem":{"title":"seata-config-center","permalink":"/blog/seata-config-center"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-config-center","metadata":{"permalink":"/blog/seata-config-center","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-config-center.md","source":"@site/blog/seata-config-center.md","title":"seata-config-center","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-community-meetup-hangzhou-ready","permalink":"/blog/seata-community-meetup-hangzhou-ready"},"nextItem":{"title":"seata-config-manager","permalink":"/blog/seata-config-manager"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-config-manager","metadata":{"permalink":"/blog/seata-config-manager","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-config-manager.md","source":"@site/blog/seata-config-manager.md","title":"seata-config-manager","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-config-center","permalink":"/blog/seata-config-center"},"nextItem":{"title":"seata-datasource-proxy","permalink":"/blog/seata-datasource-proxy"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-datasource-proxy","metadata":{"permalink":"/blog/seata-datasource-proxy","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-datasource-proxy.md","source":"@site/blog/seata-datasource-proxy.md","title":"seata-datasource-proxy","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-config-manager","permalink":"/blog/seata-config-manager"},"nextItem":{"title":"seata-dsproxy-deadlock","permalink":"/blog/seata-dsproxy-deadlock"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-dsproxy-deadlock","metadata":{"permalink":"/blog/seata-dsproxy-deadlock","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-dsproxy-deadlock.md","source":"@site/blog/seata-dsproxy-deadlock.md","title":"seata-dsproxy-deadlock","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-datasource-proxy","permalink":"/blog/seata-datasource-proxy"},"nextItem":{"title":"seata-dynamic-config-and-dynamic-disable","permalink":"/blog/seata-dynamic-config-and-dynamic-disable"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-dynamic-config-and-dynamic-disable","metadata":{"permalink":"/blog/seata-dynamic-config-and-dynamic-disable","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-dynamic-config-and-dynamic-disable.md","source":"@site/blog/seata-dynamic-config-and-dynamic-disable.md","title":"seata-dynamic-config-and-dynamic-disable","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-dsproxy-deadlock","permalink":"/blog/seata-dsproxy-deadlock"},"nextItem":{"title":"seata-go-1.2.0","permalink":"/blog/seata-go-1.2.0"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-go-1.2.0","metadata":{"permalink":"/blog/seata-go-1.2.0","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-go-1.2.0.md","source":"@site/blog/seata-go-1.2.0.md","title":"seata-go-1.2.0","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-dynamic-config-and-dynamic-disable","permalink":"/blog/seata-dynamic-config-and-dynamic-disable"},"nextItem":{"title":"seata-golang-communication-mode","permalink":"/blog/seata-golang-communication-mode"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-golang-communication-mode","metadata":{"permalink":"/blog/seata-golang-communication-mode","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-golang-communication-mode.md","source":"@site/blog/seata-golang-communication-mode.md","title":"seata-golang-communication-mode","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-go-1.2.0","permalink":"/blog/seata-go-1.2.0"},"nextItem":{"title":"seata-ha-practice","permalink":"/blog/seata-ha-practice"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-ha-practice","metadata":{"permalink":"/blog/seata-ha-practice","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-ha-practice.md","source":"@site/blog/seata-ha-practice.md","title":"seata-ha-practice","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-golang-communication-mode","permalink":"/blog/seata-golang-communication-mode"},"nextItem":{"title":"seata-meetup-hangzhou","permalink":"/blog/seata-meetup-hangzhou"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-meetup-hangzhou","metadata":{"permalink":"/blog/seata-meetup-hangzhou","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-meetup-hangzhou.md","source":"@site/blog/seata-meetup-hangzhou.md","title":"seata-meetup-hangzhou","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-ha-practice","permalink":"/blog/seata-ha-practice"},"nextItem":{"title":"seata-mybatisplus-analysis","permalink":"/blog/seata-mybatisplus-analysis"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-mybatisplus-analysis","metadata":{"permalink":"/blog/seata-mybatisplus-analysis","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-mybatisplus-analysis.md","source":"@site/blog/seata-mybatisplus-analysis.md","title":"seata-mybatisplus-analysis","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-meetup-hangzhou","permalink":"/blog/seata-meetup-hangzhou"},"nextItem":{"title":"seata-nacos-analysis","permalink":"/blog/seata-nacos-analysis"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-nacos-analysis","metadata":{"permalink":"/blog/seata-nacos-analysis","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-nacos-analysis.md","source":"@site/blog/seata-nacos-analysis.md","title":"seata-nacos-analysis","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-mybatisplus-analysis","permalink":"/blog/seata-mybatisplus-analysis"},"nextItem":{"title":"seata-nacos-docker","permalink":"/blog/seata-nacos-docker"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-nacos-docker","metadata":{"permalink":"/blog/seata-nacos-docker","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-nacos-docker.md","source":"@site/blog/seata-nacos-docker.md","title":"seata-nacos-docker","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-nacos-analysis","permalink":"/blog/seata-nacos-analysis"},"nextItem":{"title":"seata-observable-practice","permalink":"/blog/seata-observable-practice"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-observable-practice","metadata":{"permalink":"/blog/seata-observable-practice","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-observable-practice.md","source":"@site/blog/seata-observable-practice.md","title":"seata-observable-practice","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-nacos-docker","permalink":"/blog/seata-nacos-docker"},"nextItem":{"title":"seata-quick-start","permalink":"/blog/seata-quick-start"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-quick-start","metadata":{"permalink":"/blog/seata-quick-start","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-quick-start.md","source":"@site/blog/seata-quick-start.md","title":"seata-quick-start","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-observable-practice","permalink":"/blog/seata-observable-practice"},"nextItem":{"title":"seata-raft-detailed-explanation","permalink":"/blog/seata-raft-detailed-explanation"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-raft-detailed-explanation","metadata":{"permalink":"/blog/seata-raft-detailed-explanation","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-raft-detailed-explanation.md","source":"@site/blog/seata-raft-detailed-explanation.md","title":"seata-raft-detailed-explanation","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-quick-start","permalink":"/blog/seata-quick-start"},"nextItem":{"title":"seata-rpc-refactor","permalink":"/blog/seata-rpc-refactor"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-rpc-refactor","metadata":{"permalink":"/blog/seata-rpc-refactor","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-rpc-refactor.md","source":"@site/blog/seata-rpc-refactor.md","title":"seata-rpc-refactor","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-raft-detailed-explanation","permalink":"/blog/seata-raft-detailed-explanation"},"nextItem":{"title":"seata-sourcecode-client-bootstrap","permalink":"/blog/seata-sourcecode-client-bootstrap"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-sourcecode-client-bootstrap","metadata":{"permalink":"/blog/seata-sourcecode-client-bootstrap","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-sourcecode-client-bootstrap.md","source":"@site/blog/seata-sourcecode-client-bootstrap.md","title":"seata-sourcecode-client-bootstrap","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-rpc-refactor","permalink":"/blog/seata-rpc-refactor"},"nextItem":{"title":"seata-sourcecode-server-bootstrap","permalink":"/blog/seata-sourcecode-server-bootstrap"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-sourcecode-server-bootstrap","metadata":{"permalink":"/blog/seata-sourcecode-server-bootstrap","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-sourcecode-server-bootstrap.md","source":"@site/blog/seata-sourcecode-server-bootstrap.md","title":"seata-sourcecode-server-bootstrap","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-sourcecode-client-bootstrap","permalink":"/blog/seata-sourcecode-client-bootstrap"},"nextItem":{"title":"seata-spring-boot-aop-aspectj","permalink":"/blog/seata-spring-boot-aop-aspectj"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-spring-boot-aop-aspectj","metadata":{"permalink":"/blog/seata-spring-boot-aop-aspectj","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-spring-boot-aop-aspectj.md","source":"@site/blog/seata-spring-boot-aop-aspectj.md","title":"seata-spring-boot-aop-aspectj","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-sourcecode-server-bootstrap","permalink":"/blog/seata-sourcecode-server-bootstrap"},"nextItem":{"title":"seata-tcc","permalink":"/blog/seata-tcc"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-tcc","metadata":{"permalink":"/blog/seata-tcc","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-tcc.md","source":"@site/blog/seata-tcc.md","title":"seata-tcc","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-spring-boot-aop-aspectj","permalink":"/blog/seata-spring-boot-aop-aspectj"},"nextItem":{"title":"seata-xa-introduce","permalink":"/blog/seata-xa-introduce"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-xa-introduce","metadata":{"permalink":"/blog/seata-xa-introduce","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-xa-introduce.md","source":"@site/blog/seata-xa-introduce.md","title":"seata-xa-introduce","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-tcc","permalink":"/blog/seata-tcc"},"nextItem":{"title":"springboot-dubbo-mybatisplus-seata","permalink":"/blog/springboot-dubbo-mybatisplus-seata"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/springboot-dubbo-mybatisplus-seata","metadata":{"permalink":"/blog/springboot-dubbo-mybatisplus-seata","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/springboot-dubbo-mybatisplus-seata.md","source":"@site/blog/springboot-dubbo-mybatisplus-seata.md","title":"springboot-dubbo-mybatisplus-seata","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"seata-xa-introduce","permalink":"/blog/seata-xa-introduce"},"nextItem":{"title":"tcc-mode-applicable-scenario-analysis","permalink":"/blog/tcc-mode-applicable-scenario-analysis"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/tcc-mode-applicable-scenario-analysis","metadata":{"permalink":"/blog/tcc-mode-applicable-scenario-analysis","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/tcc-mode-applicable-scenario-analysis.md","source":"@site/blog/tcc-mode-applicable-scenario-analysis.md","title":"tcc-mode-applicable-scenario-analysis","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"springboot-dubbo-mybatisplus-seata","permalink":"/blog/springboot-dubbo-mybatisplus-seata"},"nextItem":{"title":"tcc-mode-design-principle","permalink":"/blog/tcc-mode-design-principle"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/tcc-mode-design-principle","metadata":{"permalink":"/blog/tcc-mode-design-principle","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/tcc-mode-design-principle.md","source":"@site/blog/tcc-mode-design-principle.md","title":"tcc-mode-design-principle","description":"Placeholder. DO NOT DELETE.","date":"2024-01-23T11:36:01.000Z","formattedDate":"January 23, 2024","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"tcc-mode-applicable-scenario-analysis","permalink":"/blog/tcc-mode-applicable-scenario-analysis"},"nextItem":{"title":"Exploring the Journey of Open Source Development in Seata Project","permalink":"/blog/explore-seata-journey"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/explore-seata-journey","metadata":{"permalink":"/blog/explore-seata-journey","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/explore-seata-journey.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/explore-seata-journey.md","title":"Exploring the Journey of Open Source Development in Seata Project","description":"In this article, I will share my journey as a developer in the Seata community, along with the experiences and insights I have gained during this adventure.","date":"2023-11-27T00:00:00.000Z","formattedDate":"November 27, 2023","tags":[],"readingTime":11.105,"hasTruncateMarker":false,"authors":[{"name":"Yinxiangkun - Tsinghua University, participant in Seata Summer of Code"}],"frontMatter":{"title":"Exploring the Journey of Open Source Development in Seata Project","keywords":["Seata","open source","Summer of Code","distributed transactions"],"description":"In this article, I will share my journey as a developer in the Seata community, along with the experiences and insights I have gained during this adventure.","author":"Yinxiangkun - Tsinghua University, participant in Seata Summer of Code","date":"2023-11-27T00:00:00.000Z"},"prevItem":{"title":"tcc-mode-design-principle","permalink":"/blog/tcc-mode-design-principle"},"nextItem":{"title":"Seata:Bridging Data and Applications","permalink":"/blog/seata-connect-data-and-application"}},"content":"> Seata is an open-source distributed transaction solution dedicated to providing high-performance and user-friendly distributed transaction services in a microservices architecture. During this year\'s Summer of Code event, I joined the Apache Seata (Incubator) community, completed the Summer of Code project, and have been actively involved in the community ever since. I was fortunate to share my developer experience at the YunQi Developer Show during the Cloud Conferen\\n\\n## Relevant Background\\n\\nBefore formally introducing my experiences, I would like to provide some relevant background information to explain why I chose to participate in open source and how I got involved. There are various motivations for participating in open source, and here are some of the main reasons I believe exist:\\n\\n- **Learning**: Participating in open source provides us with the opportunity to contribute to open-source projects developed by different organizations, interact with industry experts, and offers learning opportunities.\\n- **Skill Enhancement**: In my case, I usually work with Java and Python for backend development. However, when participating in the Seata project, I had the chance to learn the Go language, expanding my backend technology stack. Additionally, as a student, it\'s challenging to encounter production-level frameworks or applications, and the open-source community provided me with this opportunity.\\n- **Interest**: Many of my friends are passionate about open source, enjoying programming and being enthusiastic about open source.\\n- **Job Seeking**: Participating in open source can enrich our portfolio, adding weight to resumes.\\n- **Work Requirements**: Sometimes, involvement in open source is to address work-related challenges or meet job requirements.\\n\\nThese are some reasons for participating in open source. For me, learning, skill enhancement, and interest are the primary motivations. Whether you are a student or a working professional, if you have the willingness to participate in open source, don\'t hesitate. Anyone can contribute to open-source projects. Age, gender, occupation, and location are not important; the key is your passion and curiosity about open-source projects.\\n\\n**The opportunity for me to participate in open source arose when I joined the Open Source Promotion Plan (OSPP) organized by the Institute of Software, Chinese Academy of Sciences.**\\n\\nOSPP is an open-source activity for university developers. The community releases open-source projects, and student developers complete project development under the guidance of mentors. The completed results are contributed to the community, merged into the community repository, and participants receive project bonuses and certificates. OSPP is an excellent opportunity to enter the open-source community, and it was my first formal encounter with open-source projects. This experience opened a new door for me. I deeply realized that participating in the construction of open-source projects, sharing your technical achievements, and enabling more developers to use what you contribute is a joyful and meaningful endeavor.\\n\\nThe image below, officially released by OSPP, shows that the number of participating communities and students has been increasing year by year since 2020, and the event is getting better. This year, a total of 133 community projects were involved, each providing several topics, with each student selecting only one topic. Choosing a community to participate in and finding a suitable topic in such a large number of communities is a relatively complex task.\\n\\n![img](/img/blog/explore-seata-ospp.png)\\n\\n**Considering factors such as community activity, technical stack compatibility, and guidance for newcomers, I ultimately chose to join the Seata community.**\\n\\nSeata is an open-source distributed transaction framework that provides a complete distributed transaction solution, including AT, TCC, Saga, and XA transaction modes, and supports multiple programming languages and data storage solutions. Since its open source in 2019, Seata has been around for **5** years, with over **300** contributors in the community. The project has received **24k+** stars and is a mature community. Seata is compatible with **10+** mainstream RPC frameworks and RDBMS, has integration relationships with **20+** communities, and is applied to business systems by **thousands** of customers. It can be considered the de facto standard for distributed transaction solutions.\\n\\n![img](/img/blog/explore-seata-apache.png)\\n\\n### Seata\'s Journey to Apache Incubator\\n\\nOn October 29, 2023, Seata was formally donated to the Apache Software Foundation and became an incubating project. After incubation, Seata is expected to become the first top-level distributed transaction framework project under the Apache Software Foundation. This donation will propel Seata to a broader development scope, profoundly impacting ecosystem construction, and benefiting more developers. This significant milestone also opens up broader development opportunities for Seata.\\n\\n## Development Journey\\n\\n**Having introduced some basic information, the following sections will delve into my development journey in the Seata community.**\\n\\nBefore officially starting development, I undertook several preparatory steps. Given Seata\'s five years of development and the accumulation of hundreds of thousands of lines of code, direct involvement requires a certain learning curve. I share some preparatory experiences in the hope of providing inspiration.\\n\\n1. **Documentation and Blogs as Primary Resources**\\n   - Text materials such as documentation and blogs help newcomers quickly understand project background and code structure.\\n   - Official documentation is the primary reference material, providing insights into everything the official documentation deems necessary to know.\\n   ![img](/img/blog/explore-seata-docs.png)\\n   - Blogs, secondary to official documentation, are often written by developers or advanced users. Blogs may delve deeper into specific topics, such as theoretical models of projects, project structure, and source code analysis of specific modules.\\n   ![img](/img/blog/explore-seata-blogs.png)\\n   - Public accounts (such as WeChat) are similar to blogs, generally containing technical articles. An advantage of public accounts is the ability to subscribe for push notifications, allowing for technical reading during spare time.\\n   ![img](/img/blog/explore-seata-pubs.png)\\n   - Additionally, slides from online or offline community presentations and meetups provide meaningful textual materials.\\n   ![img](/img/blog/explore-seata-slides.png)\\n   - Apart from official materials, many third-party resources are available for learning, such as understanding specific implementations and practices through user-shared use cases, exploring the project\'s ecosystem through integration documentation from third-party communities, and learning through video tutorials. However, among all these materials, I consider official documentation and blogs to be the most helpful.\\n\\n2. **Familiarizing Yourself with the Framework**\\n   - Not all text materials need to be thoroughly read. Understanding is superficial if confined to paper. Practice should commence when you feel you understand enough. The \\"Get Started\\" section in the official documentation is a step-by-step guide to understanding the project\'s basic workflow.\\n   - Another approach is to find examples or demonstrations provided by the official project, build and run them, understand the meanings of code and configurations, and learn about the project\'s requirements, goals, existing features, and architecture through usage.\\n   - For instance, Seata has a repository named \\"seata-samples\\" containing over 20 use cases, covering scenarios like Seata integration with Dubbo, integration with SCA, and Nacos integration. These examples cover almost all supported scenarios.\\n\\n3. **Roughly Reading Source Code to Grasp Main Logic**\\n   - In the preparation phase, roughly reading the source code to grasp the project\'s main logic is crucial. Efficiently understanding a project\'s core content is a skill that requires long-term accumulation.\\n   - First, through the previously mentioned preparation steps, understanding the project\'s concepts, interactions, and process models is helpful.\\n   - Taking Seata as an example, through official documentation and practical operations, you can understand the three roles in Seata\'s transaction domain: TC (Transaction Coordinator), TM (Transaction Manager), and RM (Resource Manager). TC, deployed independently as a server, maintains the state of global and branch transactions, crucial for Seata\'s high availability. TM interacts with TC, defining the start, commit, or rollback of global transactions. RM manages resources for branch transaction processing, interacts with TC to register branch transactions and report branch transaction states, and drives branch transaction commit or rollback. After roughly understanding the interaction between these roles, grasping the project\'s main logic becomes easier.\\n   ![img](/img/solution.png)\\n   - Having a mental impression of these models makes it easier to extract the main logic from the source code. For example, analyzing the Seata TC transaction coordinator, as a server-side application deployed independently of the business, involves starting the server locally and tracking it through the startup class. This analysis can reveal some initialization logic, such as service registration and initialization of global locks. Tracking the code through RPC calls can reveal how TC persists global and branch transactions and how it drives global transaction commit or rollback.\\n   - However, for embedded client framework code without a startup class entry point for analysis, starting with a sample can be effective. Finding references to framework code in a sample allows for code reading. For instance, a crucial annotation in Seata is `GlobalTransaction`, used to identify a global transaction. To understand how TM analyzes this annotation, one can use the IDE\'s search function to find the interceptor for `GlobalTransaction` and analyze its logic.\\n   - Here\'s a tip: Unit tests often focus on the functional aspects of a single module. Reading unit tests can reveal a module\'s input-output, logic boundaries, and understanding the code through the unit test\'s call chain is an essential means of understanding the source code.\\n\\n**With everything prepared, the next step is to actively participate in the community.**\\n\\n## Ways to Contribute and Personal Insights\\n\\nThere are various ways to participate, with one of the most common being to check the project\'s Issues list. Communities often mark issues suitable for new contributors with special labels such as \\"good-first-issue,\\" \\"contributions-welcome,\\" and \\"help-wanted.\\" Interested tasks can be filtered through these labels.\\n\\n![img](/img/blog/explore-seata-issues.png)\\n\\nIn addition to Issues, GitHub provides a discussion feature where you can participate in open discussions and gain new ideas.\\n\\n![img](/img/blog/explore-seata-discussion.png)\\n\\nFurthermore, communities often hold regular meetings, such as weekly or bi-weekly meetings, where you can stay updated on the community\'s latest progress, ask questions, and interact with other community members.\\n\\n## Summary and Insights\\n\\nI initially joined the Seata community through the Open Source Summer Program. I completed my project, implemented new features for Seata Saga, and carried out a series of optimizations. However, I didn\'t stop there. My open-source experience with Seata provided me with the most valuable developer experience in my student career. Over time, I continued to stay active in the community through the aforementioned participation methods. This was mainly due to the following factors:\\n\\n1. **Communication and Networking:** The mentorship system provided crucial support. During development, the close collaboration between my mentor and me played a key role in adapting to community culture and workflow. My mentor not only helped me acclimate to the community but also provided design ideas and shared work-related experiences and insights, all of which were very helpful for my development. Additionally, Seata community founder Ming Cai provided a lot of assistance, including establishing contacts with other students, helping with code reviews, and offering many opportunities.\\n\\n2. **Positive Feedback:** During Seata\'s development, I experienced a virtuous cycle. Many details provided positive feedback, such as my contributions being widely used and beneficial to users, and the recognition of my development efforts by the community. This positive feedback strengthened my desire to continue contributing to the Seata community.\\n\\n3. **Skill Enhancement:** Participating in Seata development greatly enhanced my abilities. Here, I could learn production-level code, including performance optimization, interface design, and techniques for boundary judgment. I could directly participate in the operation of an open-source project, including project planning, scheduling, and communication. Additionally, I gained insights into how a distributed transaction framework is designed and implemented.\\n\\nIn addition to these valuable developer experiences, I gained some personal insights into participating in open source. To inspire other students interested in joining open-source communities, I made a simple summary:\\n\\n1. **Understand and Learn Community Culture and Values:** Every open-source community has different cultures and values. Understanding a community\'s culture and values is crucial for successful participation. Observing and understanding the daily development and communication styles of other community members is a good way to learn community culture. Respect others\' opinions and embrace different viewpoints in the community.\\n\\n2. **Dare to Take the First Step:** Don\'t be afraid of challenges; taking the first step is key to participating in open-source communities. You can start by tackling issues labeled \\"good-first-issue\\" or by contributing to documentation, unit tests, etc. Overcoming the fear of difficulties, actively trying, and learning are crucial.\\n\\n3. **Have Confidence in Your Work:** Don\'t doubt your abilities. Everyone starts from scratch, and no one is born an expert. Participating in open-source communities is a process of learning and growth that requires continuous practice and experience accumulation.\\n\\n4. **Actively Participate in Discussions, Keep Learning Different Technologies:** Don\'t hesitate to ask questions, whether about specific project technologies or challenges in the development process. Also, don\'t limit yourself to one domain. Try to learn and master different programming languages, frameworks, and tools. This broadens your technical perspective and provides valuable insights for the project.\\n\\n---\\n\\nThrough my open-source journey, I accumulated valuable experience and skills. This not only helped me grow into a more valuable developer but also gave me a profound understanding of the power of open-source communities. However, I am not just an individual participant; I represent a part of the Seata community. Seata, as a continuously growing and evolving open-source project, has tremendous potential and faces new challenges. Therefore, I want to emphasize the importance of the Seata community and its future potential. It has entered the incubation stage of the Apache Software Foundation, a significant milestone that will bring broader development opportunities for Seata. Seata welcomes more developers and contributors to join us. Let\'s work together to drive the development of this open-source project and contribute to the advancement of the distributed transaction field."},{"id":"/seata-connect-data-and-application","metadata":{"permalink":"/blog/seata-connect-data-and-application","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-connect-data-and-application.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-connect-data-and-application.md","title":"Seata:Bridging Data and Applications","description":"This article introduces the past, present, and future evolution of Seata.","date":"2023-06-30T00:00:00.000Z","formattedDate":"June 30, 2023","tags":[],"readingTime":13.905,"hasTruncateMarker":false,"authors":[{"name":"Ji Min - Founder of the Seata Open Source Community, Leader of the Distributed Transactions Team"}],"frontMatter":{"title":"Seata:Bridging Data and Applications","keywords":["Seata","Distributed Transactions","Data Consistency","Microservices"],"description":"This article introduces the past, present, and future evolution of Seata.","author":"Ji Min - Founder of the Seata Open Source Community, Leader of the Distributed Transactions Team","date":"June 30, 2023"},"prevItem":{"title":"Exploring the Journey of Open Source Development in Seata Project","permalink":"/blog/explore-seata-journey"},"nextItem":{"title":"Alibaba Seata Resolves Idempotence, Dangling, and Empty Rollback Issues in TCC Mode","permalink":"/blog/seata-tcc-fence"}},"content":"This article mainly introduces the evolutionary journey of distributed transactions from internal development to commercialization and open source, as well as the current progress and future planning of the Seata community.\\nSeata is an open-source distributed transaction solution designed to provide a comprehensive solution for distributed transactions under modern microservices architecture. Seata offers complete distributed transaction solutions, including AT, TCC, Saga, and XA transaction modes, supporting various programming languages and data storage schemes. Seata also provides easy-to-use APIs, extensive documentation, and examples to facilitate quick development and deployment for enterprises applying Seata.\\n**Seata\'s advantages lie in its high availability, high performance, and high scalability, and it does not require extra complex operations for horizontal scaling.** Seata is currently used in thousands of customer business systems on Alibaba Cloud, and its reliability has been recognized and applied by major industry manufacturers.\\nAs an open-source project, the Seata community is also expanding continuously, becoming an important platform for developers to exchange, share, and learn, attracting more and more attention and support from enterprises.\\nToday, I will primarily share about Seata on the following three topics:\\n- **From TXC/GTS to Seata**\\n- **Latest developments in the Seata community**\\n- **Future planning for the Seata community**\\n  <br/>\\n### From TXC/GTS to Seata\\n#### The Origin of Distributed Transactions\\n![Product Matrix](/img/blog/\u4ea7\u54c1\u77e9\u9635.jpg)\\nSeata is internally codenamed TXC (taobao transaction constructor) within Alibaba, a name with a strong organizational structure flavor. TXC originated from Alibaba\'s Wushi (Five Color Stones) project, which in ancient mythology were the stones used by the goddess N\xfcwa to mend the heavens, symbolizing Alibaba\'s important milestone in the evolution from monolithic architecture to distributed architecture. During this project, a batch of epoch-making Internet middleware was developed, including the well-known \\"Big Three\\":\\n- **HSF service invocation framework**\\n  Solves service communication issues after the transition from monolithic applications to service-oriented architectures.\\n- **TDDL database sharding framework**\\n  Addresses storage capacity and connection count issues of databases at scale.\\n- **MetaQ messaging framework**\\n  Addresses asynchronous invocation issues.\\n  The birth of the Big Three satisfied the basic requirements of microservices-based business development, but the data consistency issues that arose after microservices were not properly addressed, lacking a unified solution. The likelihood of data consistency issues in microservices is much higher than in monolithic applications, and the increased complexity of moving from in-process calls to network calls exacerbates the production of exceptional scenarios. The increase in service hops also makes it impossible for upstream and downstream services to coordinate data rollback in the event of a business processing exception. TXC was born to address the pain points of data consistency at the application architecture layer, and the core data consistency scenarios it aimed to address included:\\n- **Consistency across services.** Coordinates rollback of upstream and downstream service nodes in the event of system exceptions such as call timeouts and business exceptions.\\n- **Data consistency in database sharding.** Ensures internal transactions during logical SQL operations on business layers are consistent across different data shards.\\n- **Data consistency in message sending.** Addresses the inconsistency between data operations and successful message sending.\\n  To overcome the common scenarios encountered, TXC was seamlessly integrated with the Big Three. When businesses use the Big Three for development, they are completely unaware of TXC\'s presence in the background, do not have to consider the design of data consistency, and leave it to the framework to ensure, allowing businesses to focus more on their own development, greatly improving development efficiency.\\n  <br/>\\n  ![GTS Architecture](/img/blog/GTS\u67b6\u6784.jpg)\\n  TXC has been widely used within Alibaba Group for many years and has been baptized by the surging traffic of large-scale events like Singles\' Day, significantly improving business development efficiency and ensuring data accuracy, eliminating financial and reputational issues caused by data inconsistencies. With the continuous evolution of the architecture, **a standard three-node cluster can now handle peak values of nearly 100K TPS and millisecond-level transaction processing. In terms of availability and performance, it has reached a four-nines SLA guarantee, ensuring no failures throughout the year even in unattended conditions.**\\n  <br/>\\n#### The Evolution of Distributed Transactions\\nThe birth of new things is always accompanied by doubts. Is middleware capable of ensuring data consistency reliable? The initial birth of TXC was just a vague theory, lacking theoretical models and engineering practice. After we conducted MVP (Minimum Viable Product) model testing and promoted business deployment, we often encountered faults and frequently had to wake up in the middle of the night to deal with issues, wearing wristbands to sleep to cope with emergency responses. These were the most painful years I went through technically after taking over the team.\\n![Evolution of Distributed Transactions](/img/blog/\u5206\u5e03\u5f0f\u4e8b\u52a1\u6f14\u8fdb.jpg)\\nSubsequently, we had extensive discussions and systematic reviews. We first needed to define the consistency problem. Were we to achieve majority consensus consistency like RAFT, solve database consistency issues like Google Spanner, or something else? Looking at the top-down layered structure from the application node, it mainly includes development frameworks, service invocation frameworks, data middleware, database drivers, and databases. We had to decide at which layer to solve the data consistency problem. We compared the consistency requirements, universality, implementation complexity, and business integration costs faced when solving data consistency issues at different levels. In the end, we weighed the pros and cons, decided to keep the implementation complexity to ourselves, and adopted the AT mode initially as a consistency component. We needed to ensure high consistency, but not be locked into specific database implementations, ensuring the generality of scenarios and the business integration costs were low enough to be easily implemented. This is also why TXC initially adopted the AT mode.\\n**A distributed transaction is not just a framework; it\'s a system.** We defined the consistency problem in theory, abstractly conceptualized modes, roles, actions, and isolation, etc. From an engineering practice perspective, we defined the programming model, including low-intrusion annotations, simple method templates, and flexible APIs, and defined basic and enhanced transaction capabilities (e.g., how to support a large number of activities at low cost), as well as capabilities in operations, security, performance, observability, and high availability.\\n![Transaction Logical Model](/img/blog/\u4e8b\u52a1\u903b\u8f91\u6a21\u578b.jpg)\\nWhat problems do distributed transactions solve? A classic and tangible example is the money transfer scenario. The transfer process includes subtracting balance and adding balance, how do we ensure the atomicity of the operation? Without any intervention, these two steps may encounter various problems, such as account B being canceled or service call timeouts, etc.\\n**Timeout issues have always been a difficult problem to solve in distributed applications**; we cannot accurately know whether service B has executed and in what order. From a data perspective, this means the money in account B may not be successfully added. After the service-oriented transformation, each node only has partial information, while the transaction itself requires global coordination of all nodes, thus requiring a centralized role with a god\'s-eye view, capable of obtaining all information, which is the **TC (transaction coordinator)**, used to globally coordinate the transaction state. The **TM (Transaction Manager)** is the role that drives the generation of transaction proposals. However, even gods nod off, and their judgments are not always correct, so we need an **RM (resource manager)** role to verify the authenticity of the transaction as a representative of the soul. This is TXC\'s most basic philosophical model. We have methodologically verified that its data consistency is very complete, of course, our cognition is bounded. Perhaps the future will prove we were turkey engineers, but under current circumstances, its model is already sufficient to solve most existing problems.\\n![Distributed Transaction Performance](/img/blog/\u5206\u5e03\u5f0f\u4e8b\u52a1\u6027\u80fd.jpg)\\n**After years of architectural evolution, from the perspective of transaction single-link latency, TXC takes an average of about 0.2 milliseconds to process at the start of the transaction and about 0.4 milliseconds for branch registration, with the entire transaction\'s additional latency within the millisecond range. This is also the theoretical limit value we have calculated. In terms of throughput, the TPS of a single node reaches 30,000 times/second, and the TPS of a standard cluster is close to 100,000 times/second.**\\n<br/>\\n#### Seata Open Source\\nWhy go open source? This is a question many people have asked me. In 2017, we commercialized the GTS (Global Transaction Service) product sold on Alibaba Cloud, with both public and private cloud forms. At this time, the internal group developed smoothly, but we encountered various problems in the process of commercialization. The problems can be summed up in two main categories: **First, developers are quite lacking in the theory of distributed transactions,** most people do not even understand what local transactions are, let alone distributed transactions. **Second, there are problems with product maturity,** often encountering various strange scenario issues, leading to a sharp rise in support and delivery costs, and R&D turning into after-sales customer service.\\nWe reflected on why we encountered so many problems. The main issue here is that Alibaba Group internally has a unified language stack and unified technology stack, and our polishing of specific scenarios is very mature. Serving Alibaba, one company, and serving thousands of enterprises on the cloud is fundamentally different, which also made us realize that our product\'s scenario ecology was not well developed. On GitHub, more than 80% of open-source software is basic software, and basic software primarily solves the problem of scenario universality, so it cannot be locked in by a single enterprise, like Linux, which has a large number of community distributions. Therefore, in order to make our product better, we chose to open source and co-build with developers to popularize more enterprise users.\\n![Alibaba Open Source](/img/blog/\u963f\u91cc\u5f00\u6e90.jpg)\\nAlibaba\'s open-source journey has gone through three main stages. **The first stage is the stage where Dubbo is located, where developers contribute out of love,** Dubbo has been open sourced for over 10 years, and time has fully proven that Dubbo is an excellent open-source software, and its microkernel plugin extensibility design is an important reference for me when I initially open sourced Seata. When designing software, we need to consider which is more important between extensibility and performance, whether we are doing a three-year design, a five-year design, or a ten-year design that meets business development. While solving the 0-1 service call problem, can we predict the governance problems after the 1-100 scale-up?\\n**The second stage is the closed loop of open source and commercialization, where commercialization feeds back into the open-source community, promoting the development of the open-source community.** I think cloud manufacturers are more likely to do open source well for the following reasons:\\n- First, the cloud is a scaled economy, which must be established on a stable and mature kernel foundation, packaging its product capabilities including high availability, maintenance-free, and elasticity on top of it. An unstable kernel will inevitably lead to excessive delivery and support costs, and high penetration of the R&D team\'s support Q&A will prevent large-scale replication, and high penetration rates will prevent rapid evolution and iteration of products.\\n- Second, commercial products know business needs better. Our internal technical teams often YY requirements from a development perspective, and what they make is not used by anyone, and thus does not form a value conversion. The business requirements collected through commercialization are all real, so its open source kernel must also evolve in this direction. Failure to evolve in this direction will inevitably lead to architectural splits on both sides, increasing the team\'s maintenance costs.\\n- Finally, the closed loop of open source and commercialization can promote better development of both parties. If the open-source kernel often has various problems, would you believe that its commercial product is good enough?\\n  **The third stage is systematization and standardization.** First, systematization is the basis of open-source solutions. Alibaba\'s open-source projects are mostly born out of internal e-commerce scenario practices. For example, Higress is used to connect Ant Group\'s gateways; Nacos carries services with millions of instances and tens of millions of connections; Sentinel provides degradation and throttling capabilities for high availability during major promotions; and Seata ensures transaction data consistency. This set of systematized open-source solutions is designed based on the best practices of Alibaba\'s e-commerce ecosystem. Second, standardization is another important feature. Taking OpenSergo as an example, it is both a standard and an implementation. In the past few years, the number of domestic open-source projects has exploded. However, the capabilities of various open-source products vary greatly, and many compatibility issues arise when integrating with each other. Therefore, open-source projects like OpenSergo can define some standardized capabilities and interfaces and provide some implementations, which will greatly help the development of the entire open-source ecosystem.\\n  <br/>\\n### Latest Developments in the Seata Community\\n#### Introduction to the Seata Community\\n![Community Introduction](/img/blog/\u793e\u533a\u7b80\u4ecb.jpg)\\n**At present, Seata has open-sourced 4 transaction modes, including AT, TCC, Saga, and XA, and is actively exploring other viable transaction solutions.** Seata has integrated with more than 10 mainstream RPC frameworks and relational databases, and has integrated or been integrated relationships with more than 20 communities. In addition, we are also exploring languages other than Java in the multi-language system, such as Golang, PHP, Python, and JS.\\nSeata has been applied to business systems by thousands of customers. Seata applications have become more mature, with successful cooperation with the community in the financial business scenarios of CITIC Bank and Everbright Bank, and successfully adopted into core accounting systems. The landing of microservices systems in financial scenarios is very stringent, which also marks a new level of maturity for Seata\'s kernel.\\n<br/>\\n#### Seata Ecosystem Expansion\\n![Ecosystem Expansion](/img/blog/\u6269\u5c55\u751f\u6001.jpg)\\n**Seata adopts a microkernel and plugin architecture design, exposing rich extension points in APIs, registry configuration centers, storage modes, lock control, SQL parsers, load balancing, transport, protocol encoding and decoding, observability, and more.** This allows businesses to easily perform flexible extensions and select technical components.\\n<br/>\\n#### Seata Application Cases\\n![Application Cases](/img/blog/\u5e94\u7528\u6848\u4f8b.jpg)\\n**Case 1: China Aviation Information\'s Air Travel Project**\\nThe China Aviation Information Air Travel project introduced Seata in the 0.2 version to solve the data consistency problem of ticket and coupon business, greatly improving development efficiency, reducing asset losses caused by data inconsistency, and enhancing user interaction experience.\\n**Case 2: Didi Chuxing\'s Two-Wheeler Business Unit**\\nDidi Chuxing\'s Two-Wheeler Business Unit introduced Seata in version 0.6.1, solving the data consistency problem of business processes such as blue bicycles, electric vehicles, and assets, optimizing the user experience, and reducing asset loss.\\n**Case 3: Meituan\'s Infrastructure**\\nMeituan\'s infrastructure team developed the internal distributed transaction solution Swan based on the open-source Seata project, which is used to solve distributed transaction problems within Meituan\'s various businesses.\\n**Case 4: Hema Town**\\nHema Town uses Seata to control the flower-stealing process in game interactions, significantly shortening the development cycle from 20 days to 5 days, effectively reducing development costs.\\n<br/>\\n#### Evolution of Seata Transaction Modes\\n![Mode Evolution](/img/blog/\u6a21\u5f0f\u6f14\u8fdb.jpg)\\n<br/>\\n#### Current Progress of Seata\\n- Support for Oracle and PostgreSQL multi-primary keys.\\n- Support for Dubbo3.\\n- Support for Spring Boot3.\\n- Support for JDK 17.\\n- Support for ARM64 images.\\n- Support for multiple registration models.\\n- Extended support for various SQL syntaxes.\\n- Support for GraalVM Native Image.\\n- Support for Redis lua storage mode.\\n  <br/>\\n### Seata 2.x Development Planning\\n![Development Planning](/img/blog/\u53d1\u5c55\u89c4\u5212.jpg)\\nMainly includes the following aspects:\\n- **Storage/Protocol/Features**\\n  Explore storage and computing separation in Raft cluster mode; better experience, unify the current 4 transaction mode APIs; compatible with GTS protocol; support Saga annotations; support distributed lock control; support data perspective insight and governance.\\n- **Ecosystem**\\n  Support more databases, more service frameworks, while exploring support for the domestic trust creation ecosystem; support the MQ ecosystem; further enhance APM support.\\n- **Solutions**\\n  In addition to supporting microservices ecosystems, explore multi-cloud solutions; closer to cloud-native solutions; add security and traffic protection capabilities; achieve self-convergence of core components in the architecture.\\n- **Multi-Language Ecosystem**\\n  Java is the most mature in the multi-language ecosystem, continue to improve other supported programming languages, while exploring Transaction Mesh solutions that are independent of languages.\\n- **R&D Efficiency/Experience**\\n  Improve test coverage, prioritize quality, compatibility, and stability; restructure the official website\'s documentation to improve the hit rate of document searches; simplify operations and deployment on the experience side, achieve one-click installation and metadata simplification; console supports transaction control and online analysis capabilities.\\n\\nIn one sentence, the 2.x plan is summarized as: **Bigger scenarios, bigger ecosystems, from usable to user-friendly.**\\n<br/>\\n### Contact Information for the Seata Community\\n![Contact Information](/img/blog/\u8054\u7cfb\u65b9\u5f0f.jpg)"},{"id":"/seata-tcc-fence","metadata":{"permalink":"/blog/seata-tcc-fence","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-tcc-fence.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-tcc-fence.md","title":"Alibaba Seata Resolves Idempotence, Dangling, and Empty Rollback Issues in TCC Mode","description":"Seata version 1.5.1 from Alibaba has finally resolved the issues of idempotence, dangling, and empty rollback in TCC (Try-Confirm-Cancel) mode. This article mainly explains how Seata addresses these problems.","date":"2022-06-25T00:00:00.000Z","formattedDate":"June 25, 2022","tags":[],"readingTime":10.97,"hasTruncateMarker":false,"authors":[{"name":"Zhu Jinjun"}],"frontMatter":{"title":"Alibaba Seata Resolves Idempotence, Dangling, and Empty Rollback Issues in TCC Mode","author":"Zhu Jinjun","keywords":["Seata","TCC","idempotence","dangling","empty rollback"],"description":"Seata version 1.5.1 from Alibaba has finally resolved the issues of idempotence, dangling, and empty rollback in TCC (Try-Confirm-Cancel) mode. This article mainly explains how Seata addresses these problems.","date":"2022/06/25"},"prevItem":{"title":"Seata:Bridging Data and Applications","permalink":"/blog/seata-connect-data-and-application"},"nextItem":{"title":"Q&A on the New Version of Snowflake Algorithm","permalink":"/blog/seata-snowflake-explain"}},"content":"Today, let\'s talk about how the new version (1.5.1) of Alibaba\'s Seata resolves the issues of idempotence, dangling, and empty rollback in TCC mode.\\n\\n## 1 TCC \\n\\nTCC mode is the most classic solution for distributed transactions. It divides the distributed transaction into two phases. In the try phase, resources are reserved for each branch transaction. If all branch transactions successfully reserve resources, the global transaction proceeds to the commit phase for committing the transaction globally. However, if any node fails to reserve resources, the global transaction enters the cancel phase to rollback the transaction globally.\\n\\nTaking traditional order, inventory, and account services as an example, in the try phase, resources are attempted to be reserved by inserting orders, deducting inventory, and deducting amounts. These three services require local transaction commits, and the resources can be transferred to an intermediate table. In the commit phase, the resources reserved in the try phase are transferred to the final table. In the cancel phase, the resources reserved in the try phase are released, such as returning the account amount to the customer\'s account.\\n\\n**Note: The try phase must involve committing local transactions. For example, when deducting the order amount, the money must be deducted from the customer\'s account. If it is not deducted, there will be a problem in the commit phase if the customer\'s account does not have enough money.**\\n\\n### 1.1 try-commit\\n\\nIn the try phase, resources are first reserved, and then they are deducted in the commit phase. The diagram below illustrates this process:\\n\\n![fence-try-commit](/img/blog/fence-try-commit.png)\\n\\n\\n### 1.2 try-cancel\\n\\nIn the try phase, resources are first reserved. If the deduction of inventory fails, leading to a rollback of the global transaction, the resources are released in the cancel phase. The diagram below illustrates this process:\\n\\n![fence-try-cancel](/img/blog/fence-try-cancel.png)\\n\\n\\n## 2 TCC Advantages\\n\\nThe biggest advantage of TCC mode is its high efficiency. In the try phase, the resource locking in TCC mode is not a true lock, but rather a real local transaction submission that reserves resources in an intermediate state without the need for blocking and waiting. Therefore, it is more efficient than other modes.\\n\\nAdditionally, the TCC mode can be optimized as follows:\\n\\n### 2.1 Asynchronous Commit\\n\\nAfter the try phase succeeds, instead of immediately entering the confirm/cancel phase, it is considered that the global transaction has already ended. A scheduled task is started to asynchronously execute the confirm/cancel phase, which involves deducting or releasing resources. This approach can greatly improve performance.\\n\\n### 2.2 Same-Database Mode\\n\\nIn the TCC mode, there are three roles:\\n\\n- TM: Manages the global transaction, including starting the global transaction and committing/rolling back the global transaction.\\n- RM: Manages the branch transaction.\\n- TC: Manages the state of the global transaction and branch transactions.\\n\\nThe diagram below is from the Seata official website:\\n\\n![fence-fiffrent-db](/img/blog/fence-fiffrent-db.png)\\n\\nWhen TM starts a global transaction, RM needs to send a registration message to TC, and TC saves the state of the branch transaction. When TM requests a commit or rollback, TC needs to send commit or rollback messages to RM. In this way, in a distributed transaction with two branch transactions, there are four RPCs between TC and RM.\\n\\nAfter optimization, the process is as shown in the diagram below:\\n\\nTC saves the state of the global transaction. When TM starts a global transaction, RM no longer needs to send a registration message to TC. Instead, it saves the state of the branch transaction locally. After TM sends a commit or rollback message to TC, the asynchronous thread in RM first retrieves the uncommitted branch transactions saved locally, and then sends a message to TC to obtain the state of the global transaction in which the local branch transaction is located, in order to determine whether to commit or rollback the local transaction.\\n\\nWith this optimization, the number of RPCs is reduced by 50%, resulting in a significant performance improvement.\\n\\n## 3 RM Code Example\\n\\nTaking the inventory service as an example, the RM inventory service interface code is as follows:\\n```Java\\n@LocalTCC\\npublic interface StorageService {\\n\\n    /**\\n     * decrease\\n     * @param xid \\n     * @param productId \\n     * @param count \\n     * @return\\n     */\\n    @TwoPhaseBusinessAction(name = \\"storageApi\\", commitMethod = \\"commit\\", rollbackMethod = \\"rollback\\", useTCCFence = true)\\n    boolean decrease(String xid, Long productId, Integer count);\\n\\n    /**\\n     * commit\\n     * @param actionContext\\n     * @return\\n     */\\n    boolean commit(BusinessActionContext actionContext);\\n\\n    /**\\n     * rollback\\n     * @param actionContext\\n     * @return\\n     */\\n    boolean rollback(BusinessActionContext actionContext);\\n}\\n```\\nBy using the `@LocalTCC` annotation, when the RM is initialized, it registers a branch transaction with the TC. The `try` phase method (e.g., `decrease` method) is annotated with `@TwoPhaseBusinessAction`, which defines the branch transaction\'s `resourceId`, `commit` method, `cancel` method, and the `useTCCFence` property, which will be explained in the next section.\\n\\n## 4 Issues with TCC\\n\\nThere are three major issues with the TCC pattern: idempotence, suspension, and empty rollback. In version 1.5.1 of Seata, a transaction control table named `tcc_fence_log` is introduced to address these issues. The `useTCCFence` property mentioned in the previous `@TwoPhaseBusinessAction` annotation is used to enable or disable this mechanism, with a default value of `false`.\\n\\nThe creation SQL statement for the `tcc_fence_log` table (in MySQL syntax) is as follows:\\n\\n```SQL\\nCREATE TABLE IF NOT EXISTS `tcc_fence_log`\\n(\\n    `xid`           VARCHAR(128)  NOT NULL COMMENT \'global id\',\\n    `branch_id`     BIGINT        NOT NULL COMMENT \'branch id\',\\n    `action_name`   VARCHAR(64)   NOT NULL COMMENT \'action name\',\\n    `status`        TINYINT       NOT NULL COMMENT \'status(tried:1;committed:2;rollbacked:3;suspended:4)\',\\n    `gmt_create`    DATETIME(3)   NOT NULL COMMENT \'create time\',\\n    `gmt_modified`  DATETIME(3)   NOT NULL COMMENT \'update time\',\\n    PRIMARY KEY (`xid`, `branch_id`),\\n    KEY `idx_gmt_modified` (`gmt_modified`),\\n    KEY `idx_status` (`status`)\\n) ENGINE = InnoDB\\nDEFAULT CHARSET = utf8mb4;\\n```\\n\\n### 4.1 Idempotence\\n\\nDuring the commit/cancel phase, if the TC does not receive a response from the branch transaction, it needs to retry the operation. Therefore, it is necessary for the branch transaction to support idempotence.\\n\\nLet\'s take a look at how this is addressed in the new version. The following code is from the `TCCResourceManager` class:\\n\\n```Java\\n@Override\\npublic BranchStatus branchCommit(BranchType branchType, String xid, long branchId, String resourceId,\\n\\t\\t\\t\\t\\t\\t\\t\\t String applicationData) throws TransactionException {\\n\\tTCCResource tccResource = (TCCResource)tccResourceCache.get(resourceId);\\n\\tObject targetTCCBean = tccResource.getTargetBean();\\n\\tMethod commitMethod = tccResource.getCommitMethod();\\n\\ttry {\\n\\t\\t//BusinessActionContext\\n\\t\\tBusinessActionContext businessActionContext = getBusinessActionContext(xid, branchId, resourceId,\\n\\t\\t\\tapplicationData);\\n\\t\\tObject[] args = this.getTwoPhaseCommitArgs(tccResource, businessActionContext);\\n\\t\\tObject ret;\\n\\t\\tboolean result;\\n\\t\\t//whether the useTCCFence property is set to true\\n\\t\\tif (Boolean.TRUE.equals(businessActionContext.getActionContext(Constants.USE_TCC_FENCE))) {\\n\\t\\t\\ttry {\\n\\t\\t\\t\\tresult = TCCFenceHandler.commitFence(commitMethod, targetTCCBean, xid, branchId, args);\\n\\t\\t\\t} catch (SkipCallbackWrapperException | UndeclaredThrowableException e) {\\n\\t\\t\\t\\tthrow e.getCause();\\n\\t\\t\\t}\\n\\t\\t} else {\\n\\t\\t}\\n\\t\\tLOGGER.info(\\"TCC resource commit result : {}, xid: {}, branchId: {}, resourceId: {}\\", result, xid, branchId, resourceId);\\n\\t\\treturn result ? BranchStatus.PhaseTwo_Committed : BranchStatus.PhaseTwo_CommitFailed_Retryable;\\n\\t} catch (Throwable t) {\\n\\t\\treturn BranchStatus.PhaseTwo_CommitFailed_Retryable;\\n\\t}\\n}\\n```\\nThe above code shows that when executing the commit method of the branch transaction, it first checks if the `useTCCFence` property is `true`. If it is `true`, it follows the `commitFence` logic in the `TCCFenceHandler` class; otherwise, it follows the normal commit logic.\\n\\nThe `commitFence` method in the `TCCFenceHandler` class calls the `commitFence` method of the same class. The code is as follows:\\n\\n```Java\\npublic static boolean commitFence(Method commitMethod, Object targetTCCBean,\\n\\t\\t\\t\\t\\t\\t\\t\\t  String xid, Long branchId, Object[] args) {\\n\\treturn transactionTemplate.execute(status -> {\\n\\t\\ttry {\\n\\t\\t\\tConnection conn = DataSourceUtils.getConnection(dataSource);\\n\\t\\t\\tTCCFenceDO tccFenceDO = TCC_FENCE_DAO.queryTCCFenceDO(conn, xid, branchId);\\n\\t\\t\\tif (tccFenceDO == null) {\\n\\t\\t\\t\\tthrow new TCCFenceException(String.format(\\"TCC fence record not exists, commit fence method failed. xid= %s, branchId= %s\\", xid, branchId),\\n\\t\\t\\t\\t\\t\\tFrameworkErrorCode.RecordAlreadyExists);\\n\\t\\t\\t}\\n\\t\\t\\tif (TCCFenceConstant.STATUS_COMMITTED == tccFenceDO.getStatus()) {\\n\\t\\t\\t\\tLOGGER.info(\\"Branch transaction has already committed before. idempotency rejected. xid: {}, branchId: {}, status: {}\\", xid, branchId, tccFenceDO.getStatus());\\n\\t\\t\\t\\treturn true;\\n\\t\\t\\t}\\n\\t\\t\\tif (TCCFenceConstant.STATUS_ROLLBACKED == tccFenceDO.getStatus() || TCCFenceConstant.STATUS_SUSPENDED == tccFenceDO.getStatus()) {\\n\\t\\t\\t\\tif (LOGGER.isWarnEnabled()) {\\n\\t\\t\\t\\t\\tLOGGER.warn(\\"Branch transaction status is unexpected. xid: {}, branchId: {}, status: {}\\", xid, branchId, tccFenceDO.getStatus());\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\treturn false;\\n\\t\\t\\t}\\n\\t\\t\\treturn updateStatusAndInvokeTargetMethod(conn, commitMethod, targetTCCBean, xid, branchId, TCCFenceConstant.STATUS_COMMITTED, status, args);\\n\\t\\t} catch (Throwable t) {\\n\\t\\t\\tstatus.setRollbackOnly();\\n\\t\\t\\tthrow new SkipCallbackWrapperException(t);\\n\\t\\t}\\n\\t});\\n}\\n```\\nFrom the code, we can see that when committing the transaction, it first checks if there is a record in the `tcc_fence_log` table. If a record exists, it checks the transaction execution status and returns. This ensures idempotence by avoiding duplicate commits if the transaction status is already `STATUS_COMMITTED`. If there is no record in the `tcc_fence_log` table, a new record is inserted for later retry detection.\\n\\nThe rollback logic is similar to the commit logic and is implemented in the `rollbackFence` method of the `TCCFenceHandler` class.\\n\\n### 4.2 Empty Rollback\\n\\nIn the scenario shown in the following diagram, the account service consists of a cluster of two nodes. During the try phase, the account service on Node 1 encounters a failure. Without considering retries, the global transaction must reach the end state, requiring a cancel operation to be performed on the account service.\\n\\n![fence-empty-rollback](/img/blog/fence-empty-rollback.png)\\n\\nSeata\'s solution is to insert a record into the `tcc_fence_log` table during the try phase, with the `status` field set to `STATUS_TRIED`. During the rollback phase, it checks if the record exists, and if it doesn\'t, the rollback operation is not executed. The code is as follows:\\n\\n```Java\\n//TCCFenceHandler \\npublic static Object prepareFence(String xid, Long branchId, String actionName, Callback<Object> targetCallback) {\\n\\treturn transactionTemplate.execute(status -> {\\n\\t\\ttry {\\n\\t\\t\\tConnection conn = DataSourceUtils.getConnection(dataSource);\\n\\t\\t\\tboolean result = insertTCCFenceLog(conn, xid, branchId, actionName, TCCFenceConstant.STATUS_TRIED);\\n\\t\\t\\tLOGGER.info(\\"TCC fence prepare result: {}. xid: {}, branchId: {}\\", result, xid, branchId);\\n\\t\\t\\tif (result) {\\n\\t\\t\\t\\treturn targetCallback.execute();\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tthrow new TCCFenceException(String.format(\\"Insert tcc fence record error, prepare fence failed. xid= %s, branchId= %s\\", xid, branchId),\\n\\t\\t\\t\\t\\t\\tFrameworkErrorCode.InsertRecordError);\\n\\t\\t\\t}\\n\\t\\t} catch (TCCFenceException e) {\\n\\t\\t} catch (Throwable t) {\\n\\t\\t}\\n\\t});\\n}\\n```\\nThe processing logic in the Rollback phase is as follows:\\n\\n```Java\\n//TCCFenceHandler \\npublic static boolean rollbackFence(Method rollbackMethod, Object targetTCCBean,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tString xid, Long branchId, Object[] args, String actionName) {\\n\\treturn transactionTemplate.execute(status -> {\\n\\t\\ttry {\\n\\t\\t\\tConnection conn = DataSourceUtils.getConnection(dataSource);\\n\\t\\t\\tTCCFenceDO tccFenceDO = TCC_FENCE_DAO.queryTCCFenceDO(conn, xid, branchId);\\n\\t\\t\\t// non_rollback\\n\\t\\t\\tif (tccFenceDO == null) {\\n\\t\\t\\t\\t//The rollback logic is not executed\\n\\t\\t\\t\\treturn true;\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tif (TCCFenceConstant.STATUS_ROLLBACKED == tccFenceDO.getStatus() || TCCFenceConstant.STATUS_SUSPENDED == tccFenceDO.getStatus()) {\\n\\t\\t\\t\\t\\tLOGGER.info(\\"Branch transaction had already rollbacked before, idempotency rejected. xid: {}, branchId: {}, status: {}\\", xid, branchId, tccFenceDO.getStatus());\\n\\t\\t\\t\\t\\treturn true;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tif (TCCFenceConstant.STATUS_COMMITTED == tccFenceDO.getStatus()) {\\n\\t\\t\\t\\t\\tif (LOGGER.isWarnEnabled()) {\\n\\t\\t\\t\\t\\t\\tLOGGER.warn(\\"Branch transaction status is unexpected. xid: {}, branchId: {}, status: {}\\", xid, branchId, tccFenceDO.getStatus());\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\treturn false;\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t\\treturn updateStatusAndInvokeTargetMethod(conn, rollbackMethod, targetTCCBean, xid, branchId, TCCFenceConstant.STATUS_ROLLBACKED, status, args);\\n\\t\\t} catch (Throwable t) {\\n\\t\\t\\tstatus.setRollbackOnly();\\n\\t\\t\\tthrow new SkipCallbackWrapperException(t);\\n\\t\\t}\\n\\t});\\n}\\n```\\nupdateStatusAndInvokeTargetMethod method executes the following SQL:\\n\\n```sql\\nupdate tcc_fence_log set status = ?, gmt_modified = ?\\n    where xid = ? and  branch_id = ? and status = ? ;\\n```\\nAs we can see, it updates the value of the status field in the tcc_fence_log table from STATUS_TRIED to STATUS_ROLLBACKED. If the update is successful, the rollback logic is executed.\\n\\n### 4.3 Hanging\\nHanging refers to a situation where, due to network issues, the RM did not receive the try instruction initially, but after executing the rollback, the RM receives the try instruction and successfully reserves resources. This leads to the inability to release the reserved resources, as shown in the following diagram:\\n\\n![fence-suspend](/img/blog/fence-suspend.png)\\n\\nSeata solves this problem by checking if there is a record for the current xid in the tcc_fence_log table before executing the rollback method. If there is no record, it inserts a new record into the tcc_fence_log table with the status STATUS_SUSPENDED and does not perform the rollback operation. The code is as follows:\\n\\n```Java\\npublic static boolean rollbackFence(Method rollbackMethod, Object targetTCCBean,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tString xid, Long branchId, Object[] args, String actionName) {\\n\\treturn transactionTemplate.execute(status -> {\\n\\t\\ttry {\\n\\t\\t\\tConnection conn = DataSourceUtils.getConnection(dataSource);\\n\\t\\t\\tTCCFenceDO tccFenceDO = TCC_FENCE_DAO.queryTCCFenceDO(conn, xid, branchId);\\n\\t\\t\\t// non_rollback\\n\\t\\t\\tif (tccFenceDO == null) {\\n\\t\\t\\t\\tboolean result = insertTCCFenceLog(conn, xid, branchId, actionName, TCCFenceConstant.STATUS_SUSPENDED);\\n\\t\\t\\t\\treturn true;\\n\\t\\t\\t} else {\\n\\t\\t\\t}\\n\\t\\t\\treturn updateStatusAndInvokeTargetMethod(conn, rollbackMethod, targetTCCBean, xid, branchId, TCCFenceConstant.STATUS_ROLLBACKED, status, args);\\n\\t\\t} catch (Throwable t) {\\n\\t\\t}\\n\\t});\\n}\\n```\\nWhen executing the try phase method, a record for the current xid is first inserted into the tcc_fence_log table, which causes a primary key conflict. The code is as follows:\\n```Java\\n//TCCFenceHandler \\npublic static Object prepareFence(String xid, Long branchId, String actionName, Callback<Object> targetCallback) {\\n\\treturn transactionTemplate.execute(status -> {\\n\\t\\ttry {\\n\\t\\t\\tConnection conn = DataSourceUtils.getConnection(dataSource);\\n\\t\\t\\tboolean result = insertTCCFenceLog(conn, xid, branchId, actionName, TCCFenceConstant.STATUS_TRIED);\\n\\t\\t} catch (TCCFenceException e) {\\n\\t\\t\\tif (e.getErrcode() == FrameworkErrorCode.DuplicateKeyException) {\\n\\t\\t\\t\\tLOGGER.error(\\"Branch transaction has already rollbacked before,prepare fence failed. xid= {},branchId = {}\\", xid, branchId);\\n\\t\\t\\t\\taddToLogCleanQueue(xid, branchId);\\n\\t\\t\\t}\\n\\t\\t\\tstatus.setRollbackOnly();\\n\\t\\t\\tthrow new SkipCallbackWrapperException(e);\\n\\t\\t} catch (Throwable t) {\\n\\t\\t}\\n\\t});\\n}\\n```\\nNote: The queryTCCFenceDO method in the SQL statement uses for update, so there is no need to worry about not being able to determine the execution result of the local transaction in the rollback method due to the inability to obtain records from the tcc_fence_log table.\\n\\n### 5 Summary\\nTCC mode is a very important transaction mode in distributed transactions. However, idempotence, hanging, and empty rollback have always been issues that need to be considered in TCC mode. The Seata framework perfectly solves these problems in version 1.5.1.\\nThe operations on the tcc_fence_log table also need to consider transaction control. Seata uses a proxy data source to execute the operations on the tcc_fence_log table and the RM business operations in the same local transaction. This ensures that the local operations and the operations on the tcc_fence_log table succeed or fail together."},{"id":"/seata-snowflake-explain","metadata":{"permalink":"/blog/seata-snowflake-explain","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-snowflake-explain.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-snowflake-explain.md","title":"Q&A on the New Version of Snowflake Algorithm","description":"In the previous analysis of the new version of the Snowflake algorithm, we mentioned two changes made in the new version:","date":"2021-06-21T00:00:00.000Z","formattedDate":"June 21, 2021","tags":[],"readingTime":7.315,"hasTruncateMarker":false,"authors":[{"name":"selfishlover"}],"frontMatter":{"title":"Q&A on the New Version of Snowflake Algorithm","author":"selfishlover","keywords":["Seata","snowflake","UUID","page split"],"date":"2021/06/21"},"prevItem":{"title":"Alibaba Seata Resolves Idempotence, Dangling, and Empty Rollback Issues in TCC Mode","permalink":"/blog/seata-tcc-fence"},"nextItem":{"title":"Analysis of Seata\'s Distributed UUID Generator Based on Improved Snowflake Algorithm","permalink":"/blog/seata-analysis-UUID-generator"}},"content":"In the previous analysis of the new version of the Snowflake algorithm, we mentioned two changes made in the new version:\\n1. The timestamp no longer constantly follows the system clock.\\n2. The exchange of positions between node ID and timestamp. From the original:\\n   ![Original Bit Allocation Strategy](/img/blog/seata/uuid/before.png)\\n   to:\\n   ![Improved Bit Allocation Strategy](/img/blog/seata/uuid/after.png)\\n\\nA careful student raised a question: In the new version, the algorithm is indeed monotonically increasing within a single node, but in a multi-instance deployment, it is no longer globally monotonically increasing! Because it is obvious that the node ID is in the high bits, so the generated ID with a larger node ID will definitely be greater than the ID with a smaller node ID, regardless of the chronological order. In contrast, the original algorithm, with the timestamp in the high bits and always following the system clock, can ensure that IDs generated earlier are smaller than those generated later. Only when two nodes happen to generate IDs at the same timestamp, the order of the two IDs is determined by the node ID. So, does it mean that the new version of the algorithm is wrong?\\n\\nThis is a great question! The fact that students can raise this question indicates a deep understanding of the essential differences between the standard Snowflake algorithm and the new version. This is commendable! Here, let\'s first state the conclusion: indeed, the new version of the algorithm does not possess global monotonicity, but this does not affect our original intention (to reduce database page splits). This conclusion may seem counterintuitive but can be proven.\\n\\nBefore providing the proof, let\'s briefly review some knowledge about page splits in databases. Taking the classic MySQL InnoDB as an example, InnoDB uses a B+ tree index where the leaf nodes of the primary key index also store the complete records of data rows. The leaf nodes are linked together in the form of a doubly linked list. The physical storage form of the leaf nodes is a data page, and each data page can store up to N rows of records (where N is inversely proportional to the size of each row). As shown in the diagram:\\n![Data Page](/img/blog/seata/uuid/page1.png)\\nThe characteristics of the B+ tree require that the left node should be smaller than the right node. What happens if we want to insert a record with an ID of 25 at this point (assuming each data page can only hold 4 records)? The answer is that it will cause a page split, as shown in the diagram:\\n![Page Split](/img/blog/seata/uuid/page2.png)\\nPage splits are unfriendly to I/O, requiring the creation of new data pages, copying and transferring part of the records from the old data page, etc., and should be avoided as much as possible.\\n\\nIdeally, the primary key ID should be sequentially increasing (for example, setting the primary key as auto_increment). This way, a new page will only be needed when the current data page is full, and the doubly linked list will always grow sequentially at the tail, avoiding any mid-node splits.\\n\\nIn the worst-case scenario, if the primary key ID is randomly generated and unordered (for example, a UUID string in Java), new records will be randomly assigned to any data page. If the page is already full, it will trigger a page split.\\n\\nIf the primary key ID is generated by the standard Snowflake algorithm, in the best-case scenario, only one node is generating IDs within each timestamp. In this case, the algorithm\'s effect is equivalent to the ideal situation of sequential incrementation, similar to auto_increment. In the worst-case scenario, all nodes within each timestamp are generating IDs, and the algorithm\'s effect is close to unordered (but still much better than completely unordered UUIDs, as the workerId with only 10 bits limits the nodes to a maximum of 1024). In actual production, the algorithm\'s effectiveness depends on business traffic, and the lower the concurrency, the closer the algorithm is to the ideal scenario.\\n\\nSo, how does it fare with the new version of the algorithm?  \\n\\n\\nThe new version of the algorithm, from a global perspective, produces IDs in an unordered manner. However, for each workerId, the generated IDs are strictly monotonically increasing. Additionally, since workerId is finite, it can divide into a maximum of 1024 subsequences, each of which is monotonically increasing.\\n\\nFor a database, initially, the received IDs may be unordered, coming from various subsequences, as illustrated here:\\n![Initial State](/img/blog/seata/uuid/page3.png)\\n\\nIf, at this point, a worker1-seq2 arrives, it will clearly cause a page split:\\n![First Split](/img/blog/seata/uuid/page4.png)\\n\\nHowever, after the split, interesting things happen. For worker1, subsequent seq3, seq4 will not cause page splits anymore (because there is still space), and seq5 only needs to link to a new page for sequential growth (the difference is that this new page is not at the tail of the doubly linked list). Note that the subsequent IDs of worker1 will not be placed after any nodes from worker2 or beyond (thus avoiding page splits for later nodes) because they are always smaller than the IDs of worker2; nor will they be placed before the current node of worker1 (thus avoiding page splits for previous nodes) because the subsequences of worker1 are always monotonically increasing. Here, we refer to such subsequences as reaching a steady state, meaning that the subsequence has \\"stabilized,\\" and its subsequent growth will only occur at the end of the subsequence without causing page splits for other nodes.\\n\\nThe same principle can be extended to all subsequences. Regardless of how chaotic the IDs received by the database are initially, after a finite number of page splits, the doubly linked list can always reach a stable state:\\n![Steady State](/img/blog/seata/uuid/page5.png)\\n\\nAfter reaching the steady state, subsequent IDs will only grow sequentially within their respective subsequences, without causing page splits. The difference between this sequential growth and the sequential growth of auto_increment is that the former has 1024 growth points (the ends of various subsequences), while the latter only has one at the end.\\n\\nAt this point, we can answer the question posed at the beginning: indeed, the new algorithm is not globally monotonically increasing, but the algorithm **converges**. After reaching a steady state, the new algorithm can achieve the same effect as global sequential incrementation.\\n\\n## Further Considerations\\n\\nThe discussion so far has focused on the continuous growth of sequences. However, in practical production, there is not only the insertion of new data but also the deletion of old data. Data deletion may lead to page merging (InnoDB, if it finds that the space utilization of two adjacent data pages is both less than 50%, it will merge them). How does this affect the new algorithm?\\n\\nAs we have seen in the above process, the essence of the new algorithm is to utilize early page splits to gradually separate different subsequences, allowing the algorithm to continuously converge to a steady state. Page merging, on the other hand, may reverse this process by merging different subsequences back into the same data page, hindering the convergence of the algorithm. Especially in the early stages of convergence, frequent page merging may even prevent the algorithm from converging forever (I just separated them, and now I\'m merging them back together, back to square one~)! However, after convergence, only page merging at the end nodes of each subsequence has the potential to disrupt the steady state (merging the end node of one subsequence with the head node of the next subsequence). Merging on the remaining nodes of the subsequence does not affect the steady state because the subsequence remains ordered, albeit with a shorter length.\\n\\nTaking Seata\'s server as an example, the data in the three tables of the server has a relatively short lifecycle. After a global transaction ends, the data is cleared. This is not friendly to the new algorithm, as it does not provide enough time for convergence. However, there is already a pull request (PR) for delayed deletion in the review process, and with this PR, the effect will be much better. For example, periodic weekly cleanup allows sufficient time for the algorithm to converge in the early stages, and for most of the time, the database can benefit from it. At the time of cleanup, the worst-case result is that the table is cleared, and the algorithm starts from scratch.\\n\\nIf you wish to apply the new algorithm to a business system, make sure to ensure that the algorithm has time to converge. For example, for user tables or similar, where data is intended to be stored for a long time, the algorithm can naturally converge. Alternatively, implement a mechanism for delayed deletion, providing enough time for the algorithm to converge.\\n\\nIf you have better opinions and suggestions, feel free to contact the Seata community!"},{"id":"/seata-analysis-UUID-generator","metadata":{"permalink":"/blog/seata-analysis-UUID-generator","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-UUID-generator.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-UUID-generator.md","title":"Analysis of Seata\'s Distributed UUID Generator Based on Improved Snowflake Algorithm","description":"Seata incorporates a distributed UUID generator to assist in generating global transaction IDs and branch transaction IDs. The desired characteristics for this generator include high performance, global uniqueness, and trend incrementation.","date":"2021-05-08T00:00:00.000Z","formattedDate":"May 8, 2021","tags":[],"readingTime":5.71,"hasTruncateMarker":false,"authors":[{"name":"selfishlover"}],"frontMatter":{"title":"Analysis of Seata\'s Distributed UUID Generator Based on Improved Snowflake Algorithm","author":"selfishlover","keywords":["Seata","snowflake","UUID"],"date":"2021/05/08"},"prevItem":{"title":"Q&A on the New Version of Snowflake Algorithm","permalink":"/blog/seata-snowflake-explain"},"nextItem":{"title":"Seata New Feature Support -- Undo_Log Compression","permalink":"/blog/seata-feature-undo-log-compress"}},"content":"Seata incorporates a distributed UUID generator to assist in generating global transaction IDs and branch transaction IDs. The desired characteristics for this generator include high performance, global uniqueness, and trend incrementation.\\n\\nHigh performance is self-explanatory, and global uniqueness is crucial to prevent confusion between different global transactions or branch transactions. Additionally, trend incrementation is valuable for users employing databases as the storage tool for TC clusters, as it can reduce the frequency of data page splits, thereby minimizing database IO pressure (the `branch_table` table uses the branch transaction ID as the primary key).\\n\\nIn the older version of Seata (prior to 1.4), the implementation of this generator was based on the standard version of the Snowflake algorithm. The standard Snowflake algorithm has been well-documented online, so we won\'t delve into it here. If you\'re unfamiliar with it, consider referring to existing resources before continuing with this article.\\n\\nHere, we discuss some drawbacks of the standard Snowflake algorithm:\\n1. **Clock Sensitivity:** Since ID generation is always tied to the current operating system\'s timestamp (leveraging the monotonicity of time), a clock rollback may result in repeated IDs. Seata\'s strategy to handle this is by recording the last timestamp and rejecting service if the current timestamp is less than the recorded value (indicating a clock rollback). The service waits until the timestamp catches up with the recorded value. However, this means the TC will be in an unavailable state during this period.\\n\\n2. **Burst Performance Limit:** The standard Snowflake algorithm claims a high QPS, approximately 4 million/s. However, strictly speaking, this is a bit misleading. The timestamp unit of the algorithm is milliseconds, and the bit length allocated to the sequence number is 12, allowing for 4096 sequence spaces per millisecond. So, a more accurate description would be 4096/ms. The distinction between 4 million/s and 4096/ms lies in the fact that the former doesn\'t require every millisecond\'s concurrency to be below 4096. Seata also adheres to this limitation. If the sequence space for the current timestamp is exhausted, it will spin-wait for the next timestamp.\\n\\nIn newer versions (1.4 and beyond), the generator has undergone optimizations and improvements to address these issues effectively. The core idea of the improvement is to decouple from the operating system\'s timestamp, with the generator obtaining the system\'s current timestamp only during initialization as the initial timestamp. Subsequently, it no longer synchronizes with the system timestamp. The incrementation is solely driven by the incrementation of the sequence number. For example, when the sequence number reaches its maximum value (4095), the next request causes an overflow of the 12-bit space. The sequence number resets to zero, and the overflow carry is added to the timestamp, incrementing it by 1. Thus, the timestamp and sequence number can be considered as a single entity. In practice, we adjusted the bit allocation strategy for the 64-bit ID, swapping the positions of the timestamp and node ID for easier handling of this overflow carry:\\n\\nOriginal Bit Allocation Strategy:\\n![Original Bit Allocation Strategy](/img/blog/seata/uuid/before.png)\\n\\nModified Bit Allocation Strategy (swapping timestamp and node ID):\\n![Modified Bit Allocation Strategy](/img/blog/seata/uuid/after.png)\\n\\nThis arrangement allows the timestamp and sequence number to be contiguous in memory, making it easy to use an `AtomicLong` to simultaneously store them.\\n\\n```\\n/**\\n * timestamp and sequence mix in one Long\\n * highest 11 bit: not used\\n * middle  41 bit: timestamp\\n * lowest  12 bit: sequence\\n */\\nprivate AtomicLong timestampAndSequence;\\n```\\nThe highest 11 bits can be determined during initialization and remain unchanged thereafter:\\n```\\n/**\\n * business meaning: machine ID (0 ~ 1023)\\n * actual layout in memory:\\n * highest 1 bit: 0\\n * middle 10 bit: workerId\\n * lowest 53 bit: all 0\\n */\\nprivate long workerId;\\n```\\nProducing an ID is then straightforward\uff1a\\n```\\npublic long nextId() {\\n   // Obtain the incremented timestamp and sequence number\\n   long next = timestampAndSequence.incrementAndGet();\\n   // Extract the lowest 53 bits\\n   long timestampWithSequence = next & timestampAndSequenceMask;\\n   // Perform a bitwise OR operation with the previously saved top 11 bits\\n   return workerId | timestampWithSequence;\\n}\\n```\\n\\nAt this point, we can observe the following:\\n\\n1. The generator no longer has a burst performance limit of 4096/ms. If the sequence number space for a timestamp is exhausted, it will directly advance to the next timestamp, \\"borrowing\\" the sequence number space of the next timestamp (there is no need to worry about serious consequences of this \\"advance consumption,\\" as the reasons will be explained below).\\n\\n2. The generator has a weak dependency on the operating system clock. During runtime, the generator is not affected by clock backtracking (whether it is manually backtracked or due to machine clock drift) because the generator only fetches the system clock once at startup, and thereafter, they no longer stay synchronized. The only possible scenario for duplicate IDs is a significant clock backtracking during restart (either deliberate human backtracking or modification of the operating system time zone, such as changing Beijing time to London time~ Machine clock drift is typically in the millisecond range and won\'t have such a large impact).\\n\\n3. Will continuous \\"advance consumption\\" cause the generator\'s timestamps to be significantly ahead of the system timestamps, resulting in ID duplicates upon restart? In theory, yes, but practically almost impossible. To achieve this effect, it would mean that the generator\'s QPS received must be consistently stable at over 400w/s~ To be honest, even TC can\'t handle such high traffic, so, the bottleneck is definitely not in the generator.\\n\\nIn addition, we also adjusted the strategy for generating node IDs. In the original version, when the user did not manually specify a node ID, it would take the low 10 bits of the local IPv4 address as the node ID. In practical production, it was found that there were occasional occurrences of duplicate node IDs (mostly users deploying with k8s). For example, the following IPs would result in duplicates:\\n- 192.168.4.10\\n- 192.168.8.10\\n\\nMeaning, as long as the low 2 bits of the fourth byte and the third byte of the IP are the same, duplicates would occur. The new version\'s strategy is to prioritize taking the low 10 bits from the MAC address of the local network card. If the local machine does not have a valid network card configuration, it randomly picks one from [0, 1023] as the node ID. After this adjustment, it seems that new version users are no longer reporting the same issue (of course, it remains to be tested over time, but in any case, it won\'t be worse than the IP extraction strategy).\\n\\nThe above is a brief analysis of Seata\'s distributed UUID generator. If you find this generator useful, you can directly use it in your project. Its class declaration is `public`, and the full class name is:\\n`io.seata.common.util.IdWorker`\\n\\nOf course, if you have better ideas, you are also welcome to discuss them with the Seata community."},{"id":"/seata-feature-undo-log-compress","metadata":{"permalink":"/blog/seata-feature-undo-log-compress","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-feature-undo-log-compress.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-feature-undo-log-compress.md","title":"Seata New Feature Support -- Undo_Log Compression","description":"Current Situation & Pain Points","date":"2021-05-07T00:00:00.000Z","formattedDate":"May 7, 2021","tags":[],"readingTime":3.785,"hasTruncateMarker":false,"authors":[{"name":"chd"}],"frontMatter":{"title":"Seata New Feature Support -- Undo_Log Compression","author":"chd","keywords":["Seata","undo_log","compress"],"date":"2021/05/07"},"prevItem":{"title":"Analysis of Seata\'s Distributed UUID Generator Based on Improved Snowflake Algorithm","permalink":"/blog/seata-analysis-UUID-generator"},"nextItem":{"title":"Integration of Spring Cloud with Seata for Distributed Transaction - TCC Mode","permalink":"/blog/integrate-seata-tcc-mode-with-spring-cloud"}},"content":"## Current Situation & Pain Points\\n\\nFor Seata, it records the before and after data of DML operations to perform possible rollback operations, and stores this data in a blob field in the database. For batch operations such as insert, update, delete, etc., the number of affected rows may be significant, concatenated into a large field inserted into the database, which may lead to the following issues:\\n\\n1. Exceeding the maximum write limit for a single database operation (such as the `max_allowed_package` parameter in MySQL).\\n2. Significant network IO and database disk IO overhead due to a large amount of data.\\n\\n## Brainstorming\\n\\nFor the first issue, the `max_allowed_package` parameter limit can be increased based on the actual situation of the business to avoid the \\"query is too large\\" problem. For the second issue, increasing bandwidth and using high-performance SSD as the database storage medium can help.\\n\\nThe above solutions involve external or costly measures. Is there a framework-level solution to address the pain points mentioned above?\\n\\nConsidering the root cause of the pain points mentioned above, the problem lies in the generation of excessively large data fields. Therefore, if the corresponding data can be compressed at the business level before data transmission and storage, theoretically, it can solve the problems mentioned above.\\n\\n## Feasibility Analysis\\n\\nCombining the brainstorming above, in practical development, when large batch operations are required, they are often scheduled during periods of relatively low user activity and low concurrency. At such times, CPU and memory resources can be relatively more utilized to quickly complete the corresponding operations. Therefore, by consuming CPU and memory resources to compress rollback data, the size of data transmission and storage can be reduced.\\n\\nAt this point, two things need to be demonstrated:\\n\\n1. After compression, it can reduce the pressure on network IO and database disk IO. This can be measured by the total time taken for data compression + storage in the database.\\n2. After compression, the efficiency of compression compared to the original data size. This can be measured by the data size before and after compression.\\n\\nTesting the time spent on compressing network usage:\\n\\n![image](https://user-images.githubusercontent.com/22959373/95567752-f55ddf80-0a55-11eb-8092-1f1d99855bdd.png)\\n\\n## Compression Ratio Test:\\n\\n![image](https://user-images.githubusercontent.com/22959373/95567834-0ad30980-0a56-11eb-9d7e-48b74babbea4.png)\\n\\nThe test results clearly indicate that using gzip or zip compression can significantly reduce the pressure on the database and network transmission. At the same time, it can substantially decrease the size of the stored data.\\n\\n### Implementation\\n\\n#### Implementation Approach\\n\\n![Compression](https://user-images.githubusercontent.com/22959373/116281711-8f039900-a7bc-11eb-91f8-82afdbb9f932.png)\\n\\n#### Partial Code\\n\\n```properties\\n# Whether to enable undo_log compression, default is true\\nseata.client.undo.compress.enable=true\\n\\n# Compressor type, default is zip, generally recommended to be zip\\nseata.client.undo.compress.type=zip\\n\\n# Compression threshold for enabling compression, default is 64k\\nseata.client.undo.compress.threshold=64k\\n```\\n\\nDetermining Whether the Undo_Log Compression Feature is Enabled and if the Compression Threshold is Reached\\n\\n```java\\nprotected boolean needCompress(byte[] undoLogContent) {\\n// 1. Check whether undo_log compression is enabled (1.4.2 Enabled by Default).\\n// 2. Check whether the compression threshold has been reached (64k by default).\\n// If both return requirements are met, the corresponding undoLogContent is compressed\\n    return ROLLBACK_INFO_COMPRESS_ENABLE \\n        && undoLogContent.length > ROLLBACK_INFO_COMPRESS_THRESHOLD;\\n}\\n```\\n\\nInitiating Compression for Undo_Log After Determining the Need\\n\\n\\n```java\\n// If you need to compress, compress undo_log\\nif (needCompress(undoLogContent)) {\\n    // Gets the compression type, default zip\\n    compressorType = ROLLBACK_INFO_COMPRESS_TYPE;\\n    //Get the corresponding compressor and compress it\\n    undoLogContent = CompressorFactory.getCompressor(compressorType.getCode()).compress(undoLogContent);\\n}\\n// else does not need to compress and does not need to do anything\\n```\\n\\nSave the compression type synchronously to the database for use when rolling back:\\n\\n```java\\nprotected String buildContext(String serializer, CompressorType compressorType) {\\n    Map<String, String> map = new HashMap<>();\\n    map.put(UndoLogConstants.SERIALIZER_KEY, serializer);\\n    // Save the compression type to the database\\n    map.put(UndoLogConstants.COMPRESSOR_TYPE_KEY, compressorType.name());\\n    return CollectionUtils.encodeMap(map);\\n}\\n```\\n\\nDecompress the corresponding information when rolling back:\\n\\n```java\\nprotected byte[] getRollbackInfo(ResultSet rs) throws SQLException  {\\n    // Gets a byte array of rollback information saved to the database\\n    byte[] rollbackInfo = rs.getBytes(ClientTableColumnsName.UNDO_LOG_ROLLBACK_INFO);\\n    // Gets the compression type\\n    // getOrDefault uses the default value CompressorType.NONE to directly upgrade 1.4.2+ to compatible versions earlier than 1.4.2\\n    String rollbackInfoContext = rs.getString(ClientTableColumnsName.UNDO_LOG_CONTEXT);\\n    Map<String, String> context = CollectionUtils.decodeMap(rollbackInfoContext);\\n    CompressorType compressorType = CompressorType.getByName(context.getOrDefault(UndoLogConstants.COMPRESSOR_TYPE_KEY,\\n    CompressorType.NONE.name()));\\n    // Get the corresponding compressor and uncompress it\\n    return CompressorFactory.getCompressor(compressorType.getCode())\\n        .decompress(rollbackInfo);\\n}\\n```\\n\\n\\n\\n### peroration\\n\\nBy compressing undo_log, Seata can further improve its performance when processing large amounts of data at the framework level. At the same time, it also provides the corresponding switch and relatively reasonable default value, which is convenient for users to use out of the box, but also convenient for users to adjust according to actual needs, so that the corresponding function is more suitable for the actual use scenario."},{"id":"/integrate-seata-tcc-mode-with-spring-cloud","metadata":{"permalink":"/blog/integrate-seata-tcc-mode-with-spring-cloud","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/integrate-seata-tcc-mode-with-spring-cloud.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/integrate-seata-tcc-mode-with-spring-cloud.md","title":"Integration of Spring Cloud with Seata for Distributed Transaction - TCC Mode","description":"This article mainly introduces the integration of Spring Cloud with Seata for distributed transaction using the TCC mode.","date":"2021-01-23T00:00:00.000Z","formattedDate":"January 23, 2021","tags":[],"readingTime":5.76,"hasTruncateMarker":false,"authors":[{"name":"gongxing(zhijian.tan)"}],"frontMatter":{"title":"Integration of Spring Cloud with Seata for Distributed Transaction - TCC Mode","keywords":["TCC","Seata","Spring Cloud","Distributed","Transaction"],"description":"This article mainly introduces the integration of Spring Cloud with Seata for distributed transaction using the TCC mode.","author":"gongxing(zhijian.tan)","date":"2021-01-23T00:00:00.000Z"},"prevItem":{"title":"Seata New Feature Support -- Undo_Log Compression","permalink":"/blog/seata-feature-undo-log-compress"},"nextItem":{"title":"Designing More Flexible Financial Applications with Seata Saga","permalink":"/blog/design-more-flexable-application-by-saga"}},"content":"This article will introduce how to integrate Seata (1.4.0) with Spring Cloud and Feign using the TCC mode. In practice, Seata\'s AT mode can meet about 80% of our distributed transaction needs. However, when dealing with operations on databases and middleware (such as Redis) that do not support transactions, or when using databases that are not currently supported by the AT mode (currently AT supports MySQL, Oracle, and PostgreSQL), cross-company service invocations, cross-language application invocations, or the need for manual control of the entire two-phase commit process, we need to combine the TCC mode. Moreover, the TCC mode also supports mixed usage with the AT mode.\\n\\n\\n# \u4e00\u3001The concept of TCC mode\\n\\nIn Seata, a distributed global transaction follows a two-phase commit model with a Try-[Confirm/Cancel] pattern. Both the AT (Automatic Transaction) mode and the TCC (Try-Confirm-Cancel) mode in Seata are implementations of the two-phase commit. The main differences between them are as follows:\\n\\nAT mode is based on relational databases that support local ACID transactions (currently supporting MySQL, Oracle, and PostgreSQL):\\n\\nThe first phase, prepare: In the local transaction, it combines the submission of business data updates and the recording of corresponding rollback logs.\\nThe second phase, commit: It immediately completes successfully and automatically asynchronously cleans up the rollback logs.\\nThe second phase, rollback: It automatically generates compensation operations through the rollback logs to complete data rollback.\\n\\nOn the other hand, TCC mode does not rely on transaction support from underlying data resources:\\n\\nThe first phase, prepare: It calls a custom-defined prepare logic.\\nThe second phase, commit: It calls a custom-defined commit logic.\\nThe second phase, rollback: It calls a custom-defined rollback logic.\\n\\nTCC mode refers to the ability to include custom-defined branch transactions in the management of global transactions.\\n\\nIn summary, Seata\'s TCC mode is a manual implementation of the AT mode that allows you to define the processing logic for the two phases without relying on the undo_log used in the AT mode.\\n\\n# \u4e8c\u3001prepare\\n\\n- regist center [nacos](https://nacos.io/zh-cn/ \\"nacos\\") \\n- [seata server(TC\uff09](/docs/ops/deploy-guide-beginner/ \\"seata\u670d\u52a1\u7aef(TC\uff09\\")\\n\\n\\n# \u4e09\u3001Building TM and TCC-RM\\n\\nThis chapter focuses on the implementation of TCC using Spring Cloud + Feign. For the project setup, please refer to the source code (this project provides demos for both AT mode and TCC mode).\\n\\n[DEMO](https://github.com/tanzzj/springcloud-seata-feign \\"\u670d\u52a1\u7aef\u642d\u5efa\u6587\u6863\\")\\n\\n## 3.1 build seata server \\n\\n[build server doc](/docs/ops/deploy-guide-beginner/ \\"\u670d\u52a1\u7aef\u642d\u5efa\u6587\u6863\\")\\n\\n## 3.2 build TM\\n\\n[service-tm](https://github.com/tanzzj/springcloud-seata-feign/tree/master/service-tm)\\n\\n## 3.3 build RM-TCC\\n\\n### 3.3.1 Defining TCC Interface\\n\\nSince we are using Spring Cloud + Feign, which relies on HTTP for communication, we can use @LocalTCC here. It is important to note that @LocalTCC must be annotated on the interface. This interface can be a regular business interface as long as it implements the corresponding methods for the two-phase commit in TCC. The TCC-related annotations are as follows:\\n\\n- @LocalTCC: Used for TCC in the Spring Cloud + Feign mode.\\n- @TwoPhaseBusinessAction: Annotates the try method. The name attribute represents the bean name of the current TCC method, which can be the method name (globally unique). The commitMethod attribute points to the commit method, and the rollbackMethod attribute points to the transaction rollback method. After specifying these three methods, Seata will automatically invoke the commit or rollback method based on the success or failure of the global transaction.\\n- @BusinessActionContextParameter: Annotates the parameters to be passed to the second phase (commitMethod/rollbackMethod) methods.\\n- BusinessActionContext: Represents the TCC transaction context.\\n\\nHere is an example:\\n\\n```java\\n/**\\n * Here we define the TCC interface.\\n * It must be defined on the interface.\\n * We are using Spring Cloud for remote invocation.\\n * Therefore, we can use LocalTCC here.\\n *\\n */\\n@LocalTCC\\npublic interface TccService {\\n \\n    /**\\n     * Define the two-phase commit.\\n     * name = The bean name of this TCC, globally unique.\\n     * commitMethod = The method for the second phase confirmation.\\n     * rollbackMethod = The method for the second phase cancellation.\\n     * Use the BusinessActionContextParameter annotation to pass parameters to the second phase.\\n     *\\n     * @param params  \\n     * @return String\\n     */\\n    @TwoPhaseBusinessAction(name = \\"insert\\", commitMethod = \\"commitTcc\\", rollbackMethod = \\"cancel\\")\\n    String insert(\\n            @BusinessActionContextParameter(paramName = \\"params\\") Map<String, String> params\\n    );\\n \\n    /**\\n     *  The confirmation method can be named differently, but it must be consistent with the commitMethod.\\n     *  The context can be used to pass the parameters from the try method.\\n     * @param context \\n     * @return boolean\\n     */\\n    boolean commitTcc(BusinessActionContext context);\\n \\n    /**\\n     * two phase cancel\\n     *\\n     * @param context \\n     * @return boolean\\n     */\\n    boolean cancel(BusinessActionContext context);\\n}\\n```\\n\\n### 3.3.2 Business Implementation of TCC Interface\\n\\nTo keep the code concise, we will combine the routing layer with the business layer for explanation here. However, in actual projects, this may not be the case.\\n\\n- Using @Transactional in the try method allows for direct rollback of operations in relational databases through Spring transactions. The rollback of operations in non-relational databases or other middleware can be handled in the rollbackMethod.\\n- By using context.getActionContext(\\"params\\"), you can retrieve the parameters defined in the try phase and perform business rollback operations on these parameters in the second phase.\\n- Note 1: It is not advisable to catch exceptions here (similarly, handle exceptions with aspects), as doing so would cause TCC to recognize the operation as successful, and the second phase would directly execute the commitMethod.\\n- Note 2: In TCC mode, it is the responsibility of the developer to ensure idempotence and transaction suspension prevention.\\n\\n```java\\n@Slf4j\\n@RestController\\npublic class TccServiceImpl implements  TccService {\\n \\n    @Autowired\\n    TccDAO tccDAO;\\n \\n    /**\\n     * tcc t\uff08try\uff09method\\n     * Choose the actual business execution logic or resource reservation logic based on the actual business scenario.\\n     *\\n     * @param params - name\\n     * @return String\\n     */\\n    @Override\\n    @PostMapping(\\"/tcc-insert\\")\\n    @Transactional(rollbackFor = Exception.class, propagation = Propagation.REQUIRED)\\n    public String insert(@RequestBody Map<String, String> params) {\\n        log.info(\\"xid = \\" + RootContext.getXID());\\n        //todo Perform actual operations or operations on MQ, Redis, etc.\\n        tccDAO.insert(params);\\n        //Remove the following annotations to throw an exception\\n        //throw new RuntimeException(\\"\u670d\u52a1tcc\u6d4b\u8bd5\u56de\u6eda\\");\\n        return \\"success\\";\\n    }\\n \\n    /**\\n     * TCC service confirm method\\n     * If resource reservation is used in the first phase, the reserved resources should be committed during the second phase confirmation\\n     * @param context \\n     * @return boolean\\n     */\\n    @Override\\n    public boolean commitTcc(BusinessActionContext context) {\\n        log.info(\\"xid = \\" + context.getXid() + \\"\u63d0\u4ea4\u6210\u529f\\");\\n        //todo If resource reservation is used in the first phase, resources should be committed here.\\n        return true;\\n    }\\n \\n    /**\\n     * tcc  cancel method\\n     *\\n     * @param context \\n     * @return boolean\\n     */\\n    @Override\\n    public boolean cancel(BusinessActionContext context) {\\n        //todo Here, write the rollback operations for middleware or non-relational databases.\\n        System.out.println(\\"please manually rollback this data:\\" + context.getActionContext(\\"params\\"));\\n        return true;\\n    }\\n}\\n```\\n\\n### 3.3.3 Starting a Global Transaction in TM and Invoking RM-TCC Interface\\n\\nPlease refer to the project source code in section 3.2.\\n\\nWith this, the integration of TCC mode with Spring Cloud is complete."},{"id":"/design-more-flexable-application-by-saga","metadata":{"permalink":"/blog/design-more-flexable-application-by-saga","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/design-more-flexable-application-by-saga.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/design-more-flexable-application-by-saga.md","title":"Designing More Flexible Financial Applications with Seata Saga","description":"This article delves into the pain points of developing distributed financial applications, analyzing solutions from both theoretical and practical perspectives. It explains how to design more flexible financial applications using Seata Saga.","date":"2019-11-04T00:00:00.000Z","formattedDate":"November 4, 2019","tags":[],"readingTime":19.94,"hasTruncateMarker":false,"authors":[{"name":"long187"}],"frontMatter":{"title":"Designing More Flexible Financial Applications with Seata Saga","keywords":["Saga","Seata","Consistency","Financial","Flexibility","Distributed","Transaction"],"description":"This article delves into the pain points of developing distributed financial applications, analyzing solutions from both theoretical and practical perspectives. It explains how to design more flexible financial applications using Seata Saga.","author":"long187","date":"2019-11-04T00:00:00.000Z"},"prevItem":{"title":"Integration of Spring Cloud with Seata for Distributed Transaction - TCC Mode","permalink":"/blog/integrate-seata-tcc-mode-with-spring-cloud"},"nextItem":{"title":"Fescar Integration with Spring Cloud In-Depth Analysis of Source Code","permalink":"/blog/how-to-support-spring-cloud"}},"content":"Seata, short for Simple Extensible Autonomous Transaction Architecture, is an all-in-one distributed transaction solution. It provides AT, TCC, Saga, and XA transaction modes. This article provides a detailed explanation of the Saga mode within Seata, with the project hosted on [GitHub](https://github.com/apache/incubator-seata).\\n\\nAuthor: Yiyuan (Chen Long), Core Developer of Distributed Transactions at Ant Financial, Seata Committer.\\n\\n\\n<a name=\\"uTwja\\"></a>\\n\\n# Pain Points in Financial Distributed Application Development\\n\\nDistributed systems face a prominent challenge where a business process requires a composition of various services. This challenge becomes even more pronounced in a microservices architecture, as it necessitates consistency guarantees at the business level. In other words, if a step fails, it either needs to roll back to the previous service invocation or continuously retry to ensure the success of all steps. - From \\"Left Ear Wind - Resilient Design: Compensation Transaction\\"\\n\\nIn the domain of financial microservices architecture, business processes are often more complex. Processes are lengthy, such as a typical internet microloan business process involving calls to more than ten services. When combined with exception handling processes, the complexity increases further. Developers with experience in financial business development can relate to these challenges.\\n\\nDuring the development of financial distributed applications, we encounter several pain points:\\n\\n- **Difficulty Ensuring Business Consistency**<br />\\n\\n    In many of the systems we encounter (e.g., in channel layers, product layers, and integration layers), ensuring eventual business consistency often involves adopting a \\"compensation\\" approach. Without a coordinator to support this, the development difficulty is significant. Each step requires handling \\"rollback\\" operations in catch blocks, resulting in a code structure resembling an \\"arrow,\\" with poor readability and maintainability. Alternatively, retrying exceptional operations, if unsuccessful, might lead to asynchronous retries or even manual intervention. These challenges impose a significant burden on developers, reducing development efficiency and increasing the likelihood of errors.\\n\\n- **Difficulty Managing Business State**<br />\\n\\n    With numerous business entities and their corresponding states, developers often update the entity\'s state in the database after completing a business activity. Lack of a state machine to manage the entire state transition process results in a lack of intuitiveness, increases the likelihood of errors, and causes the business to enter an incorrect state.\\n\\n- **Difficulty Ensuring Idempotence**<br />\\n\\n    Idempotence of services is a fundamental requirement in a distributed environment. Ensuring the idempotence of services often requires developers to design each service individually, using unique keys in databases or distributed caches. There is no unified solution, creating a significant burden on developers and increasing the chances of oversight, leading to financial losses.\\n\\n- **Challenges in Business Monitoring and Operations; Lack of Unified Error Guardian Capability**<br />\\n\\n    Monitoring the execution of business operations is usually done by logging, and monitoring platforms are based on log analysis. While this is generally sufficient, in the case of business errors, these monitors lack immediate access to the business context and require additional database queries. Additionally, the reliance on developers for log printing makes it prone to omissions. For compensatory transactions, there is often a need for \\"error guardian triggering compensation\\" and \\"worker-triggered compensation\\" operations. The lack of a unified error guardian and processing standard requires developers to implement these individually, resulting in a heavy development burden.\\n\\n\\n<a name=\\"hvEU6\\"></a>\\n\\n# Theoretical Foundation\\n\\nIn certain scenarios where strong consistency is required for data, we may adopt distributed transaction schemes like \\"Two-Phase Commit\\" at the business layer. However, in other scenarios, where such strong consistency is not necessary, ensuring eventual consistency is sufficient.\\n\\nFor example, Ant Financial currently employs the TCC (Try, Confirm, Cancel) pattern in its financial core systems. The characteristics of financial core systems include high consistency requirements (business isolation), short processes, and high concurrency.\\n\\nOn the other hand, in many business systems above the financial core (e.g., systems in the channel layer, product layer, and integration layer), the emphasis is on achieving eventual consistency. These systems typically have complex processes, long flows, and may need to call services from other companies (such as financial networks). Developing Try, Confirm, Cancel methods for each service in these scenarios incurs high costs. Additionally, when there are services from other companies in the transaction, it is impractical to require those services to follow the TCC development model. Long processes can negatively impact performance if transaction boundaries are too extensive.\\n\\nWhen it comes to transactions, we are familiar with ACID, and we are also acquainted with the CAP theorem, which states that at most two out of three\u2014Consistency (C), Availability (A), and Partition Tolerance (P)\u2014can be achieved simultaneously. To enhance performance, a variant of ACID known as BASE emerged. While ACID emphasizes consistency (C in CAP), BASE emphasizes availability (A in CAP). Achieving strong consistency (ACID) is often challenging, especially when dealing with multiple systems that are not provided by a single company. BASE systems are designed to create more resilient systems. In many situations, particularly when dealing with multiple systems and providers, BASE systems acknowledge the risk of data inconsistency in the short term. This allows new transactions to occur, with potentially problematic transactions addressed later through compensatory means to ensure eventual consistency.\\n\\nTherefore, in practical development, we make trade-offs. For many business systems above the financial core, compensatory transactions can be adopted. The concept of compensatory transactions has been proposed for about 30 years, with the Saga theory emerging as a solution for long transactions. With the recent rise of microservices, Saga has gradually gained attention in recent years. Currently, the industry generally recognizes Saga as a solution for handling long transactions.\\n\\n> [https://github.com/aphyr/dist-sagas/blob/master/sagas.pdf](https://github.com/aphyr/dist-sagas/blob/master/sagas.pdf)[1]\\n> [http://microservices.io/patterns/data/saga.html](http://microservices.io/patterns/data/saga.html)[2]\\n\\n\\n<a name=\\"k8kbY\\"></a>\\n\\n# Community and Industry Solutions\\n\\n<a name=\\"Oc5Er\\"></a>\\n## Apache Camel Saga\\n\\nCamel is an open-source product that implements Enterprise Integration Patterns (EIP). It is based on an event-driven architecture and offers good performance and throughput. In version 2.21, Camel introduced the Saga EIP.\\n\\nThe Saga EIP provides a way to define a series of related actions through Camel routes. These actions either all succeed or all roll back. Saga can coordinate distributed services or local services using any communication protocol, achieving global eventual consistency. Saga does not require the entire process to be completed in a short time because it does not occupy any database locks. It can support requests that require long processing times, ranging from seconds to days. Camel\'s Saga EIP is based on [MicroProfile\'s LRA](https://github.com/eclipse/microprofile-sandbox/tree/master/proposals/0009-LRA)[3] (Long Running Action). It also supports the coordination of distributed services implemented in any language using any communication protocol.\\n\\nThe implementation of Saga does not lock data. Instead, it defines \\"compensating operations\\" for each operation. When an error occurs during the normal process execution, the \\"compensating operations\\" for the operations that have already been executed are triggered to roll back the process. \\"Compensating operations\\" can be defined on Camel routes using Java or XML DSL (Definition Specific Language).\\n\\nHere is an example of Java DSL:\\n\\n```java\\n// Java DSL example goes here\\n\\n```java\\n// action\\nfrom(\\"direct:reserveCredit\\")\\n  .bean(idService, \\"generateCustomId\\") // generate a custom Id and set it in the body\\n  .to(\\"direct:creditReservation\\")\\n\\n// delegate action\\nfrom(\\"direct:creditReservation\\")\\n  .saga()\\n  .propagation(SagaPropagation.SUPPORTS)\\n  .option(\\"CreditId\\", body()) // mark the current body as needed in the compensating action\\n  .compensation(\\"direct:creditRefund\\")\\n    .bean(creditService, \\"reserveCredit\\")\\n    .log(\\"Credit ${header.amount} reserved. Custom Id used is ${body}\\");\\n\\n// called only if the saga is cancelled\\nfrom(\\"direct:creditRefund\\")\\n  .transform(header(\\"CreditId\\")) // retrieve the CreditId option from headers\\n  .bean(creditService, \\"refundCredit\\")\\n  .log(\\"Credit for Custom Id ${body} refunded\\");\\n```\\n\\nXML DSL sample:\\n```xml\\n<route>\\n  <from uri=\\"direct:start\\"/>\\n  <saga>\\n    <compensation uri=\\"direct:compensation\\" />\\n    <completion uri=\\"direct:completion\\" />\\n    <option optionName=\\"myOptionKey\\">\\n      <constant>myOptionValue</constant>\\n    </option>\\n    <option optionName=\\"myOptionKey2\\">\\n      <constant>myOptionValue2</constant>\\n    </option>\\n  </saga>\\n  <to uri=\\"direct:action1\\" />\\n  <to uri=\\"direct:action2\\" />\\n</route>\\n```\\n\\n<a name=\\"pQWuF\\"></a>\\n\\n## Eventuate Tram Saga\\n\\n[Eventuate Tram Saga](https://github.com/eventuate-tram/eventuate-tram-sagas)[4]\xa0The framework is a Saga framework for Java microservices using JDBC/JPA. Similar to Camel Saga, it also adopts Java DSL to define compensating operations:\\n\\n\\n```java\\npublic class CreateOrderSaga implements SimpleSaga<CreateOrderSagaData> {\\n\\n  private SagaDefinition<CreateOrderSagaData> sagaDefinition =\\n          step()\\n            .withCompensation(this::reject)\\n          .step()\\n            .invokeParticipant(this::reserveCredit)\\n          .step()\\n            .invokeParticipant(this::approve)\\n          .build();\\n\\n\\n  @Override\\n  public SagaDefinition<CreateOrderSagaData> getSagaDefinition() {\\n    return this.sagaDefinition;\\n  }\\n\\n\\n  private CommandWithDestination reserveCredit(CreateOrderSagaData data) {\\n    long orderId = data.getOrderId();\\n    Long customerId = data.getOrderDetails().getCustomerId();\\n    Money orderTotal = data.getOrderDetails().getOrderTotal();\\n    return send(new ReserveCreditCommand(customerId, orderId, orderTotal))\\n            .to(\\"customerService\\")\\n            .build();\\n\\n...\\n```\\n\\n<a name=\\"scN9h\\"></a>\\n\\n## Apache ServiceComb Saga\\n\\n[ServiceComb Saga](https://github.com/apache/incubator-servicecomb-saga)[5] is also a solution for achieving data eventual consistency in microservices applications. In contrast to [TCC](http://design.inf.usi.ch/sites/default/files/biblio/rest-tcc.pdf), Saga directly commits transactions in the try phase, and the subsequent rollback phase is completed through compensating operations in reverse. What sets it apart is the use of Java annotations and interceptors to define \\"compensating\\" services.<br />\\n\\n#### Architecture:\\n\\nSaga consists of **alpha** and **omega**, where:\\n\\n- Alpha acts as the coordinator, primarily responsible for managing and coordinating transactions;<br />\\n- Omega is an embedded agent in microservices, responsible for intercepting network requests and reporting transaction events to alpha;<br />\\n\\nThe diagram below illustrates the relationship between alpha, omega, and microservices:<br />\\n\\n![ServiceComb Saga](/img/saga/service-comb-saga.png?raw=true)\\n\\n<a name=\\"ggflbq\\"></a>\\n\\n#### sample\uff1a\\n```java\\npublic class ServiceA extends AbsService implements IServiceA {\\n\\n  private static final Logger LOG = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\\n\\n  @Autowired\\n  private IServiceB serviceB;\\n\\n  @Autowired\\n  private IServiceC serviceC;\\n\\n  @Override\\n  public String getServiceName() {\\n    return \\"servicea\\";\\n  }\\n\\n  @Override\\n  public String getTableName() {\\n    return \\"testa\\";\\n  }\\n\\n  @Override\\n  @SagaStart\\n  @Compensable(compensationMethod = \\"cancelRun\\")\\n  @Transactional(rollbackFor = Exception.class)\\n  public Object run(InvokeContext invokeContext) throws Exception {\\n    LOG.info(\\"A.run called\\");\\n    doRunBusi();\\n    if (invokeContext.isInvokeB(getServiceName())) {\\n      serviceB.run(invokeContext);\\n    }\\n    if (invokeContext.isInvokeC(getServiceName())) {\\n      serviceC.run(invokeContext);\\n    }\\n    if (invokeContext.isException(getServiceName())) {\\n      LOG.info(\\"A.run exception\\");\\n      throw new Exception(\\"A.run exception\\");\\n    }\\n    return null;\\n  }\\n\\n  public void cancelRun(InvokeContext invokeContext) {\\n    LOG.info(\\"A.cancel called\\");\\n    doCancelBusi();\\n  }\\n```\\n\\n<a name=\\"CnD8r\\"></a>\\n\\n## Ant Financial\'s Practice\\n\\nAnt Financial extensively uses the TCC mode for distributed transactions, mainly in scenarios where high consistency and performance are required, such as in financial core systems. In upper-level business systems with complex and lengthy processes, developing TCC can be costly. In such cases, most businesses opt for the Saga mode to achieve eventual business consistency. Due to historical reasons, different business units have their own set of \\"compensating\\" transaction solutions, basically falling into two categories:\\n\\n1. When a service needs to \\"retry\\" or \\"compensate\\" in case of failure, a record is inserted into the database with the status before executing the service. When an exception occurs, a scheduled task queries the database record and performs \\"retry\\" or \\"compensation.\\" If the business process is successful, the record is deleted.\\n\\n2. Designing a state machine engine and a simple DSL to orchestrate business processes and record business states. The state machine engine can define \\"compensating services.\\" In case of an exception, the state machine engine invokes \\"compensating services\\" in reverse. There is also an \\"error guardian\\" platform that monitors failed or uncompensated business transactions and continuously performs \\"compensation\\" or \\"retry.\\"\\n\\n## Solution Comparison\\n\\nGenerally, there are two common solutions in the community and industry: one is based on a state machine or a process engine that orchestrates processes and defines compensation through DSL; the other is based on Java annotations and interceptors to implement compensation. What are the advantages and disadvantages of these two approaches?\\n\\n| Approach             | Pros                                                                                                                                                                                                                                                                                                  | Cons                                                                                                                                                                            |\\n| -------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| State Machine + DSL  | <br />- Business processes can be defined using visual tools, standardized, readable, and can achieve service orchestration functionality<br />- Improves communication efficiency between business analysts and developers<br />- Business state management: Processes are essentially state machines, reflecting the flow of business states<br />- Enhances flexibility in exception handling: Can implement \\"forward retry\\" or \\"backward compensation\\" after recovery from a crash<br />- Naturally supports asynchronous processing engines such as Actor model or SEDA architecture, improving overall throughput<br /> | <br />- Business processes are composed of JAVA programs and DSL configurations, making development relatively cumbersome<br />- High intrusiveness into existing business if it is a transformation<br />- High implementation cost of the engine<br />          |\\n| Interceptor + Java Annotation | <br />- Programs and annotations are integrated, simple development, low learning curve<br />- Easy integration into existing businesses<br />- Low framework implementation cost                                                                                                                                                                              | <br />- The framework cannot provide asynchronous processing modes such as the Actor model or SEDA architecture to improve system throughput<br />- The framework cannot provide business state management<br />- Difficult to achieve \\"forward retry\\" after crash recovery due to the inability to restore thread context<br />          |\\n\\n## Seata Saga Approach\\n\\nThe introduction of Seata Saga can be found in [Seata Saga Official Documentation](http://seata.io/zh-cn/docs/user/saga.html)[6].\\n\\nSeata Saga adopts the state machine + DSL approach for the following reasons:\\n\\n- The state machine + DSL approach is more widely used in practical production scenarios.\\n- Can use asynchronous processing engines such as the Actor model or SEDA architecture to improve overall throughput.\\n- Typically, business systems above the core system have \\"service orchestration\\" requirements, and service orchestration has transactional eventual consistency requirements. These two are challenging to separate. The state machine + DSL approach can simultaneously meet these two requirements.\\n- Because Saga mode theoretically does not guarantee isolation, in extreme cases, it may not complete the rollback operation due to dirty writing. For example, in a distributed transaction, if you recharge user A first and then deduct the balance from user B, if A user consumes the balance before the transaction is committed, and the transaction is rolled back, there is no way to compensate. Some business scenarios may allow the business to eventually succeed, and in cases where rollback is impossible, it can continue to retry the subsequent process. The state machine + DSL approach can achieve the ability to \\"forward\\" recover context and continue execution, making the business eventually successful and achieving eventual consistency.\\n\\n> In cases where isolation is not guaranteed: When designing business processes, follow the principle of \\"prefer long\u6b3e, not short\u6b3e.\\" Long\u6b3e means fewer funds for customers and more funds for institutions. Institutions can refund customers based on their credibility. Conversely, short\u6b3e means less funding for institutions, and the funds may not be recovered. Therefore, in business process design, deduction should be done first.\\n\\n### State Definition Language (Seata State Language)\\n\\n1. Define the service call process through a state diagram and generate a JSON state language definition file.\\n2. In the state diagram, a node can be a service call, and the node can configure its compensating node.\\n3. The JSON state diagram is driven by the state machine engine. When an exception occurs, the state engine executes the compensating node corresponding to the successfully executed node to roll back the transaction.\\n> Note: Whether to compensate when an exception occurs can also be user-defined.\\n\\n4. It can meet service orchestration requirements, supporting one-way selection, concurrency, asynchronous, sub-state machine, parameter conversion, parameter mapping, service execution status judgment, exception capture, and other functions.\\n\\nAssuming a business process calls two services, deducting inventory (InventoryService) and deducting balance (BalanceService), to ensure that in a distributed scenario, either both succeed or both roll back. Both participant services have a `reduce` method for inventory deduction or balance deduction, and a `compensateReduce` method for compensating deduction operations. Let\'s take a look at the interface definition of InventoryService:\\n\\n```java\\npublic interface InventoryService {\\n\\n    /**\\n     * reduce\\n     * @param businessKey\\n     * @param amount\\n     * @param params\\n     * @return\\n     */\\n    boolean reduce(String businessKey, BigDecimal amount, Map<String, Object> params);\\n\\n    /**\\n     * compensateReduce\\n     * @param businessKey\\n     * @param params\\n     * @return\\n     */\\n    boolean compensateReduce(String businessKey, Map<String, Object> params);\\n}\\n```\\n\\n## This is the state diagram corresponding to the business process:\\n\\n![Example State Diagram](/img/saga/demo_statelang.png?raw=true)\\n<br />Corresponding JSON\\n\\n\\n```json\\n{\\n    \\"Name\\": \\"reduceInventoryAndBalance\\",\\n    \\"Comment\\": \\"reduce inventory then reduce balance in a transaction\\",\\n    \\"StartState\\": \\"ReduceInventory\\",\\n    \\"Version\\": \\"0.0.1\\",\\n    \\"States\\": {\\n        \\"ReduceInventory\\": {\\n            \\"Type\\": \\"ServiceTask\\",\\n            \\"ServiceName\\": \\"inventoryAction\\",\\n            \\"ServiceMethod\\": \\"reduce\\",\\n            \\"CompensateState\\": \\"CompensateReduceInventory\\",\\n            \\"Next\\": \\"ChoiceState\\",\\n            \\"Input\\": [\\n                \\"$.[businessKey]\\",\\n                \\"$.[count]\\"\\n            ],\\n            \\"Output\\": {\\n                \\"reduceInventoryResult\\": \\"$.#root\\"\\n            },\\n            \\"Status\\": {\\n                \\"#root == true\\": \\"SU\\",\\n                \\"#root == false\\": \\"FA\\",\\n                \\"$Exception{java.lang.Throwable}\\": \\"UN\\"\\n            }\\n        },\\n        \\"ChoiceState\\":{\\n            \\"Type\\": \\"Choice\\",\\n            \\"Choices\\":[\\n                {\\n                    \\"Expression\\":\\"[reduceInventoryResult] == true\\",\\n                    \\"Next\\":\\"ReduceBalance\\"\\n                }\\n            ],\\n            \\"Default\\":\\"Fail\\"\\n        },\\n        \\"ReduceBalance\\": {\\n            \\"Type\\": \\"ServiceTask\\",\\n            \\"ServiceName\\": \\"balanceAction\\",\\n            \\"ServiceMethod\\": \\"reduce\\",\\n            \\"CompensateState\\": \\"CompensateReduceBalance\\",\\n            \\"Input\\": [\\n                \\"$.[businessKey]\\",\\n                \\"$.[amount]\\",\\n                {\\n                    \\"throwException\\" : \\"$.[mockReduceBalanceFail]\\"\\n                }\\n            ],\\n            \\"Output\\": {\\n                \\"compensateReduceBalanceResult\\": \\"$.#root\\"\\n            },\\n            \\"Status\\": {\\n                \\"#root == true\\": \\"SU\\",\\n                \\"#root == false\\": \\"FA\\",\\n                \\"$Exception{java.lang.Throwable}\\": \\"UN\\"\\n            },\\n            \\"Catch\\": [\\n                {\\n                    \\"Exceptions\\": [\\n                        \\"java.lang.Throwable\\"\\n                    ],\\n                    \\"Next\\": \\"CompensationTrigger\\"\\n                }\\n            ],\\n            \\"Next\\": \\"Succeed\\"\\n        },\\n        \\"CompensateReduceInventory\\": {\\n            \\"Type\\": \\"ServiceTask\\",\\n            \\"ServiceName\\": \\"inventoryAction\\",\\n            \\"ServiceMethod\\": \\"compensateReduce\\",\\n            \\"Input\\": [\\n                \\"$.[businessKey]\\"\\n            ]\\n        },\\n        \\"CompensateReduceBalance\\": {\\n            \\"Type\\": \\"ServiceTask\\",\\n            \\"ServiceName\\": \\"balanceAction\\",\\n            \\"ServiceMethod\\": \\"compensateReduce\\",\\n            \\"Input\\": [\\n                \\"$.[businessKey]\\"\\n            ]\\n        },\\n        \\"CompensationTrigger\\": {\\n            \\"Type\\": \\"CompensationTrigger\\",\\n            \\"Next\\": \\"Fail\\"\\n        },\\n        \\"Succeed\\": {\\n            \\"Type\\":\\"Succeed\\"\\n        },\\n        \\"Fail\\": {\\n            \\"Type\\":\\"Fail\\",\\n            \\"ErrorCode\\": \\"PURCHASE_FAILED\\",\\n            \\"Message\\": \\"purchase failed\\"\\n        }\\n    }\\n}\\n```\\n\\n## This is the state language to some extent referring to [AWS Step Functions](https://docs.aws.amazon.com/zh_cn/step-functions/latest/dg/tutorial-creating-lambda-state-machine.html)[7].\\n\\n<a name=\\"2de9b28a\\"></a>\\n\\n#### Introduction to \\"State Machine\\" Attributes:\\n\\n- Name: Represents the name of the state machine, must be unique;\\n- Comment: Description of the state machine;\\n- Version: Version of the state machine definition;\\n- StartState: The first \\"state\\" to run when starting;\\n- States: List of states, a map structure, where the key is the name of the \\"state,\\" which must be unique within the state machine;\\n\\n<a name=\\"2b956670\\"></a>\\n\\n#### Introduction to \\"State\\" Attributes:\\n\\n- Type: The type of the \\"state,\\" such as:\\n  - ServiceTask: Executes the service task;\\n  - Choice: Single conditional choice route;\\n  - CompensationTrigger: Triggers the compensation process;\\n  - Succeed: Normal end of the state machine;\\n  - Fail: Exceptional end of the state machine;\\n  - SubStateMachine: Calls a sub-state machine;\\n- ServiceName: Service name, usually the beanId of the service;\\n- ServiceMethod: Service method name;\\n- CompensateState: Compensatory \\"state\\" for this state;\\n- Input: List of input parameters for the service call, an array corresponding to the parameter list of the service method, $. represents using an expression to retrieve parameters from the state machine context. The expression uses [SpringEL](https://docs.spring.io/spring/docs/4.3.10.RELEASE/spring-framework-reference/html/expressions.html)[8], and if it is a constant, write the value directly;\\n- Output: Assigns the parameters returned by the service to the state machine context, a map structure, where the key is the key when placing it in the state machine context (the state machine context is also a map), and the value uses $. as a SpringEL expression, indicating the value is taken from the return parameters of the service, #root represents the entire return parameters of the service;\\n- Status: Mapping of the service execution status, the framework defines three statuses, SU success, FA failure, UN unknown. We need to map the execution status of the service into these three statuses, helping the framework judge the overall consistency of the transaction. It is a map structure, where the key is a condition expression, usually based on the return value of the service or the exception thrown for judgment. The default is a SpringEL expression to judge the return parameters of the service. Those starting with $Exception{ indicate judging the exception type, and the value is mapped to this value when this condition expression is true;\\n- Catch: Route after catching an exception;\\n- Next: The next \\"state\\" to execute after the service is completed;\\n- Choices: List of optional branches in the Choice type \\"state,\\" where Expression is a SpringEL expression, and Next is the next \\"state\\" to execute when the expression is true;\\n- ErrorCode: Error code for the Fail type \\"state\\";\\n- Message: Error message for the Fail type \\"state\\";\\n\\nFor more detailed explanations of the state language, please refer to [Seata Saga Official Documentation](http://seata.io/zh-cn/docs/user/saga.html)[6[http://seata.io/zh-cn/docs/user/saga.html](http://seata.io/zh-cn/docs/user/saga.html)].\\n\\n<a name=\\"209f0e37\\"></a>\\n\\n### State Machine Engine Principle:\\n\\n![State Machine Engine Principle](/img/saga/saga_engine_mechanism.png?raw=true)\\n\\n- The state diagram in the image first executes stateA, then executes stateB, and then executes stateC;\\n- The execution of \\"states\\" is based on an event-driven model. After stateA is executed, a routing message is generated and placed in the EventQueue. The event consumer takes the message from the EventQueue and executes stateB;\\n- When the entire state machine is started, Seata Server is called to start a distributed transaction, and the xid is generated. Then, the start event of the \\"state machine instance\\" is recorded in the local database;\\n- When a \\"state\\" is executed, Seata Server is called to register a branch transaction, and the branchId is generated. Then, the start event of the \\"state instance\\" is recorded in the local database;\\n- After a \\"state\\" is executed, the end event of the \\"state instance\\" is recorded in the local database, and Seata Server is called to report the status of the branch transaction;\\n- When the entire state machine is executed, the completion event of the \\"state machine instance\\" is recorded in the local database, and Seata Server is called to commit or roll back the distributed transaction;\\n\\n\\n<a name=\\"808e95dc\\"></a>\\n\\n### Design of State Machine Engine:\\n\\n![Design of State Machine Engine](/img/saga/saga_engine.png?raw=true)\\n\\nThe design of the state machine engine is mainly divided into three layers, with the upper layer depending on the lower layer. From bottom to top, they are:\\n\\n- Eventing Layer:\\n  - Implements an event-driven architecture that can push events and be consumed by a consumer. This layer does not care about what the event is or what the consumer executes; it is implemented by the upper layer.\\n\\n- ProcessController Layer:\\n  - Driven by the above Eventing to execute a \\"empty\\" process. The behavior and routing of \\"states\\" are not implemented. It is implemented by the upper layer.\\n> Based on the above two layers, theoretically, any \\"process\\" engine can be customly extended. The design of these two layers is based on the internal design of the financial network platform.\\n\\n\\n- StateMachineEngine Layer:\\n  - Implements the behavior and routing logic of each type of state in the state machine engine;\\n  - Provides API and state machine language repository;\\n\\n<a name=\\"73a9fddd\\"></a>\\n\\n### Practical Experience in Service Design under Saga Mode\\n\\nBelow are some practical experiences summarized in the design of microservices under Saga mode. Of course, these are recommended practices, not necessarily to be followed 100%. There are \\"workaround\\" solutions even if not followed.\\n> Good news: Seata Saga mode has no specific requirements for the interface parameters of microservices, making Saga mode suitable for integrating legacy systems or services from external institutions.\\n\\n\\n<a name=\\"d64c5051\\"></a>\\n\\n#### Allow Empty Compensation\\n\\n- Empty Compensation: The original service was not executed, but the compensation service was executed;\\n- Reasons:\\n  - Timeout (packet loss) of the original service;\\n  - Saga transaction triggers a rollback;\\n  - The request of the original service is not received, but the compensation request is received first;\\n\\nTherefore, when designing services, it is necessary to allow empty compensation, that is, if the business primary key to be compensated is not found, return compensation success and record the original business primary key.\\n\\n<a name=\\"88a92b17\\"></a>\\n\\n#### Hang Prevention Control\\n\\n- Hang: Compensation service is executed before the original service;\\n- Reasons:\\n  - Timeout (congestion) of the original service;\\n  - Saga transaction rollback triggers a rollback;\\n  - Congested original service arrives;\\n\\nTherefore, check whether the current business primary key already exists in the business primary keys recorded by empty compensation. If it exists, reject the execution of the service.\\n\\n<a name=\\"ce766631\\"></a>\\n\\n#### Idempotent Control\\n\\n- Both the original service and the compensation service need to ensure idempotence. Due to possible network timeouts, a retry strategy can be set. When a retry occurs, idempotent control should be used to avoid duplicate updates to business data.\\n\\n<a name=\\"FO5YS\\"></a>\\n\\n# Summary\\n\\nMany times, we don\'t need to emphasize strong consistency. We design more resilient systems based on the BASE and Saga theories to achieve better performance and fault tolerance in distributed architecture. There is no silver bullet in distributed architecture, only solutions suitable for specific scenarios. In fact, Seata Saga is a product with the capabilities of \\"service orchestration\\" and \\"Saga distributed transactions.\\" Summarizing, its applicable scenarios are:\\n\\n- Suitable for handling \\"long transactions\\" in a microservices architecture;\\n- Suitable for \\"service orchestration\\" requirements in a microservices architecture;\\n- Suitable for business systems with a large number of composite services above the financial core system (such as systems in the channel layer, product layer, integration layer);\\n- Suitable for scenarios where integration with services provided by legacy systems or external institutions is required (these services are immutable and cannot be required to be modified).\\n\\n<a name=\\"3X7vO\\"></a>\\n\\n## Related Links Mentioned in the Article\\n\\n[1][https://github.com/aphyr/dist-sagas/blob/master/sagas.pdf](https://github.com/aphyr/dist-sagas/blob/master/sagas.pdf)<br />[2][http://microservices.io/patterns/data/saga.html](http://microservices.io/patterns/data/saga.html)<br />[3][Microprofile \u7684 LRA](https://github.com/eclipse/microprofile-sandbox/tree/master/proposals/0009-LRA)\uff1a[https://github.com/eclipse/microprofile-sandbox/tree/master/proposals/0009-LRA](https://github.com/eclipse/microprofile-sandbox/tree/master/proposals/0009-LRA)<br />[4][Eventuate Tram Saga](https://github.com/eventuate-tram/eventuate-tram-sagas)\uff1a[https://github.com/eventuate-tram/eventuate-tram-sagas](https://github.com/eventuate-tram/eventuate-tram-sagas)<br />[5][ServiceComb Saga](https://github.com/apache/incubator-servicecomb-saga)\uff1a[https://github.com/apache/servicecomb-pack](https://github.com/apache/servicecomb-pack)<br />[6][Seata Saga \u5b98\u7f51\u6587\u6863](http://seata.io/zh-cn/docs/user/saga.html)\uff1a[http://seata.io/zh-cn/docs/user/saga.html](http://seata.io/zh-cn/docs/user/saga.html)<br />[7][AWS Step Functions](https://docs.aws.amazon.com/zh_cn/step-functions/latest/dg/tutorial-creating-lambda-state-machine.html)\uff1a[https://docs.aws.amazon.com/zh_cn/step-functions/latest/dg/tutorial-creating-lambda-state-machine.html](https://docs.aws.amazon.com/zh_cn/step-functions/latest/dg/tutorial-creating-lambda-state-machine.html)<br />[8][SpringEL](https://docs.spring.io/spring/docs/4.3.10.RELEASE/spring-framework-reference/html/expressions.html)\uff1a[https://docs.spring.io/spring/docs/4.3.10.RELEASE/spring-framework-reference/html/expressions.html](https://docs.spring.io/spring/docs/4.3.10.RELEASE/spring-framework-reference/html/expressions.html)<br />"},{"id":"/how-to-support-spring-cloud","metadata":{"permalink":"/blog/how-to-support-spring-cloud","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/how-to-support-spring-cloud.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/how-to-support-spring-cloud.md","title":"Fescar Integration with Spring Cloud In-Depth Analysis of Source Code","description":"Fescar","date":"2019-04-15T00:00:00.000Z","formattedDate":"April 15, 2019","tags":[],"readingTime":16.17,"hasTruncateMarker":false,"authors":[{"name":"shukang.guo min.ji"}],"frontMatter":{"title":"Fescar Integration with Spring Cloud In-Depth Analysis of Source Code","author":"shukang.guo min.ji","date":"2019/04/15","keywords":["fescar","seata","Distributed","transaction"]},"prevItem":{"title":"Designing More Flexible Financial Applications with Seata Saga","permalink":"/blog/design-more-flexable-application-by-saga"},"nextItem":{"title":"How to use Seata to ensure consistency between Dubbo Microservices","permalink":"/blog/quick-start-use-seata-and-dubbo-services"}},"content":"### Fescar \\n\\nCommon distributed transaction approaches include XA based on 2PC (e.g., Atomikos), TCC (e.g., ByteTCC) focusing on the business layer, and transactional messaging (e.g., RocketMQ Half Message). XA is a protocol for distributed transactions that requires support from local databases. However, the resource locking at the database level can lead to poor performance. On the other hand, TCC, introduced by Alibaba as a preacher, requires a significant amount of business code to ensure transactional consistency, resulting in higher development and maintenance costs.\\n\\nDistributed transactions are a widely discussed topic in the industry, and this is one of the reasons why Fescar has gained 6k stars in a short period of time. The name \\"Fescar\\" stands for Fast & Easy Commit And Rollback. In simple terms, Fescar drives global transactions by coordinating local RDBMS branch transactions. It is a middleware that operates at the application layer. The main advantages of Fescar are better performance compared to XA, as it does not occupy connection resources for a long time, and lower development cost and business invasiveness compared to TCC.\\n\\nSimilar to XA, Fescar divides roles into TC (Transaction Coordinator), RM (Resource Manager), and TM (Transaction Manager). The overall transaction process model of Fescar is as follows:\\n\\n![Fescar\u4e8b\u52a1\u8fc7\u7a0b](/img/blog/fescar-microservices.png)\\n\\n```\\n1.The TM (Transaction Manager) requests the TC (Transaction Coordinator) to start a global transaction. The global transaction is successfully created, and a globally unique XID (Transaction ID) is generated.\\n2.The XID is propagated in the context of the microservice invocation chain.\\n3.The RM (Resource Manager) registers the branch transaction with the TC, bringing it under the jurisdiction of the global transaction corresponding to the XID.\\n4.The TM initiates a global commit or rollback resolution for the XID with the TC.\\n5.The TC schedules the completion of commit or rollback requests for all branch transactions under the jurisdiction of the XID.\\n```\\n\\nIn the current implementation version, the TC (Transaction Coordinator) is deployed as a separate process. It is responsible for maintaining the operation records and global lock records of the global transaction, as well as coordinating and driving the global transaction\'s commit or rollback. On the other hand, the TM (Transaction Manager) and RM (Resource Manager) work in the same application process as the application.\\n\\nThe RM manages the underlying database through proxying the JDBC data source. It uses syntax parsing to retain snapshots and generate undo logs during transaction execution. This ensures that the transaction can be rolled back to its previous state if needed.\\n\\nThis covers the general flow and model division of Fescar. Now, let\'s proceed with the analysis of Fescar\'s transaction propagation mechanism.\\n\\n\\n### Fescar Transaction Propagation Mechanism\\n\\nThe transaction propagation in Fescar includes both nested transaction calls within an application and transaction propagation across different services. So, how does Fescar propagate transactions in a microservices call chain? Fescar provides a transaction API that allows users to manually bind a transaction\'s XID and join it to the global transaction. Therefore, depending on the specific service framework mechanism, we can propagate the XID in the call chain to achieve transaction propagation.\\n\\nThe RPC request process consists of two parts: the caller and the callee. We need to handle the XID during the request and response. The general process is as follows: the caller (or the requester) retrieves the XID from the current transaction context and passes it to the callee through the RPC protocol. The callee extracts the XID from the request and binds it to its own transaction context, thereby participating in the global transaction. Common microservices frameworks usually provide corresponding Filter and Interceptor mechanisms. Now, let\'s analyze the integration process of Spring Cloud and Fescar in more detail.\\n\\n### Partial Source Code Analysis of Fescar Integration with Spring Cloud Alibaba\\n\\nThis section of the source code is entirely from spring-cloud-alibaba-fescar. The source code analysis mainly includes three parts: AutoConfiguration, the microservice provider, and the microservice consumer. Regarding the microservice consumer, it can be further divided into two specific approaches: RestTemplate and Feign. For the Feign request approach, it is further categorized into usage patterns that integrate with Hystrix and Sentine.\\n\\n#### Fescar AutoConfiguration\\nFor the AutoConfiguration analysis, this section will only cover the parts related to the startup of Fescar. The analysis of other parts will be interspersed in the \'Microservice Provider\' and \'Microservice Consumer\' sections.\\n\\nThe startup of Fescar requires the configuration of GlobalTransactionScanner. The GlobalTransactionScanner is responsible for initializing Fescar\'s RM client, TM client, and automatically proxying classes annotated with the GlobalTransactional annotation. The startup of the GlobalTransactionScanner bean is loaded and injected through GlobalTransactionAutoConfiguration, which also injects FescarProperties.\\n\\nFescarProperties contains important properties of Fescar, such as txServiceGroup. The value of this property can be read from the application.properties file using the key \'spring.cloud.alibaba.fescar.txServiceGroup\', with a default value of \'${spring.application.name}-fescar-service-group\'. txServiceGroup represents the logical transaction group name in Fescar. This group name is obtained from the configuration center (currently supporting file and Apollo) to retrieve the TC cluster name corresponding to the logical transaction group name. The TC cluster\'s service name is then constructed based on the cluster name. The RM client, TM client, and TC interact through RPC by using the registry center (currently supporting Nacos, Redis, ZooKeeper, and Eureka) and the service name to find available TC service nodes.\\n\\n#### Microservice Provider\\n\\nSince the logic of the consumer is a bit more complex, let\'s first analyze the logic of the provider. For Spring Cloud projects, the default RPC transport protocol is HTTP, so the HandlerInterceptor mechanism is used to intercept HTTP requests.\\n\\nHandlerInterceptor is an interface provided by Spring, and it has three methods that can be overridden.\\n\\n```java\\n    /**\\n\\t * Intercept the execution of a handler. Called after HandlerMapping determined\\n\\t * an appropriate handler object, but before HandlerAdapter invokes the handler.\\n\\t */\\n\\tdefault boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler)\\n\\t\\t\\tthrows Exception {\\n\\n\\t\\treturn true;\\n\\t}\\n\\n\\t/**\\n\\t * Intercept the execution of a handler. Called after HandlerAdapter actually\\n\\t * invoked the handler, but before the DispatcherServlet renders the view.\\n\\t * Can expose additional model objects to the view via the given ModelAndView.\\n\\t */\\n\\tdefault void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler,\\n\\t\\t\\t@Nullable ModelAndView modelAndView) throws Exception {\\n\\t}\\n\\n\\t/**\\n\\t * Callback after completion of request processing, that is, after rendering\\n\\t * the view. Will be called on any outcome of handler execution, thus allows\\n\\t * for proper resource cleanup.\\n\\t */\\n\\tdefault void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler,\\n\\t\\t\\t@Nullable Exception ex) throws Exception {\\n\\t}\\n```\\n\\nAccording to the comments, we can clearly see the timing and common use cases of each method. For Fescar integration, it overrides the preHandle and afterCompletion methods as needed.\\n\\nThe purpose of FescarHandlerInterceptor is to bind the XID passed from the service chain to the transaction context of the service node and clean up related resources after the request is completed. FescarHandlerInterceptorConfiguration is responsible for configuring the interception of all URLs. This interceptor will be executed for all incoming requests to perform XID conversion and transaction binding.\\n\\n```java\\n/**\\n * @author xiaojing\\n *\\n * Fescar HandlerInterceptor, Convert Fescar information into\\n * @see com.alibaba.fescar.core.context.RootContext from http request\'s header in\\n * {@link org.springframework.web.servlet.HandlerInterceptor#preHandle(HttpServletRequest , HttpServletResponse , Object )},\\n * And clean up Fescar information after servlet method invocation in\\n * {@link org.springframework.web.servlet.HandlerInterceptor#afterCompletion(HttpServletRequest, HttpServletResponse, Object, Exception)}\\n */\\npublic class FescarHandlerInterceptor implements HandlerInterceptor {\\n\\n\\tprivate static final Logger log = LoggerFactory\\n\\t\\t\\t.getLogger(FescarHandlerInterceptor.class);\\n\\n\\t@Override\\n\\tpublic boolean preHandle(HttpServletRequest request, HttpServletResponse response,\\n\\t\\t\\tObject handler) throws Exception {\\n\\n\\t\\tString xid = RootContext.getXID();\\n\\t\\tString rpcXid = request.getHeader(RootContext.KEY_XID);\\n\\t\\tif (log.isDebugEnabled()) {\\n\\t\\t\\tlog.debug(\\"xid in RootContext {} xid in RpcContext {}\\", xid, rpcXid);\\n\\t\\t}\\n\\n\\t\\tif (xid == null && rpcXid != null) {\\n\\t\\t\\tRootContext.bind(rpcXid);\\n\\t\\t\\tif (log.isDebugEnabled()) {\\n\\t\\t\\t\\tlog.debug(\\"bind {} to RootContext\\", rpcXid);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\treturn true;\\n\\t}\\n\\n\\t@Override\\n\\tpublic void afterCompletion(HttpServletRequest request, HttpServletResponse response,\\n\\t\\t\\tObject handler, Exception e) throws Exception {\\n\\n\\t\\tString rpcXid = request.getHeader(RootContext.KEY_XID);\\n\\n\\t\\tif (StringUtils.isEmpty(rpcXid)) {\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\n\\t\\tString unbindXid = RootContext.unbind();\\n\\t\\tif (log.isDebugEnabled()) {\\n\\t\\t\\tlog.debug(\\"unbind {} from RootContext\\", unbindXid);\\n\\t\\t}\\n\\t\\tif (!rpcXid.equalsIgnoreCase(unbindXid)) {\\n\\t\\t\\tlog.warn(\\"xid in change during RPC from {} to {}\\", rpcXid, unbindXid);\\n\\t\\t\\tif (unbindXid != null) {\\n\\t\\t\\t\\tRootContext.bind(unbindXid);\\n\\t\\t\\t\\tlog.warn(\\"bind {} back to RootContext\\", unbindXid);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n}\\n\\n```\\n\\n\\n\\nThe preHandle method is called before the request is executed. The xid parameter represents the unique identifier of the global transaction already bound to the current transaction context, while rpcXid represents the global transaction identifier that needs to be bound to the request and is passed through the HTTP header. In the preHandle method, it checks if there is no XID in the current transaction context and if rpcXid is not empty. If so, it binds rpcXid to the current transaction context.\\n\\nThe afterCompletion method is called after the request is completed and is used to perform resource cleanup actions. Fescar uses the RootContext.unbind() method to unbind the XID involved in the transaction context. The logic in the if statement is for code robustness. If rpcXid and unbindXid are not equal, it rebinds unbindXid.\\n\\nFor Spring Cloud, the default RPC method is HTTP. Therefore, for the provider, there is no need to differentiate the request interception method. It only needs to extract the XID from the header and bind it to its own transaction context. However, for the consumer, due to the variety of request components, including circuit breakers and isolation mechanisms, different situations need to be distinguished and handled. We will analyze this in more detail later.\\n\\n\\n#### Microservice Consumer\\n\\nFescar categorizes the request methods into RestTemplate, Feign, Feign+Hystrix, and Feign+Sentinel. Different components are automatically configured through Spring Boot\'s Auto Configuration. The specific configuration classes can be found in the spring.factories file, and we will also discuss the relevant configuration classes later in this document.\\n\\n#####  RestTemplate\\n\\nLet\'s take a look at how Fescar passes XID if the consumer is using RestTemplate for requests.\\n\\n```java\\npublic class FescarRestTemplateInterceptor implements ClientHttpRequestInterceptor {\\n\\t@Override\\n\\tpublic ClientHttpResponse intercept(HttpRequest httpRequest, byte[] bytes,\\n\\t\\t\\tClientHttpRequestExecution clientHttpRequestExecution) throws IOException {\\n\\t\\tHttpRequestWrapper requestWrapper = new HttpRequestWrapper(httpRequest);\\n\\n\\t\\tString xid = RootContext.getXID();\\n\\n\\t\\tif (!StringUtils.isEmpty(xid)) {\\n\\t\\t\\trequestWrapper.getHeaders().add(RootContext.KEY_XID, xid);\\n\\t\\t}\\n\\t\\treturn clientHttpRequestExecution.execute(requestWrapper, bytes);\\n\\t}\\n}\\n```\\n\\nThe FescarRestTemplateInterceptor implements the intercept method of the ClientHttpRequestInterceptor interface. It wraps the outgoing request and, if there is an existing Fescar transaction context XID, retrieves it and adds it to the HTTP headers of the request.\\n\\nFescarRestTemplateInterceptor is configured in RestTemplate through FescarRestTemplateAutoConfiguration.\\n\\n```java\\n@Configuration\\npublic class FescarRestTemplateAutoConfiguration {\\n\\n\\t@Bean\\n\\tpublic FescarRestTemplateInterceptor fescarRestTemplateInterceptor() {\\n\\t\\treturn new FescarRestTemplateInterceptor();\\n\\t}\\n\\n\\t@Autowired(required = false)\\n\\tprivate Collection<RestTemplate> restTemplates;\\n\\n\\t@Autowired\\n\\tprivate FescarRestTemplateInterceptor fescarRestTemplateInterceptor;\\n\\n\\t@PostConstruct\\n\\tpublic void init() {\\n\\t\\tif (this.restTemplates != null) {\\n\\t\\t\\tfor (RestTemplate restTemplate : restTemplates) {\\n\\t\\t\\t\\tList<ClientHttpRequestInterceptor> interceptors = new ArrayList<ClientHttpRequestInterceptor>(\\n\\t\\t\\t\\t\\t\\trestTemplate.getInterceptors());\\n\\t\\t\\t\\tinterceptors.add(this.fescarRestTemplateInterceptor);\\n\\t\\t\\t\\trestTemplate.setInterceptors(interceptors);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n}\\n```\\n\\nThe init method iterates through all the RestTemplate instances, retrieves the original interceptors from each RestTemplate, adds the fescarRestTemplateInterceptor, and then reorders the interceptors.\\n\\n##### Feign\\n\\n![Feign \u7c7b\u5173\u7cfb\u56fe](/img/blog/20190305184812.png)\\n\\nNext, let\'s take a look at the code related to Feign. There are quite a few classes in this package, so let\'s start with its AutoConfiguration.\\n\\n```java\\n@Configuration\\n@ConditionalOnClass(Client.class)\\n@AutoConfigureBefore(FeignAutoConfiguration.class)\\npublic class FescarFeignClientAutoConfiguration {\\n\\n\\t@Bean\\n\\t@Scope(\\"prototype\\")\\n\\t@ConditionalOnClass(name = \\"com.netflix.hystrix.HystrixCommand\\")\\n\\t@ConditionalOnProperty(name = \\"feign.hystrix.enabled\\", havingValue = \\"true\\")\\n\\tFeign.Builder feignHystrixBuilder(BeanFactory beanFactory) {\\n\\t\\treturn FescarHystrixFeignBuilder.builder(beanFactory);\\n\\t}\\n\\n\\t@Bean\\n\\t@Scope(\\"prototype\\")\\n\\t@ConditionalOnClass(name = \\"com.alibaba.csp.sentinel.SphU\\")\\n\\t@ConditionalOnProperty(name = \\"feign.sentinel.enabled\\", havingValue = \\"true\\")\\n\\tFeign.Builder feignSentinelBuilder(BeanFactory beanFactory) {\\n\\t\\treturn FescarSentinelFeignBuilder.builder(beanFactory);\\n\\t}\\n\\n\\t@Bean\\n\\t@ConditionalOnMissingBean\\n\\t@Scope(\\"prototype\\")\\n\\tFeign.Builder feignBuilder(BeanFactory beanFactory) {\\n\\t\\treturn FescarFeignBuilder.builder(beanFactory);\\n\\t}\\n\\n\\t@Configuration\\n\\tprotected static class FeignBeanPostProcessorConfiguration {\\n\\n\\t\\t@Bean\\n\\t\\tFescarBeanPostProcessor fescarBeanPostProcessor(\\n\\t\\t\\t\\tFescarFeignObjectWrapper fescarFeignObjectWrapper) {\\n\\t\\t\\treturn new FescarBeanPostProcessor(fescarFeignObjectWrapper);\\n\\t\\t}\\n\\n\\t\\t@Bean\\n\\t\\tFescarContextBeanPostProcessor fescarContextBeanPostProcessor(\\n\\t\\t\\t\\tBeanFactory beanFactory) {\\n\\t\\t\\treturn new FescarContextBeanPostProcessor(beanFactory);\\n\\t\\t}\\n\\n\\t\\t@Bean\\n\\t\\tFescarFeignObjectWrapper fescarFeignObjectWrapper(BeanFactory beanFactory) {\\n\\t\\t\\treturn new FescarFeignObjectWrapper(beanFactory);\\n\\t\\t}\\n\\t}\\n\\n}\\n```\\n\\nThe FescarFeignClientAutoConfiguration is enabled when the Client.class exists and requires it to be applied before FeignAutoConfiguration. Since FeignClientsConfiguration is responsible for generating the FeignContext and is enabled by FeignAutoConfiguration, based on the dependency relationship, FescarFeignClientAutoConfiguration is also applied before FeignClientsConfiguration.\\n\\nFescarFeignClientAutoConfiguration customizes the Feign.Builder and adapts it for feign.sentinel, feign.hystrix, and regular feign cases. The purpose is to customize the actual implementation of the Client in Feign to be FescarFeignClient.\\n\\n```java\\nHystrixFeign.builder().retryer(Retryer.NEVER_RETRY)\\n      .client(new FescarFeignClient(beanFactory))\\n```\\n\\n```java\\nSentinelFeign.builder().retryer(Retryer.NEVER_RETRY)\\n\\t\\t\\t\\t.client(new FescarFeignClient(beanFactory));\\n```\\n\\n```java\\nFeign.builder().client(new FescarFeignClient(beanFactory));\\n```\\n\\n\\nFescarFeignClient is an enhancement of the original Feign client proxy.\\n\\n```java\\npublic class FescarFeignClient implements Client {\\n\\n\\tprivate final Client delegate;\\n\\tprivate final BeanFactory beanFactory;\\n\\n\\tFescarFeignClient(BeanFactory beanFactory) {\\n\\t\\tthis.beanFactory = beanFactory;\\n\\t\\tthis.delegate = new Client.Default(null, null);\\n\\t}\\n\\n\\tFescarFeignClient(BeanFactory beanFactory, Client delegate) {\\n\\t\\tthis.delegate = delegate;\\n\\t\\tthis.beanFactory = beanFactory;\\n\\t}\\n\\n\\t@Override\\n\\tpublic Response execute(Request request, Request.Options options) throws IOException {\\n\\n\\t\\tRequest modifiedRequest = getModifyRequest(request);\\n\\n\\t\\ttry {\\n\\t\\t\\treturn this.delegate.execute(modifiedRequest, options);\\n\\t\\t}\\n\\t\\tfinally {\\n\\n\\t\\t}\\n\\t}\\n\\n\\tprivate Request getModifyRequest(Request request) {\\n\\n\\t\\tString xid = RootContext.getXID();\\n\\n\\t\\tif (StringUtils.isEmpty(xid)) {\\n\\t\\t\\treturn request;\\n\\t\\t}\\n\\n\\t\\tMap<String, Collection<String>> headers = new HashMap<>();\\n\\t\\theaders.putAll(request.headers());\\n\\n\\t\\tList<String> fescarXid = new ArrayList<>();\\n\\t\\tfescarXid.add(xid);\\n\\t\\theaders.put(RootContext.KEY_XID, fescarXid);\\n\\n\\t\\treturn Request.create(request.method(), request.url(), headers, request.body(),\\n\\t\\t\\t\\trequest.charset());\\n\\t}\\n\\n```\\n\\n\\nIn the above process, we can see that FescarFeignClient modifies the original Request. It first retrieves the XID from the current transaction context and, if the XID is not empty, adds it to the request\'s header.\\n\\nFeignBeanPostProcessorConfiguration defines three beans: FescarContextBeanPostProcessor, FescarBeanPostProcessor, and FescarFeignObjectWrapper. FescarContextBeanPostProcessor and FescarBeanPostProcessor both implement the Spring BeanPostProcessor interface.\\n\\nHere is the implementation of FescarContextBeanPostProcessor\\n\\n```java\\n    @Override\\n\\tpublic Object postProcessBeforeInitialization(Object bean, String beanName)\\n\\t\\t\\tthrows BeansException {\\n\\t\\tif (bean instanceof FeignContext && !(bean instanceof FescarFeignContext)) {\\n\\t\\t\\treturn new FescarFeignContext(getFescarFeignObjectWrapper(),\\n\\t\\t\\t\\t\\t(FeignContext) bean);\\n\\t\\t}\\n\\t\\treturn bean;\\n\\t}\\n\\n\\t@Override\\n\\tpublic Object postProcessAfterInitialization(Object bean, String beanName)\\n\\t\\t\\tthrows BeansException {\\n\\t\\treturn bean;\\n\\t}\\n```\\n\\nThe two methods in BeanPostProcessor allow for pre- and post-processing of beans in the Spring container. The postProcessBeforeInitialization method is called before initialization, while the postProcessAfterInitialization method is called after initialization. The return value of these methods can be the original bean instance or a wrapped instance using a wrapper.\\n\\nFescarContextBeanPostProcessor wraps FeignContext into FescarFeignContext. FescarBeanPostProcessor wraps FeignClient into FescarLoadBalancerFeignClient and FescarFeignClient, depending on whether it inherits from LoadBalancerFeignClient.\\n\\nIn FeignAutoConfiguration, the FeignContext does not have any ConditionalOnXXX conditions. Therefore, Fescar uses a pre-processing approach to wrap FeignContext into FescarFeignContext.\\n\\n```java\\n    @Bean\\n\\tpublic FeignContext feignContext() {\\n\\t\\tFeignContext context = new FeignContext();\\n\\t\\tcontext.setConfigurations(this.configurations);\\n\\t\\treturn context;\\n\\t}\\n```\\n\\nFor Feign Clients, the FeignClientFactoryBean retrieves an instance of FeignContext. For custom Feign Client objects configured by developers using the @Configuration annotation, they are configured into the builder, which causes the enhanced FescarFeignClient in FescarFeignBuilder to become ineffective. The key code in FeignClientFactoryBean is as follows\\n\\n```java\\n\\t/**\\n\\t * @param <T> the target type of the Feign client\\n\\t * @return a {@link Feign} client created with the specified data and the context information\\n\\t */\\n\\t<T> T getTarget() {\\n\\t\\tFeignContext context = applicationContext.getBean(FeignContext.class);\\n\\t\\tFeign.Builder builder = feign(context);\\n\\n\\t\\tif (!StringUtils.hasText(this.url)) {\\n\\t\\t\\tif (!this.name.startsWith(\\"http\\")) {\\n\\t\\t\\t\\turl = \\"http://\\" + this.name;\\n\\t\\t\\t}\\n\\t\\t\\telse {\\n\\t\\t\\t\\turl = this.name;\\n\\t\\t\\t}\\n\\t\\t\\turl += cleanPath();\\n\\t\\t\\treturn (T) loadBalance(builder, context, new HardCodedTarget<>(this.type,\\n\\t\\t\\t\\t\\tthis.name, url));\\n\\t\\t}\\n\\t\\tif (StringUtils.hasText(this.url) && !this.url.startsWith(\\"http\\")) {\\n\\t\\t\\tthis.url = \\"http://\\" + this.url;\\n\\t\\t}\\n\\t\\tString url = this.url + cleanPath();\\n\\t\\tClient client = getOptional(context, Client.class);\\n\\t\\tif (client != null) {\\n\\t\\t\\tif (client instanceof LoadBalancerFeignClient) {\\n\\t\\t\\t\\t// not load balancing because we have a url,\\n\\t\\t\\t\\t// but ribbon is on the classpath, so unwrap\\n\\t\\t\\t\\tclient = ((LoadBalancerFeignClient)client).getDelegate();\\n\\t\\t\\t}\\n\\t\\t\\tbuilder.client(client);\\n\\t\\t}\\n\\t\\tTargeter targeter = get(context, Targeter.class);\\n\\t\\treturn (T) targeter.target(this, builder, context, new HardCodedTarget<>(\\n\\t\\t\\t\\tthis.type, this.name, url));\\n\\t}\\n```\\nThe above code determines whether to make a direct call to the specified URL or use load balancing based on whether the URL parameter is specified in the annotation. The targeter.target method creates the object through dynamic proxy. The general process is as follows: the parsed Feign methods are stored in a map, and then passed as a parameter to generate the InvocationHandler, which in turn generates the dynamic proxy object.\\n\\nThe presence of FescarContextBeanPostProcessor ensures that even if developers customize operations on FeignClient, the enhancement of global transactions required by Fescar can still be achieved.\\n\\nAs for FescarFeignObjectWrapper, let\'s focus on the Wrapper method:\\n\\n```java\\n\\tObject wrap(Object bean) {\\n\\t\\tif (bean instanceof Client && !(bean instanceof FescarFeignClient)) {\\n\\t\\t\\tif (bean instanceof LoadBalancerFeignClient) {\\n\\t\\t\\t\\tLoadBalancerFeignClient client = ((LoadBalancerFeignClient) bean);\\n\\t\\t\\t\\treturn new FescarLoadBalancerFeignClient(client.getDelegate(), factory(),\\n\\t\\t\\t\\t\\t\\tclientFactory(), this.beanFactory);\\n\\t\\t\\t}\\n\\t\\t\\treturn new FescarFeignClient(this.beanFactory, (Client) bean);\\n\\t\\t}\\n\\t\\treturn bean;\\n\\t}\\n```\\n\\nIn the wrap method, if the bean is an instance of LoadBalancerFeignClient, it first retrieves the actual Client object that the LoadBalancerFeignClient proxies using the client.getDelegate() method. It then wraps the Client object into FescarFeignClient and generates a subclass of LoadBalancerFeignClient called FescarLoadBalancerFeignClient. If the bean is an instance of Client and not FescarFeignClient or LoadBalancerFeignClient, it is directly wrapped and transformed into FescarFeignClient.\\n\\nThe above process design is quite clever. It controls the order of configuration based on Spring Boot\'s Auto Configuration and customizes the Feign Builder bean to ensure that all Clients are enhanced with FescarFeignClient. It also wraps the beans in the Spring container using BeanPostProcessor, ensuring that all beans in the container are enhanced with FescarFeignClient, thus avoiding the replacement action in the getTarget method of FeignClientFactoryBean.\\n\\n##### Hystrix Isolation\\n\\nNow let\'s take a look at the Hystrix part. Why do we separate Hystrix and implement a separate strategy class in Fescar? Currently, the default implementation of the transaction context RootContext is based on ThreadLocal, which means the context is bound to the thread. Hystrix itself has two isolation modes: semaphore-based isolation and thread pool-based isolation. Hystrix officially recommends using thread pool isolation for better separation, which is the commonly used mode:\\n\\n```\\nThread or Semaphore\\nThe default, and the recommended setting, is to run HystrixCommands using thread isolation (THREAD) and HystrixObservableCommands using semaphore isolation (SEMAPHORE).\\n\\nCommands executed in threads have an extra layer of protection against latencies beyond what network timeouts can offer.\\n\\nGenerally the only time you should use semaphore isolation for HystrixCommands is when the call is so high volume (hundreds per second, per instance) that the overhead of separate threads is too high; this typically only applies to non-network calls.\\n```\\n\\nYou are correct that the service layer\'s business code and the thread that sends the request are not the same. Therefore, the ThreadLocal approach cannot pass the XID to the Hystrix thread and subsequently to the callee. To address this issue, Hystrix provides a mechanism for developers to customize the concurrency strategy. This can be done by extending the HystrixConcurrencyStrategy class and overriding the wrapCallable method:\\n\\n```java\\npublic class FescarHystrixConcurrencyStrategy extends HystrixConcurrencyStrategy {\\n\\n\\tprivate HystrixConcurrencyStrategy delegate;\\n\\n\\tpublic FescarHystrixConcurrencyStrategy() {\\n\\t\\tthis.delegate = HystrixPlugins.getInstance().getConcurrencyStrategy();\\n\\t\\tHystrixPlugins.reset();\\n\\t\\tHystrixPlugins.getInstance().registerConcurrencyStrategy(this);\\n\\t}\\n\\n\\t@Override\\n\\tpublic <K> Callable<K> wrapCallable(Callable<K> c) {\\n\\t\\tif (c instanceof FescarContextCallable) {\\n\\t\\t\\treturn c;\\n\\t\\t}\\n\\n\\t\\tCallable<K> wrappedCallable;\\n\\t\\tif (this.delegate != null) {\\n\\t\\t\\twrappedCallable = this.delegate.wrapCallable(c);\\n\\t\\t}\\n\\t\\telse {\\n\\t\\t\\twrappedCallable = c;\\n\\t\\t}\\n\\t\\tif (wrappedCallable instanceof FescarContextCallable) {\\n\\t\\t\\treturn wrappedCallable;\\n\\t\\t}\\n\\n\\t\\treturn new FescarContextCallable<>(wrappedCallable);\\n\\t}\\n\\n\\tprivate static class FescarContextCallable<K> implements Callable<K> {\\n\\n\\t\\tprivate final Callable<K> actual;\\n\\t\\tprivate final String xid;\\n\\n\\t\\tFescarContextCallable(Callable<K> actual) {\\n\\t\\t\\tthis.actual = actual;\\n\\t\\t\\tthis.xid = RootContext.getXID();\\n\\t\\t}\\n\\n\\t\\t@Override\\n\\t\\tpublic K call() throws Exception {\\n\\t\\t\\ttry {\\n\\t\\t\\t\\tRootContext.bind(xid);\\n\\t\\t\\t\\treturn actual.call();\\n\\t\\t\\t}\\n\\t\\t\\tfinally {\\n\\t\\t\\t\\tRootContext.unbind();\\n\\t\\t\\t}\\n\\t\\t}\\n\\n\\t}\\n}\\n```\\n\\nFescar also provides a FescarHystrixAutoConfiguration, which generates the FescarHystrixConcurrencyStrategy when HystrixCommand is present.\\n\\n```java\\n@Configuration\\n@ConditionalOnClass(HystrixCommand.class)\\npublic class FescarHystrixAutoConfiguration {\\n\\n\\t@Bean\\n\\tFescarHystrixConcurrencyStrategy fescarHystrixConcurrencyStrategy() {\\n\\t\\treturn new FescarHystrixConcurrencyStrategy();\\n\\t}\\n\\n}\\n```\\n\\n### reference\\n\\n- Fescar: https://github.com/alibaba/fescar\\n\\n- Spring Cloud Alibaba: https://github.com/spring-cloud-incubator/spring-cloud-alibaba\\n\\n- spring-cloud-openfeign: https://github.com/spring-cloud/spring-cloud-openfeign\\n\\n ### author\\n\\n  kangshu.guo\uff0cCommunity nickname ywind, formerly employed at Huawei Terminal Cloud, currently a Java engineer at Sohu Intelligent Media Center. Mainly responsible for development related to Sohu accounts. Has a strong interest in distributed transactions, distributed systems, and microservices architecture.\\n  min.ji(qinming)\uff0cCommunity nickname slievrly, Fescar project leader, core developer of Alibaba middleware TXC/GTS. Engaged in core research and development work in distributed middleware for a long time. Has extensive technical expertise in the field of distributed transactions."},{"id":"/quick-start-use-seata-and-dubbo-services","metadata":{"permalink":"/blog/quick-start-use-seata-and-dubbo-services","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/quick-start-use-seata-and-dubbo-services.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/quick-start-use-seata-and-dubbo-services.md","title":"How to use Seata to ensure consistency between Dubbo Microservices","description":"This article will introduce you how to use Seata to ensure consistency between Dubbo Microservices.","date":"2019-03-07T00:00:00.000Z","formattedDate":"March 7, 2019","tags":[],"readingTime":2.84,"hasTruncateMarker":false,"authors":[{"name":"slievrly"}],"frontMatter":{"title":"How to use Seata to ensure consistency between Dubbo Microservices","keywords":["Dubbo","Seata","Consistency"],"description":"This article will introduce you how to use Seata to ensure consistency between Dubbo Microservices.","author":"slievrly","date":"2019-03-07T00:00:00.000Z"},"prevItem":{"title":"Fescar Integration with Spring Cloud In-Depth Analysis of Source Code","permalink":"/blog/how-to-support-spring-cloud"},"nextItem":{"title":"MT mode","permalink":"/blog/manual-transaction-mode"}},"content":"## Use case\\n\\nA business logic for user purchasing commodities. The whole business logic is powered by 3 microservices:\\n\\n- Storage service: deduct storage count on given commodity.\\n- Order service: create order according to purchase request.\\n- Account service: debit the balance of user\'s account.\\n\\n### Architecture\\n\\n![Architecture](/img/blog/seata/seata-1.png) \\n\\n\\n### StorageService\\n\\n```java\\npublic interface StorageService {\\n\\n    /**\\n     * deduct storage count\\n     */\\n    void deduct(String commodityCode, int count);\\n}\\n```\\n\\n### OrderService\\n\\n```java\\npublic interface OrderService {\\n\\n    /**\\n     * create order\\n     */\\n    Order create(String userId, String commodityCode, int orderCount);\\n}\\n```\\n\\n### AccountService\\n\\n```java\\npublic interface AccountService {\\n\\n    /**\\n     * debit balance of user\'s account\\n     */\\n    void debit(String userId, int money);\\n}\\n```\\n\\n### Main business logic\\n\\n```java\\npublic class BusinessServiceImpl implements BusinessService {\\n\\n    private StorageService storageService;\\n\\n    private OrderService orderService;\\n\\n    /**\\n     * purchase\\n     */\\n    public void purchase(String userId, String commodityCode, int orderCount) {\\n\\n        storageService.deduct(commodityCode, orderCount);\\n\\n        orderService.create(userId, commodityCode, orderCount);\\n    }\\n}\\n```\\n\\n```java\\npublic class StorageServiceImpl implements StorageService {\\n\\n  private StorageDAO storageDAO;\\n  \\n    @Override\\n    public void deduct(String commodityCode, int count) {\\n        Storage storage = new Storage();\\n        storage.setCount(count);\\n        storage.setCommodityCode(commodityCode);\\n        storageDAO.update(storage);\\n    }\\n}\\n```\\n\\n```java\\npublic class OrderServiceImpl implements OrderService {\\n\\n    private OrderDAO orderDAO;\\n\\n    private AccountService accountService;\\n\\n    public Order create(String userId, String commodityCode, int orderCount) {\\n\\n        int orderMoney = calculate(commodityCode, orderCount);\\n\\n        accountService.debit(userId, orderMoney);\\n\\n        Order order = new Order();\\n        order.userId = userId;\\n        order.commodityCode = commodityCode;\\n        order.count = orderCount;\\n        order.money = orderMoney;\\n\\n        return orderDAO.insert(order);\\n    }\\n}\\n```\\n\\n## Distributed Transaction Solution with Seata\\n\\n![undefined](/img/blog/seata/seata-2.png) \\n\\nWe just need an annotation `@GlobalTransactional` on business method: \\n\\n```java\\n\\n    @GlobalTransactional\\n    public void purchase(String userId, String commodityCode, int orderCount) {\\n        ......\\n    }\\n```\\n\\n## Example powered by Dubbo + Seata\\n\\n### Step 1: Setup database\\n\\n- Requirement: MySQL with InnoDB engine.\\n\\n**Note:** In fact, there should be 3 database for the 3 services in the example use case. However, we can just create one database and configure 3 data sources for simple. \\n\\nModify Spring XML with the database URL/username/password you just created.\\n\\ndubbo-account-service.xml\\ndubbo-order-service.xml\\ndubbo-storage-service.xml\\n\\n```xml\\n    <property name=\\"url\\" value=\\"jdbc:mysql://x.x.x.x:3306/xxx\\" />\\n    <property name=\\"username\\" value=\\"xxx\\" />\\n    <property name=\\"password\\" value=\\"xxx\\" />\\n```\\n### Step 2: Create UNDO_LOG table for Seata\\n\\n`UNDO_LOG` table is required by Seata AT mode.\\n\\n```sql\\nCREATE TABLE `undo_log` (\\n  `id` bigint(20) NOT NULL AUTO_INCREMENT,\\n  `branch_id` bigint(20) NOT NULL,\\n  `xid` varchar(100) NOT NULL,\\n  `rollback_info` longblob NOT NULL,\\n  `log_status` int(11) NOT NULL,\\n  `log_created` datetime NOT NULL,\\n  `log_modified` datetime NOT NULL,\\n  `ext` varchar(100) DEFAULT NULL,\\n  PRIMARY KEY (`id`),\\n  KEY `idx_unionkey` (`xid`,`branch_id`)\\n) ENGINE=InnoDB AUTO_INCREMENT=159 DEFAULT CHARSET=utf8\\n```\\n\\n### Step 3: Create tables for example business\\n\\n```sql\\n\\nDROP TABLE IF EXISTS `storage_tbl`;\\nCREATE TABLE `storage_tbl` (\\n  `id` int(11) NOT NULL AUTO_INCREMENT,\\n  `commodity_code` varchar(255) DEFAULT NULL,\\n  `count` int(11) DEFAULT 0,\\n  PRIMARY KEY (`id`),\\n  UNIQUE KEY (`commodity_code`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\\n\\n\\nDROP TABLE IF EXISTS `order_tbl`;\\nCREATE TABLE `order_tbl` (\\n  `id` int(11) NOT NULL AUTO_INCREMENT,\\n  `user_id` varchar(255) DEFAULT NULL,\\n  `commodity_code` varchar(255) DEFAULT NULL,\\n  `count` int(11) DEFAULT 0,\\n  `money` int(11) DEFAULT 0,\\n  PRIMARY KEY (`id`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\\n\\n\\nDROP TABLE IF EXISTS `account_tbl`;\\nCREATE TABLE `account_tbl` (\\n  `id` int(11) NOT NULL AUTO_INCREMENT,\\n  `user_id` varchar(255) DEFAULT NULL,\\n  `money` int(11) DEFAULT 0,\\n  PRIMARY KEY (`id`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\\n```\\n### Step 4: Start Seata-Server\\n\\n- Download server [package](https://github.com/apache/incubator-seata/releases), unzip it.\\n- Start Seata-Server\\n\\n```shell\\nsh seata-server.sh $LISTEN_PORT $PATH_FOR_PERSISTENT_DATA\\n\\ne.g.\\n\\nsh seata-server.sh 8091 /home/admin/seata/data/\\n```\\n\\n### Step 5: Run example\\n\\n- Start AccountService ([DubboAccountServiceStarter](https://github.com/apache/incubator-seata-samples/blob/master/dubbo/src/main/java/com/seata/seata/samples/dubbo/starter/DubboAccountServiceStarter.java)).\\n- Start StorageService ([DubboStorageServiceStarter](https://github.com/apache/incubator-seata-samples/blob/master/dubbo/src/main/java/com/seata/seata/samples/dubbo/starter/DubboStorageServiceStarter.java)).\\n- Start OrderService ([DubboOrderServiceStarter](https://github.com/apache/incubator-seata-samples/blob/master/dubbo/src/main/java/com/seata/seata/samples/dubbo/starter/DubboOrderServiceStarter.java)).\\n- Run BusinessService for test ([DubboBusinessTester](https://github.com/apache/incubator-seata-samples/blob/master/dubbo/src/main/java/com/seata/seata/samples/dubbo/starter/DubboBusinessTester.java)).\\n\\n### Related projects\\n* seata:          https://github.com/apache/incubator-seata/\\n* seata-samples : https://github.com/apache/incubator-seata-samples"},{"id":"/manual-transaction-mode","metadata":{"permalink":"/blog/manual-transaction-mode","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/manual-transaction-mode.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/manual-transaction-mode.md","title":"MT mode","description":"introduce MT mode","date":"2019-02-13T00:00:00.000Z","formattedDate":"February 13, 2019","tags":[],"readingTime":1.12,"hasTruncateMarker":false,"authors":[{"name":"kmmshmily"}],"frontMatter":{"title":"MT mode","keywords":["MT mode"],"description":"introduce MT mode","author":"kmmshmily","date":"2019-02-13T00:00:00.000Z"},"prevItem":{"title":"How to use Seata to ensure consistency between Dubbo Microservices","permalink":"/blog/quick-start-use-seata-and-dubbo-services"}},"content":"Review the description in the overview: a distributed global transaction, the whole is a model of **the two-phase commit**. A global transaction consists of several branch transactions that meet the model requirements of **the two-phase commit**, which requires each branch transaction to have its own:\\n\\n- One phase prepare behavior\\n- Two phase commit or rollback behavior\\n\\n![Overview of a global transaction](https://upload-images.jianshu.io/upload_images/4420767-e48f0284a037d1df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\\n\\nAccording to the two phase behavior pattern\uff0cWe divide the branch transaction into **Automatic (Branch) Transaction Mode** and **Manual (Branch) Transaction Mode**.\\n\\nThe AT mode is based on the **Relational Database** that **supports local ACID transactions**\uff1a\\n\\n- One phase prepare behavior: In the local transaction, the business data update and the corresponding rollback log record are submitted together.\\n- Two phase commit behavior: Immediately ended successfully, **Auto** asynchronous batch cleanup of the rollback log.\\n- Two phase rollback behavior: By rolling back the log, **automatic** generates a compensation operation to complete the data rollback.\\n\\nAccordingly, the MT mode does not rely on transaction support for the underlying data resources:\\n\\n- One phase prepare behavior: Call the prepare logic of **custom** .\\n- Two phase commit behavior:Call the commit logic of **custom** .\\n- Two phase rollback behavior:Call the rollback logic of **custom** .\\n\\nThe so-called MT mode refers to the support of the branch transaction of **custom** into the management of global transactions."}]}')}}]);